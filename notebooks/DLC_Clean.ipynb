{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project \"/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-06-03\" already exists!\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "\n",
    "config_path = deeplabcut.create_new_project(\n",
    "    'FESFatigue', \n",
    "    'Jake', \n",
    "    ['/home/jakejoseph/Desktop/Joseph_Code/FESNewCameraclips-Jake-2024-05-05/videos/fatiguetest0523ecrb12_2.mp4'], \n",
    "    working_directory='/home/jakejoseph/Desktop/Joseph_Code/', \n",
    "    copy_videos=True, \n",
    "    multianimal=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 12:22:01.804401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-21 12:22:02.161994: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-21 12:22:02.287243: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-21 12:22:03.117426: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-06-21 12:22:03.117528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-06-21 12:22:03.117536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.10...\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "config_path = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/config.yaml'\n",
    "# deeplabcut.add_new_videos(config_path, ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/ECRB_interleaved_stim_5_30_1.mp4','/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4'])\n",
    "# deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=False, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([48, 74, 59, 54, 34, 26, 68, 33, 45, 69, 27, 60,  3, 42, 43,  7, 22,\n",
       "          41, 50, 38, 61, 53, 62, 56, 72,  6, 52, 70,  4, 30, 49,  2, 28, 11,\n",
       "          23, 10, 31, 40, 57,  1, 32, 66, 14, 76, 19, 29, 63, 35, 18,  0, 75,\n",
       "          15,  5, 55, 16, 51, 20, 71,  8, 13, 25, 37, 17, 24, 46, 39, 65, 58,\n",
       "          12, 36, 21,  9, 73]),\n",
       "   array([67, 64, 47, 44])))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config_path, augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-2/UnaugmentedDataSet_FESFatigueMay31/FESFatigue_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-1/FESFatigueMay31-trainset95shuffle1/train/snapshot-297000',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 2000,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-2/UnaugmentedDataSet_FESFatigueMay31/Documentation_data-FESFatigue_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-2/FESFatigueMay31-trainset95shuffle1/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakejoseph/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2024-06-21 14:34:13.195185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.198397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.200002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.201623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.203231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.205003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9789 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-06-21 14:34:13.358370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.360325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.361883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.363471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.364991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-21 14:34:13.366522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9789 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading already trained DLC with backbone: resnet_50\n",
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-2/FESFatigueMay31-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4]], 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-2/UnaugmentedDataSet_FESFatigueMay31/FESFatigue_Jake95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-1/FESFatigueMay31-trainset95shuffle1/train/snapshot-297000', 'lr_init': 0.0005, 'max_input_size': 2000, 'metadataset': 'training-datasets/iteration-2/UnaugmentedDataSet_FESFatigueMay31/Documentation_data-FESFatigue_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 5, 'pos_dist_thresh': 17, 'project_path': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 297100 loss: 0.0031 lr: 0.005\n",
      "iteration: 297200 loss: 0.0025 lr: 0.005\n",
      "iteration: 297300 loss: 0.0025 lr: 0.005\n",
      "iteration: 297400 loss: 0.0026 lr: 0.005\n",
      "iteration: 297500 loss: 0.0020 lr: 0.005\n",
      "iteration: 297600 loss: 0.0019 lr: 0.005\n",
      "iteration: 297700 loss: 0.0021 lr: 0.005\n",
      "iteration: 297800 loss: 0.0022 lr: 0.005\n",
      "iteration: 297900 loss: 0.0021 lr: 0.005\n",
      "iteration: 298000 loss: 0.0019 lr: 0.005\n",
      "iteration: 298100 loss: 0.0020 lr: 0.005\n",
      "iteration: 298200 loss: 0.0021 lr: 0.005\n",
      "iteration: 298300 loss: 0.0018 lr: 0.005\n",
      "iteration: 298400 loss: 0.0019 lr: 0.005\n",
      "iteration: 298500 loss: 0.0018 lr: 0.005\n",
      "iteration: 298600 loss: 0.0018 lr: 0.005\n",
      "iteration: 298700 loss: 0.0022 lr: 0.005\n",
      "iteration: 298800 loss: 0.0018 lr: 0.005\n",
      "iteration: 298900 loss: 0.0020 lr: 0.005\n",
      "iteration: 299000 loss: 0.0018 lr: 0.005\n",
      "iteration: 299100 loss: 0.0024 lr: 0.005\n",
      "iteration: 299200 loss: 0.0024 lr: 0.005\n",
      "iteration: 299300 loss: 0.0017 lr: 0.005\n",
      "iteration: 299400 loss: 0.0022 lr: 0.005\n",
      "iteration: 299500 loss: 0.0020 lr: 0.005\n",
      "iteration: 299600 loss: 0.0018 lr: 0.005\n",
      "iteration: 299700 loss: 0.0019 lr: 0.005\n",
      "iteration: 299800 loss: 0.0023 lr: 0.005\n",
      "iteration: 299900 loss: 0.0022 lr: 0.005\n",
      "iteration: 300000 loss: 0.0018 lr: 0.005\n",
      "iteration: 300100 loss: 0.0019 lr: 0.005\n",
      "iteration: 300200 loss: 0.0020 lr: 0.005\n",
      "iteration: 300300 loss: 0.0017 lr: 0.005\n",
      "iteration: 300400 loss: 0.0017 lr: 0.005\n",
      "iteration: 300500 loss: 0.0016 lr: 0.005\n",
      "iteration: 300600 loss: 0.0016 lr: 0.005\n",
      "iteration: 300700 loss: 0.0020 lr: 0.005\n",
      "iteration: 300800 loss: 0.0015 lr: 0.005\n",
      "iteration: 300900 loss: 0.0017 lr: 0.005\n",
      "iteration: 301000 loss: 0.0016 lr: 0.005\n",
      "iteration: 301100 loss: 0.0022 lr: 0.005\n",
      "iteration: 301200 loss: 0.0021 lr: 0.005\n",
      "iteration: 301300 loss: 0.0017 lr: 0.005\n",
      "iteration: 301400 loss: 0.0018 lr: 0.005\n",
      "iteration: 301500 loss: 0.0016 lr: 0.005\n",
      "iteration: 301600 loss: 0.0016 lr: 0.005\n",
      "iteration: 301700 loss: 0.0018 lr: 0.005\n",
      "iteration: 301800 loss: 0.0017 lr: 0.005\n",
      "iteration: 301900 loss: 0.0019 lr: 0.005\n",
      "iteration: 302000 loss: 0.0016 lr: 0.005\n",
      "iteration: 302100 loss: 0.0017 lr: 0.005\n",
      "iteration: 302200 loss: 0.0014 lr: 0.005\n",
      "iteration: 302300 loss: 0.0019 lr: 0.005\n",
      "iteration: 302400 loss: 0.0018 lr: 0.005\n",
      "iteration: 302500 loss: 0.0016 lr: 0.005\n",
      "iteration: 302600 loss: 0.0018 lr: 0.005\n",
      "iteration: 302700 loss: 0.0021 lr: 0.005\n",
      "iteration: 302800 loss: 0.0017 lr: 0.005\n",
      "iteration: 302900 loss: 0.0021 lr: 0.005\n",
      "iteration: 303000 loss: 0.0018 lr: 0.005\n",
      "iteration: 303100 loss: 0.0016 lr: 0.005\n",
      "iteration: 303200 loss: 0.0018 lr: 0.005\n",
      "iteration: 303300 loss: 0.0016 lr: 0.005\n",
      "iteration: 303400 loss: 0.0015 lr: 0.005\n",
      "iteration: 303500 loss: 0.0014 lr: 0.005\n",
      "iteration: 303600 loss: 0.0018 lr: 0.005\n",
      "iteration: 303700 loss: 0.0015 lr: 0.005\n",
      "iteration: 303800 loss: 0.0017 lr: 0.005\n",
      "iteration: 303900 loss: 0.0016 lr: 0.005\n",
      "iteration: 304000 loss: 0.0017 lr: 0.005\n",
      "iteration: 304100 loss: 0.0016 lr: 0.005\n",
      "iteration: 304200 loss: 0.0015 lr: 0.005\n",
      "iteration: 304300 loss: 0.0017 lr: 0.005\n",
      "iteration: 304400 loss: 0.0016 lr: 0.005\n",
      "iteration: 304500 loss: 0.0017 lr: 0.005\n",
      "iteration: 304600 loss: 0.0017 lr: 0.005\n",
      "iteration: 304700 loss: 0.0015 lr: 0.005\n",
      "iteration: 304800 loss: 0.0016 lr: 0.005\n",
      "iteration: 304900 loss: 0.0016 lr: 0.005\n",
      "iteration: 305000 loss: 0.0019 lr: 0.005\n",
      "iteration: 305100 loss: 0.0019 lr: 0.005\n",
      "iteration: 305200 loss: 0.0019 lr: 0.005\n",
      "iteration: 305300 loss: 0.0017 lr: 0.005\n",
      "iteration: 305400 loss: 0.0017 lr: 0.005\n",
      "iteration: 305500 loss: 0.0016 lr: 0.005\n",
      "iteration: 305600 loss: 0.0018 lr: 0.005\n",
      "iteration: 305700 loss: 0.0018 lr: 0.005\n",
      "iteration: 305800 loss: 0.0016 lr: 0.005\n",
      "iteration: 305900 loss: 0.0016 lr: 0.005\n",
      "iteration: 306000 loss: 0.0019 lr: 0.005\n",
      "iteration: 306100 loss: 0.0018 lr: 0.005\n",
      "iteration: 306200 loss: 0.0016 lr: 0.005\n",
      "iteration: 306300 loss: 0.0017 lr: 0.005\n",
      "iteration: 306400 loss: 0.0018 lr: 0.005\n",
      "iteration: 306500 loss: 0.0018 lr: 0.005\n",
      "iteration: 306600 loss: 0.0018 lr: 0.005\n",
      "iteration: 306700 loss: 0.0015 lr: 0.005\n",
      "iteration: 306800 loss: 0.0015 lr: 0.005\n",
      "iteration: 306900 loss: 0.0018 lr: 0.005\n",
      "iteration: 307000 loss: 0.0018 lr: 0.005\n",
      "iteration: 307100 loss: 0.0019 lr: 0.02\n",
      "iteration: 307200 loss: 0.0023 lr: 0.02\n",
      "iteration: 307300 loss: 0.0017 lr: 0.02\n",
      "iteration: 307400 loss: 0.0019 lr: 0.02\n",
      "iteration: 307500 loss: 0.0017 lr: 0.02\n",
      "iteration: 307600 loss: 0.0016 lr: 0.02\n",
      "iteration: 307700 loss: 0.0020 lr: 0.02\n",
      "iteration: 307800 loss: 0.0018 lr: 0.02\n",
      "iteration: 307900 loss: 0.0020 lr: 0.02\n",
      "iteration: 308000 loss: 0.0020 lr: 0.02\n",
      "iteration: 308100 loss: 0.0020 lr: 0.02\n",
      "iteration: 308200 loss: 0.0020 lr: 0.02\n",
      "iteration: 308300 loss: 0.0019 lr: 0.02\n",
      "iteration: 308400 loss: 0.0018 lr: 0.02\n",
      "iteration: 308500 loss: 0.0018 lr: 0.02\n",
      "iteration: 308600 loss: 0.0019 lr: 0.02\n",
      "iteration: 308700 loss: 0.0018 lr: 0.02\n",
      "iteration: 308800 loss: 0.0020 lr: 0.02\n",
      "iteration: 308900 loss: 0.0019 lr: 0.02\n",
      "iteration: 309000 loss: 0.0019 lr: 0.02\n",
      "iteration: 309100 loss: 0.0018 lr: 0.02\n",
      "iteration: 309200 loss: 0.0017 lr: 0.02\n",
      "iteration: 309300 loss: 0.0021 lr: 0.02\n",
      "iteration: 309400 loss: 0.0020 lr: 0.02\n",
      "iteration: 309500 loss: 0.0018 lr: 0.02\n",
      "iteration: 309600 loss: 0.0019 lr: 0.02\n",
      "iteration: 309700 loss: 0.0019 lr: 0.02\n",
      "iteration: 309800 loss: 0.0020 lr: 0.02\n",
      "iteration: 309900 loss: 0.0017 lr: 0.02\n",
      "iteration: 310000 loss: 0.0018 lr: 0.02\n",
      "iteration: 310100 loss: 0.0022 lr: 0.02\n",
      "iteration: 310200 loss: 0.0018 lr: 0.02\n",
      "iteration: 310300 loss: 0.0018 lr: 0.02\n",
      "iteration: 310400 loss: 0.0019 lr: 0.02\n",
      "iteration: 310500 loss: 0.0017 lr: 0.02\n",
      "iteration: 310600 loss: 0.0018 lr: 0.02\n",
      "iteration: 310700 loss: 0.0017 lr: 0.02\n",
      "iteration: 310800 loss: 0.0017 lr: 0.02\n",
      "iteration: 310900 loss: 0.0018 lr: 0.02\n",
      "iteration: 311000 loss: 0.0019 lr: 0.02\n",
      "iteration: 311100 loss: 0.0018 lr: 0.02\n",
      "iteration: 311200 loss: 0.0021 lr: 0.02\n",
      "iteration: 311300 loss: 0.0018 lr: 0.02\n",
      "iteration: 311400 loss: 0.0018 lr: 0.02\n",
      "iteration: 311500 loss: 0.0017 lr: 0.02\n",
      "iteration: 311600 loss: 0.0018 lr: 0.02\n",
      "iteration: 311700 loss: 0.0019 lr: 0.02\n",
      "iteration: 311800 loss: 0.0022 lr: 0.02\n",
      "iteration: 311900 loss: 0.0017 lr: 0.02\n",
      "iteration: 312000 loss: 0.0016 lr: 0.02\n",
      "iteration: 312100 loss: 0.0020 lr: 0.02\n",
      "iteration: 312200 loss: 0.0017 lr: 0.02\n",
      "iteration: 312300 loss: 0.0017 lr: 0.02\n",
      "iteration: 312400 loss: 0.0019 lr: 0.02\n",
      "iteration: 312500 loss: 0.0019 lr: 0.02\n",
      "iteration: 312600 loss: 0.0018 lr: 0.02\n",
      "iteration: 312700 loss: 0.0019 lr: 0.02\n",
      "iteration: 312800 loss: 0.0019 lr: 0.02\n",
      "iteration: 312900 loss: 0.0018 lr: 0.02\n",
      "iteration: 313000 loss: 0.0019 lr: 0.02\n",
      "iteration: 313100 loss: 0.0018 lr: 0.02\n",
      "iteration: 313200 loss: 0.0020 lr: 0.02\n",
      "iteration: 313300 loss: 0.0017 lr: 0.02\n",
      "iteration: 313400 loss: 0.0018 lr: 0.02\n",
      "iteration: 313500 loss: 0.0019 lr: 0.02\n",
      "iteration: 313600 loss: 0.0018 lr: 0.02\n",
      "iteration: 313700 loss: 0.0017 lr: 0.02\n",
      "iteration: 313800 loss: 0.0019 lr: 0.02\n",
      "iteration: 313900 loss: 0.0017 lr: 0.02\n",
      "iteration: 314000 loss: 0.0019 lr: 0.02\n",
      "iteration: 314100 loss: 0.0017 lr: 0.02\n",
      "iteration: 314200 loss: 0.0016 lr: 0.02\n",
      "iteration: 314300 loss: 0.0017 lr: 0.02\n",
      "iteration: 314400 loss: 0.0016 lr: 0.02\n",
      "iteration: 314500 loss: 0.0020 lr: 0.02\n",
      "iteration: 314600 loss: 0.0019 lr: 0.02\n",
      "iteration: 314700 loss: 0.0017 lr: 0.02\n",
      "iteration: 314800 loss: 0.0018 lr: 0.02\n",
      "iteration: 314900 loss: 0.0017 lr: 0.02\n",
      "iteration: 315000 loss: 0.0021 lr: 0.02\n",
      "iteration: 315100 loss: 0.0017 lr: 0.02\n",
      "iteration: 315200 loss: 0.0016 lr: 0.02\n",
      "iteration: 315300 loss: 0.0020 lr: 0.02\n",
      "iteration: 315400 loss: 0.0017 lr: 0.02\n",
      "iteration: 315500 loss: 0.0017 lr: 0.02\n",
      "iteration: 315600 loss: 0.0019 lr: 0.02\n",
      "iteration: 315700 loss: 0.0021 lr: 0.02\n",
      "iteration: 315800 loss: 0.0020 lr: 0.02\n",
      "iteration: 315900 loss: 0.0018 lr: 0.02\n",
      "iteration: 316000 loss: 0.0019 lr: 0.02\n",
      "iteration: 316100 loss: 0.0020 lr: 0.02\n",
      "iteration: 316200 loss: 0.0017 lr: 0.02\n",
      "iteration: 316300 loss: 0.0019 lr: 0.02\n",
      "iteration: 316400 loss: 0.0016 lr: 0.02\n",
      "iteration: 316500 loss: 0.0016 lr: 0.02\n",
      "iteration: 316600 loss: 0.0018 lr: 0.02\n",
      "iteration: 316700 loss: 0.0018 lr: 0.02\n",
      "iteration: 316800 loss: 0.0018 lr: 0.02\n",
      "iteration: 316900 loss: 0.0015 lr: 0.02\n",
      "iteration: 317000 loss: 0.0019 lr: 0.02\n",
      "iteration: 317100 loss: 0.0018 lr: 0.02\n",
      "iteration: 317200 loss: 0.0016 lr: 0.02\n",
      "iteration: 317300 loss: 0.0018 lr: 0.02\n",
      "iteration: 317400 loss: 0.0017 lr: 0.02\n",
      "iteration: 317500 loss: 0.0019 lr: 0.02\n",
      "iteration: 317600 loss: 0.0018 lr: 0.02\n",
      "iteration: 317700 loss: 0.0017 lr: 0.02\n",
      "iteration: 317800 loss: 0.0017 lr: 0.02\n",
      "iteration: 317900 loss: 0.0016 lr: 0.02\n",
      "iteration: 318000 loss: 0.0017 lr: 0.02\n",
      "iteration: 318100 loss: 0.0021 lr: 0.02\n",
      "iteration: 318200 loss: 0.0019 lr: 0.02\n",
      "iteration: 318300 loss: 0.0019 lr: 0.02\n",
      "iteration: 318400 loss: 0.0017 lr: 0.02\n",
      "iteration: 318500 loss: 0.0018 lr: 0.02\n",
      "iteration: 318600 loss: 0.0017 lr: 0.02\n",
      "iteration: 318700 loss: 0.0015 lr: 0.02\n",
      "iteration: 318800 loss: 0.0017 lr: 0.02\n",
      "iteration: 318900 loss: 0.0017 lr: 0.02\n",
      "iteration: 319000 loss: 0.0016 lr: 0.02\n",
      "iteration: 319100 loss: 0.0018 lr: 0.02\n",
      "iteration: 319200 loss: 0.0017 lr: 0.02\n",
      "iteration: 319300 loss: 0.0017 lr: 0.02\n",
      "iteration: 319400 loss: 0.0019 lr: 0.02\n",
      "iteration: 319500 loss: 0.0016 lr: 0.02\n",
      "iteration: 319600 loss: 0.0018 lr: 0.02\n",
      "iteration: 319700 loss: 0.0015 lr: 0.02\n",
      "iteration: 319800 loss: 0.0017 lr: 0.02\n",
      "iteration: 319900 loss: 0.0020 lr: 0.02\n",
      "iteration: 320000 loss: 0.0016 lr: 0.02\n",
      "iteration: 320100 loss: 0.0019 lr: 0.02\n",
      "iteration: 320200 loss: 0.0016 lr: 0.02\n",
      "iteration: 320300 loss: 0.0017 lr: 0.02\n",
      "iteration: 320400 loss: 0.0016 lr: 0.02\n",
      "iteration: 320500 loss: 0.0017 lr: 0.02\n",
      "iteration: 320600 loss: 0.0016 lr: 0.02\n",
      "iteration: 320700 loss: 0.0018 lr: 0.02\n",
      "iteration: 320800 loss: 0.0016 lr: 0.02\n",
      "iteration: 320900 loss: 0.0016 lr: 0.02\n",
      "iteration: 321000 loss: 0.0018 lr: 0.02\n",
      "iteration: 321100 loss: 0.0018 lr: 0.02\n",
      "iteration: 321200 loss: 0.0017 lr: 0.02\n",
      "iteration: 321300 loss: 0.0018 lr: 0.02\n",
      "iteration: 321400 loss: 0.0018 lr: 0.02\n",
      "iteration: 321500 loss: 0.0016 lr: 0.02\n",
      "iteration: 321600 loss: 0.0016 lr: 0.02\n",
      "iteration: 321700 loss: 0.0015 lr: 0.02\n",
      "iteration: 321800 loss: 0.0017 lr: 0.02\n",
      "iteration: 321900 loss: 0.0019 lr: 0.02\n",
      "iteration: 322000 loss: 0.0016 lr: 0.02\n",
      "iteration: 322100 loss: 0.0017 lr: 0.02\n",
      "iteration: 322200 loss: 0.0019 lr: 0.02\n",
      "iteration: 322300 loss: 0.0017 lr: 0.02\n",
      "iteration: 322400 loss: 0.0015 lr: 0.02\n",
      "iteration: 322500 loss: 0.0016 lr: 0.02\n",
      "iteration: 322600 loss: 0.0017 lr: 0.02\n",
      "iteration: 322700 loss: 0.0018 lr: 0.02\n",
      "iteration: 322800 loss: 0.0017 lr: 0.02\n",
      "iteration: 322900 loss: 0.0016 lr: 0.02\n",
      "iteration: 323000 loss: 0.0017 lr: 0.02\n",
      "iteration: 323100 loss: 0.0019 lr: 0.02\n",
      "iteration: 323200 loss: 0.0017 lr: 0.02\n",
      "iteration: 323300 loss: 0.0018 lr: 0.02\n",
      "iteration: 323400 loss: 0.0019 lr: 0.02\n",
      "iteration: 323500 loss: 0.0018 lr: 0.02\n",
      "iteration: 323600 loss: 0.0018 lr: 0.02\n",
      "iteration: 323700 loss: 0.0016 lr: 0.02\n",
      "iteration: 323800 loss: 0.0017 lr: 0.02\n",
      "iteration: 323900 loss: 0.0016 lr: 0.02\n",
      "iteration: 324000 loss: 0.0015 lr: 0.02\n",
      "iteration: 324100 loss: 0.0015 lr: 0.02\n",
      "iteration: 324200 loss: 0.0016 lr: 0.02\n",
      "iteration: 324300 loss: 0.0017 lr: 0.02\n",
      "iteration: 324400 loss: 0.0017 lr: 0.02\n",
      "iteration: 324500 loss: 0.0016 lr: 0.02\n",
      "iteration: 324600 loss: 0.0019 lr: 0.02\n",
      "iteration: 324700 loss: 0.0018 lr: 0.02\n",
      "iteration: 324800 loss: 0.0017 lr: 0.02\n",
      "iteration: 324900 loss: 0.0019 lr: 0.02\n",
      "iteration: 325000 loss: 0.0018 lr: 0.02\n",
      "iteration: 325100 loss: 0.0021 lr: 0.02\n",
      "iteration: 325200 loss: 0.0016 lr: 0.02\n",
      "iteration: 325300 loss: 0.0018 lr: 0.02\n",
      "iteration: 325400 loss: 0.0017 lr: 0.02\n",
      "iteration: 325500 loss: 0.0021 lr: 0.02\n",
      "iteration: 325600 loss: 0.0016 lr: 0.02\n",
      "iteration: 325700 loss: 0.0020 lr: 0.02\n",
      "iteration: 325800 loss: 0.0017 lr: 0.02\n",
      "iteration: 325900 loss: 0.0017 lr: 0.02\n",
      "iteration: 326000 loss: 0.0016 lr: 0.02\n",
      "iteration: 326100 loss: 0.0016 lr: 0.02\n",
      "iteration: 326200 loss: 0.0018 lr: 0.02\n",
      "iteration: 326300 loss: 0.0018 lr: 0.02\n",
      "iteration: 326400 loss: 0.0017 lr: 0.02\n",
      "iteration: 326500 loss: 0.0017 lr: 0.02\n",
      "iteration: 326600 loss: 0.0017 lr: 0.02\n",
      "iteration: 326700 loss: 0.0017 lr: 0.02\n",
      "iteration: 326800 loss: 0.0016 lr: 0.02\n",
      "iteration: 326900 loss: 0.0014 lr: 0.02\n",
      "iteration: 327000 loss: 0.0018 lr: 0.02\n",
      "iteration: 327100 loss: 0.0016 lr: 0.02\n",
      "iteration: 327200 loss: 0.0021 lr: 0.02\n",
      "iteration: 327300 loss: 0.0017 lr: 0.02\n",
      "iteration: 327400 loss: 0.0016 lr: 0.02\n",
      "iteration: 327500 loss: 0.0017 lr: 0.02\n",
      "iteration: 327600 loss: 0.0018 lr: 0.02\n",
      "iteration: 327700 loss: 0.0015 lr: 0.02\n",
      "iteration: 327800 loss: 0.0017 lr: 0.02\n",
      "iteration: 327900 loss: 0.0017 lr: 0.02\n",
      "iteration: 328000 loss: 0.0016 lr: 0.02\n",
      "iteration: 328100 loss: 0.0016 lr: 0.02\n",
      "iteration: 328200 loss: 0.0016 lr: 0.02\n",
      "iteration: 328300 loss: 0.0019 lr: 0.02\n",
      "iteration: 328400 loss: 0.0016 lr: 0.02\n",
      "iteration: 328500 loss: 0.0016 lr: 0.02\n",
      "iteration: 328600 loss: 0.0016 lr: 0.02\n",
      "iteration: 328700 loss: 0.0018 lr: 0.02\n",
      "iteration: 328800 loss: 0.0016 lr: 0.02\n",
      "iteration: 328900 loss: 0.0016 lr: 0.02\n",
      "iteration: 329000 loss: 0.0016 lr: 0.02\n",
      "iteration: 329100 loss: 0.0015 lr: 0.02\n",
      "iteration: 329200 loss: 0.0018 lr: 0.02\n",
      "iteration: 329300 loss: 0.0015 lr: 0.02\n",
      "iteration: 329400 loss: 0.0016 lr: 0.02\n",
      "iteration: 329500 loss: 0.0016 lr: 0.02\n",
      "iteration: 329600 loss: 0.0016 lr: 0.02\n",
      "iteration: 329700 loss: 0.0017 lr: 0.02\n",
      "iteration: 329800 loss: 0.0016 lr: 0.02\n",
      "iteration: 329900 loss: 0.0018 lr: 0.02\n",
      "iteration: 330000 loss: 0.0015 lr: 0.02\n",
      "iteration: 330100 loss: 0.0016 lr: 0.02\n",
      "iteration: 330200 loss: 0.0017 lr: 0.02\n",
      "iteration: 330300 loss: 0.0017 lr: 0.02\n",
      "iteration: 330400 loss: 0.0016 lr: 0.02\n",
      "iteration: 330500 loss: 0.0019 lr: 0.02\n",
      "iteration: 330600 loss: 0.0016 lr: 0.02\n",
      "iteration: 330700 loss: 0.0014 lr: 0.02\n",
      "iteration: 330800 loss: 0.0017 lr: 0.02\n",
      "iteration: 330900 loss: 0.0018 lr: 0.02\n",
      "iteration: 331000 loss: 0.0016 lr: 0.02\n",
      "iteration: 331100 loss: 0.0020 lr: 0.02\n",
      "iteration: 331200 loss: 0.0016 lr: 0.02\n",
      "iteration: 331300 loss: 0.0016 lr: 0.02\n",
      "iteration: 331400 loss: 0.0018 lr: 0.02\n",
      "iteration: 331500 loss: 0.0017 lr: 0.02\n",
      "iteration: 331600 loss: 0.0017 lr: 0.02\n",
      "iteration: 331700 loss: 0.0016 lr: 0.02\n",
      "iteration: 331800 loss: 0.0016 lr: 0.02\n",
      "iteration: 331900 loss: 0.0016 lr: 0.02\n",
      "iteration: 332000 loss: 0.0016 lr: 0.02\n",
      "iteration: 332100 loss: 0.0017 lr: 0.02\n",
      "iteration: 332200 loss: 0.0015 lr: 0.02\n",
      "iteration: 332300 loss: 0.0016 lr: 0.02\n",
      "iteration: 332400 loss: 0.0014 lr: 0.02\n",
      "iteration: 332500 loss: 0.0016 lr: 0.02\n",
      "iteration: 332600 loss: 0.0016 lr: 0.02\n",
      "iteration: 332700 loss: 0.0017 lr: 0.02\n",
      "iteration: 332800 loss: 0.0016 lr: 0.02\n",
      "iteration: 332900 loss: 0.0015 lr: 0.02\n",
      "iteration: 333000 loss: 0.0018 lr: 0.02\n",
      "iteration: 333100 loss: 0.0017 lr: 0.02\n",
      "iteration: 333200 loss: 0.0017 lr: 0.02\n",
      "iteration: 333300 loss: 0.0017 lr: 0.02\n",
      "iteration: 333400 loss: 0.0017 lr: 0.02\n",
      "iteration: 333500 loss: 0.0017 lr: 0.02\n",
      "iteration: 333600 loss: 0.0017 lr: 0.02\n",
      "iteration: 333700 loss: 0.0016 lr: 0.02\n",
      "iteration: 333800 loss: 0.0015 lr: 0.02\n",
      "iteration: 333900 loss: 0.0017 lr: 0.02\n",
      "iteration: 334000 loss: 0.0017 lr: 0.02\n",
      "iteration: 334100 loss: 0.0018 lr: 0.02\n",
      "iteration: 334200 loss: 0.0016 lr: 0.02\n",
      "iteration: 334300 loss: 0.0016 lr: 0.02\n",
      "iteration: 334400 loss: 0.0017 lr: 0.02\n",
      "iteration: 334500 loss: 0.0016 lr: 0.02\n",
      "iteration: 334600 loss: 0.0018 lr: 0.02\n",
      "iteration: 334700 loss: 0.0016 lr: 0.02\n",
      "iteration: 334800 loss: 0.0016 lr: 0.02\n",
      "iteration: 334900 loss: 0.0018 lr: 0.02\n",
      "iteration: 335000 loss: 0.0015 lr: 0.02\n",
      "iteration: 335100 loss: 0.0019 lr: 0.02\n",
      "iteration: 335200 loss: 0.0015 lr: 0.02\n",
      "iteration: 335300 loss: 0.0018 lr: 0.02\n",
      "iteration: 335400 loss: 0.0017 lr: 0.02\n",
      "iteration: 335500 loss: 0.0016 lr: 0.02\n",
      "iteration: 335600 loss: 0.0018 lr: 0.02\n",
      "iteration: 335700 loss: 0.0017 lr: 0.02\n",
      "iteration: 335800 loss: 0.0018 lr: 0.02\n",
      "iteration: 335900 loss: 0.0016 lr: 0.02\n",
      "iteration: 336000 loss: 0.0016 lr: 0.02\n",
      "iteration: 336100 loss: 0.0016 lr: 0.02\n",
      "iteration: 336200 loss: 0.0017 lr: 0.02\n",
      "iteration: 336300 loss: 0.0016 lr: 0.02\n",
      "iteration: 336400 loss: 0.0015 lr: 0.02\n",
      "iteration: 336500 loss: 0.0017 lr: 0.02\n",
      "iteration: 336600 loss: 0.0017 lr: 0.02\n",
      "iteration: 336700 loss: 0.0017 lr: 0.02\n",
      "iteration: 336800 loss: 0.0017 lr: 0.02\n",
      "iteration: 336900 loss: 0.0018 lr: 0.02\n",
      "iteration: 337000 loss: 0.0015 lr: 0.02\n",
      "iteration: 337100 loss: 0.0015 lr: 0.02\n",
      "iteration: 337200 loss: 0.0015 lr: 0.02\n",
      "iteration: 337300 loss: 0.0017 lr: 0.02\n",
      "iteration: 337400 loss: 0.0016 lr: 0.02\n",
      "iteration: 337500 loss: 0.0018 lr: 0.02\n",
      "iteration: 337600 loss: 0.0015 lr: 0.02\n",
      "iteration: 337700 loss: 0.0015 lr: 0.02\n",
      "iteration: 337800 loss: 0.0015 lr: 0.02\n",
      "iteration: 337900 loss: 0.0016 lr: 0.02\n",
      "iteration: 338000 loss: 0.0016 lr: 0.02\n",
      "iteration: 338100 loss: 0.0015 lr: 0.02\n",
      "iteration: 338200 loss: 0.0016 lr: 0.02\n",
      "iteration: 338300 loss: 0.0015 lr: 0.02\n",
      "iteration: 338400 loss: 0.0014 lr: 0.02\n",
      "iteration: 338500 loss: 0.0016 lr: 0.02\n",
      "iteration: 338600 loss: 0.0014 lr: 0.02\n",
      "iteration: 338700 loss: 0.0016 lr: 0.02\n",
      "iteration: 338800 loss: 0.0016 lr: 0.02\n",
      "iteration: 338900 loss: 0.0016 lr: 0.02\n",
      "iteration: 339000 loss: 0.0015 lr: 0.02\n",
      "iteration: 339100 loss: 0.0015 lr: 0.02\n",
      "iteration: 339200 loss: 0.0016 lr: 0.02\n",
      "iteration: 339300 loss: 0.0016 lr: 0.02\n",
      "iteration: 339400 loss: 0.0017 lr: 0.02\n",
      "iteration: 339500 loss: 0.0018 lr: 0.02\n",
      "iteration: 339600 loss: 0.0018 lr: 0.02\n",
      "iteration: 339700 loss: 0.0016 lr: 0.02\n",
      "iteration: 339800 loss: 0.0015 lr: 0.02\n",
      "iteration: 339900 loss: 0.0017 lr: 0.02\n",
      "iteration: 340000 loss: 0.0016 lr: 0.02\n",
      "iteration: 340100 loss: 0.0015 lr: 0.02\n",
      "iteration: 340200 loss: 0.0015 lr: 0.02\n",
      "iteration: 340300 loss: 0.0015 lr: 0.02\n",
      "iteration: 340400 loss: 0.0018 lr: 0.02\n",
      "iteration: 340500 loss: 0.0016 lr: 0.02\n",
      "iteration: 340600 loss: 0.0014 lr: 0.02\n",
      "iteration: 340700 loss: 0.0015 lr: 0.02\n",
      "iteration: 340800 loss: 0.0017 lr: 0.02\n",
      "iteration: 340900 loss: 0.0015 lr: 0.02\n",
      "iteration: 341000 loss: 0.0017 lr: 0.02\n",
      "iteration: 341100 loss: 0.0015 lr: 0.02\n",
      "iteration: 341200 loss: 0.0014 lr: 0.02\n",
      "iteration: 341300 loss: 0.0016 lr: 0.02\n",
      "iteration: 341400 loss: 0.0016 lr: 0.02\n",
      "iteration: 341500 loss: 0.0013 lr: 0.02\n",
      "iteration: 341600 loss: 0.0017 lr: 0.02\n",
      "iteration: 341700 loss: 0.0017 lr: 0.02\n",
      "iteration: 341800 loss: 0.0016 lr: 0.02\n",
      "iteration: 341900 loss: 0.0016 lr: 0.02\n",
      "iteration: 342000 loss: 0.0016 lr: 0.02\n",
      "iteration: 342100 loss: 0.0015 lr: 0.02\n",
      "iteration: 342200 loss: 0.0018 lr: 0.02\n",
      "iteration: 342300 loss: 0.0017 lr: 0.02\n",
      "iteration: 342400 loss: 0.0016 lr: 0.02\n",
      "iteration: 342500 loss: 0.0016 lr: 0.02\n",
      "iteration: 342600 loss: 0.0014 lr: 0.02\n",
      "iteration: 342700 loss: 0.0015 lr: 0.02\n",
      "iteration: 342800 loss: 0.0016 lr: 0.02\n",
      "iteration: 342900 loss: 0.0015 lr: 0.02\n",
      "iteration: 343000 loss: 0.0017 lr: 0.02\n",
      "iteration: 343100 loss: 0.0015 lr: 0.02\n",
      "iteration: 343200 loss: 0.0014 lr: 0.02\n",
      "iteration: 343300 loss: 0.0016 lr: 0.02\n",
      "iteration: 343400 loss: 0.0014 lr: 0.02\n",
      "iteration: 343500 loss: 0.0015 lr: 0.02\n",
      "iteration: 343600 loss: 0.0016 lr: 0.02\n",
      "iteration: 343700 loss: 0.0015 lr: 0.02\n",
      "iteration: 343800 loss: 0.0016 lr: 0.02\n",
      "iteration: 343900 loss: 0.0015 lr: 0.02\n",
      "iteration: 344000 loss: 0.0014 lr: 0.02\n",
      "iteration: 344100 loss: 0.0019 lr: 0.02\n",
      "iteration: 344200 loss: 0.0019 lr: 0.02\n",
      "iteration: 344300 loss: 0.0015 lr: 0.02\n",
      "iteration: 344400 loss: 0.0017 lr: 0.02\n",
      "iteration: 344500 loss: 0.0015 lr: 0.02\n",
      "iteration: 344600 loss: 0.0017 lr: 0.02\n",
      "iteration: 344700 loss: 0.0016 lr: 0.02\n",
      "iteration: 344800 loss: 0.0015 lr: 0.02\n",
      "iteration: 344900 loss: 0.0014 lr: 0.02\n",
      "iteration: 345000 loss: 0.0016 lr: 0.02\n",
      "iteration: 345100 loss: 0.0017 lr: 0.02\n",
      "iteration: 345200 loss: 0.0015 lr: 0.02\n",
      "iteration: 345300 loss: 0.0013 lr: 0.02\n",
      "iteration: 345400 loss: 0.0015 lr: 0.02\n",
      "iteration: 345500 loss: 0.0015 lr: 0.02\n",
      "iteration: 345600 loss: 0.0016 lr: 0.02\n",
      "iteration: 345700 loss: 0.0015 lr: 0.02\n",
      "iteration: 345800 loss: 0.0017 lr: 0.02\n",
      "iteration: 345900 loss: 0.0017 lr: 0.02\n",
      "iteration: 346000 loss: 0.0015 lr: 0.02\n",
      "iteration: 346100 loss: 0.0013 lr: 0.02\n",
      "iteration: 346200 loss: 0.0017 lr: 0.02\n",
      "iteration: 346300 loss: 0.0018 lr: 0.02\n",
      "iteration: 346400 loss: 0.0016 lr: 0.02\n",
      "iteration: 346500 loss: 0.0017 lr: 0.02\n",
      "iteration: 346600 loss: 0.0017 lr: 0.02\n",
      "iteration: 346700 loss: 0.0015 lr: 0.02\n",
      "iteration: 346800 loss: 0.0016 lr: 0.02\n",
      "iteration: 346900 loss: 0.0017 lr: 0.02\n",
      "iteration: 347000 loss: 0.0015 lr: 0.02\n",
      "iteration: 347100 loss: 0.0016 lr: 0.02\n",
      "iteration: 347200 loss: 0.0017 lr: 0.02\n",
      "iteration: 347300 loss: 0.0016 lr: 0.02\n",
      "iteration: 347400 loss: 0.0016 lr: 0.02\n",
      "iteration: 347500 loss: 0.0016 lr: 0.02\n",
      "iteration: 347600 loss: 0.0017 lr: 0.02\n",
      "iteration: 347700 loss: 0.0015 lr: 0.02\n",
      "iteration: 347800 loss: 0.0016 lr: 0.02\n",
      "iteration: 347900 loss: 0.0016 lr: 0.02\n",
      "iteration: 348000 loss: 0.0015 lr: 0.02\n",
      "iteration: 348100 loss: 0.0015 lr: 0.02\n",
      "iteration: 348200 loss: 0.0016 lr: 0.02\n",
      "iteration: 348300 loss: 0.0016 lr: 0.02\n",
      "iteration: 348400 loss: 0.0015 lr: 0.02\n",
      "iteration: 348500 loss: 0.0015 lr: 0.02\n",
      "iteration: 348600 loss: 0.0014 lr: 0.02\n",
      "iteration: 348700 loss: 0.0015 lr: 0.02\n",
      "iteration: 348800 loss: 0.0017 lr: 0.02\n",
      "iteration: 348900 loss: 0.0015 lr: 0.02\n",
      "iteration: 349000 loss: 0.0015 lr: 0.02\n",
      "iteration: 349100 loss: 0.0015 lr: 0.02\n",
      "iteration: 349200 loss: 0.0016 lr: 0.02\n",
      "iteration: 349300 loss: 0.0017 lr: 0.02\n",
      "iteration: 349400 loss: 0.0016 lr: 0.02\n",
      "iteration: 349500 loss: 0.0016 lr: 0.02\n",
      "iteration: 349600 loss: 0.0015 lr: 0.02\n",
      "iteration: 349700 loss: 0.0014 lr: 0.02\n",
      "iteration: 349800 loss: 0.0014 lr: 0.02\n",
      "iteration: 349900 loss: 0.0016 lr: 0.02\n",
      "iteration: 350000 loss: 0.0015 lr: 0.02\n",
      "iteration: 350100 loss: 0.0016 lr: 0.02\n",
      "iteration: 350200 loss: 0.0016 lr: 0.02\n",
      "iteration: 350300 loss: 0.0017 lr: 0.02\n",
      "iteration: 350400 loss: 0.0015 lr: 0.02\n",
      "iteration: 350500 loss: 0.0016 lr: 0.02\n",
      "iteration: 350600 loss: 0.0016 lr: 0.02\n",
      "iteration: 350700 loss: 0.0015 lr: 0.02\n",
      "iteration: 350800 loss: 0.0016 lr: 0.02\n",
      "iteration: 350900 loss: 0.0015 lr: 0.02\n",
      "iteration: 351000 loss: 0.0015 lr: 0.02\n",
      "iteration: 351100 loss: 0.0016 lr: 0.02\n",
      "iteration: 351200 loss: 0.0016 lr: 0.02\n",
      "iteration: 351300 loss: 0.0016 lr: 0.02\n",
      "iteration: 351400 loss: 0.0014 lr: 0.02\n",
      "iteration: 351500 loss: 0.0015 lr: 0.02\n",
      "iteration: 351600 loss: 0.0015 lr: 0.02\n",
      "iteration: 351700 loss: 0.0017 lr: 0.02\n",
      "iteration: 351800 loss: 0.0015 lr: 0.02\n",
      "iteration: 351900 loss: 0.0017 lr: 0.02\n",
      "iteration: 352000 loss: 0.0016 lr: 0.02\n",
      "iteration: 352100 loss: 0.0018 lr: 0.02\n",
      "iteration: 352200 loss: 0.0016 lr: 0.02\n",
      "iteration: 352300 loss: 0.0015 lr: 0.02\n",
      "iteration: 352400 loss: 0.0016 lr: 0.02\n",
      "iteration: 352500 loss: 0.0014 lr: 0.02\n",
      "iteration: 352600 loss: 0.0015 lr: 0.02\n",
      "iteration: 352700 loss: 0.0016 lr: 0.02\n",
      "iteration: 352800 loss: 0.0017 lr: 0.02\n",
      "iteration: 352900 loss: 0.0015 lr: 0.02\n",
      "iteration: 353000 loss: 0.0016 lr: 0.02\n",
      "iteration: 353100 loss: 0.0016 lr: 0.02\n",
      "iteration: 353200 loss: 0.0016 lr: 0.02\n",
      "iteration: 353300 loss: 0.0016 lr: 0.02\n",
      "iteration: 353400 loss: 0.0015 lr: 0.02\n",
      "iteration: 353500 loss: 0.0014 lr: 0.02\n",
      "iteration: 353600 loss: 0.0014 lr: 0.02\n",
      "iteration: 353700 loss: 0.0016 lr: 0.02\n",
      "iteration: 353800 loss: 0.0016 lr: 0.02\n",
      "iteration: 353900 loss: 0.0015 lr: 0.02\n",
      "iteration: 354000 loss: 0.0014 lr: 0.02\n",
      "iteration: 354100 loss: 0.0014 lr: 0.02\n",
      "iteration: 354200 loss: 0.0016 lr: 0.02\n",
      "iteration: 354300 loss: 0.0013 lr: 0.02\n",
      "iteration: 354400 loss: 0.0015 lr: 0.02\n",
      "iteration: 354500 loss: 0.0014 lr: 0.02\n",
      "iteration: 354600 loss: 0.0019 lr: 0.02\n",
      "iteration: 354700 loss: 0.0016 lr: 0.02\n",
      "iteration: 354800 loss: 0.0015 lr: 0.02\n",
      "iteration: 354900 loss: 0.0016 lr: 0.02\n",
      "iteration: 355000 loss: 0.0016 lr: 0.02\n",
      "iteration: 355100 loss: 0.0014 lr: 0.02\n",
      "iteration: 355200 loss: 0.0015 lr: 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplayiters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveiters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/training.py:284\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning)\u001b[0m\n\u001b[1;32m    273\u001b[0m         train(\n\u001b[1;32m    274\u001b[0m             \u001b[38;5;28mstr\u001b[39m(poseconfigfile),\n\u001b[1;32m    275\u001b[0m             displayiters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m             allow_growth\u001b[38;5;241m=\u001b[39mallow_growth,\n\u001b[1;32m    281\u001b[0m         )  \u001b[38;5;66;03m# pass on path and file name for pose_cfg.yaml!\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;28mstr\u001b[39m(start_path))\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/training.py:273\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeeplabcut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose_estimation_tensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelecting single-animal trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mposeconfigfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisplayiters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43msaveiters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmaxiters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_snapshots_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeepdeconvweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdeconvweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass on path and file name for pose_cfg.yaml!\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/core/train.py:287\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    284\u001b[0m     current_lr \u001b[38;5;241m=\u001b[39m lr_gen\u001b[38;5;241m.\u001b[39mget_lr(it \u001b[38;5;241m-\u001b[39m start_iter)\n\u001b[1;32m    285\u001b[0m     lr_dict \u001b[38;5;241m=\u001b[39m {learning_rate: current_lr}\n\u001b[0;32m--> 287\u001b[0m [_, loss_val, summary] \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_summaries\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_dict\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m cum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_val\n\u001b[1;32m    291\u001b[0m train_writer\u001b[38;5;241m.\u001b[39madd_summary(summary, it)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path, shuffle=1, displayiters=100, saveiters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FESNewCameraApr19/FESNewCamera_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/dlc-models/iteration-0/FESNewCameraApr19-trainset95shuffle1/test/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FESNewCameraApr19/FESNewCamera_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 2000,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FESNewCameraApr19/Documentation_data-FESNewCamera_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/dlc-models/iteration-0/FESNewCameraApr19-trainset95shuffle1/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "/home/jakejoseph/anaconda3/envs/DLCGPU/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet50_FESNewCameraApr19shuffle1_50000  with # of training iterations: 50000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:19,  1.03it/s]\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/pose_estimation_tensorflow/core/evaluate.py:930: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  DataMachine.to_hdf(resultsfilename, \"df_with_missing\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-50000\n",
      "Results for 50000  training iterations: 95 1 train error: 2.36 pixels. Test error: 28.35  pixels.\n",
      "With pcutoff of 0.6  train error: 2.36 pixels. Test error: 33.95 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  DataCombined[loopscorer][bp][\"y\"][imagenr]\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  + DataCombined[loopscorer][bp][\"x\"][imagenr]\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  int(DataCombined[loopscorer][bp][\"y\"][imagenr]),\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  int(DataCombined[loopscorer][bp][\"x\"][imagenr]),\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = DataCombined[loopscorer][bp][\"likelihood\"][imagenr]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2048x1536 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(config_path,Shuffles=[1], plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-2000 for model /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-0/FESFatigueMay31-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2024-06-03 10:17:06.392409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-03 10:17:06.392612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.392653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.392684: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.392715: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.413813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.413878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.413883: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-06-03 10:17:06.414816: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 10:17:06.429121: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  /home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/videos/Individual Motor Points top 03-05.mp4\n",
      "Loading  /home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/videos/Individual Motor Points top 03-05.mp4\n",
      "Duration of video [s]:  2355.87 , recorded with  30.0 fps!\n",
      "Overall # of frames:  70676  found with (before cropping) frame dimensions:  2048 1536\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2024/70676 [31:23<21:18:04,  1.12s/it]"
     ]
    }
   ],
   "source": [
    "video = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/ECRB_interleaved_stim_5_30_1.mp4'\n",
    "config_path = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/config.yaml'\n",
    "deeplabcut.analyze_videos(config_path, video, shuffle=1, save_as_csv=True, videotype='mp4')\n",
    "# deeplabcut.extract_outlier_frames(config_path, video,outlieralgorithm='uncertain',p_bound=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets/iteration-2/UnaugmentedDataSet_FESFatigueMay31/FESFatigue_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-2/FESFatigueMay31-trainset95shuffle1/test/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "/home/jakejoseph/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-355000 for model /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-2/FESFatigueMay31-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 15:17:52.787364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-27 15:17:52.788692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-27 15:17:52.789819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-27 15:17:52.790940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-27 15:17:52.792043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-27 15:17:52.793208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9789 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueEDC24_6_20.mp4\n",
      "Loading  /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueEDC24_6_20.mp4\n",
      "Duration of video [s]:  1248.6 , recorded with  30.0 fps!\n",
      "Overall # of frames:  37458  found with (before cropping) frame dimensions:  2048 1536\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37458/37458 [42:46<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "Starting to process video: /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueEDC24_6_20.mp4\n",
      "Loading /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueEDC24_6_20.mp4 and data.\n",
      "Duration of video [s]: 1248.6, recorded with 30.0 fps!\n",
      "Overall # of frames: 37458 with cropped frame dimensions: 2048 1536\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37458/37458 [08:52<00:00, 70.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue_6_13_2024_2.mp4',\n",
    "           '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueadditional_6_13.mp4',\n",
    "           '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue2_6_13.mp4']\n",
    "\n",
    "video = \"/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueEDC24_6_20.mp4\"\n",
    "deeplabcut.analyze_videos(config_path, video, shuffle=1, save_as_csv=True, videotype='mp4')\n",
    "deeplabcut.create_labeled_video(config_path, video, videotype = 'mp4', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new_video = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/fatiguetest0523ecrb12_2.mp4'\n",
    "# videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue_6_13_2024_2.mp4',\n",
    "#            '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueadditional_6_13.mp4',\n",
    "#            '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue2_6_13.mp4']\n",
    "# original_videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/ECRB_interleaved_stim_5_30_1.mp4',\n",
    "#                     '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/fatiguetest0523ecrb12_2.mp4',\n",
    "                    # '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4']\n",
    "# deeplabcut.extract_outlier_frames(config_path, video, automatic=True)\n",
    "deeplabcut.refine_labels(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 2.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([ 38, 111,  81, 153, 188, 119, 124,  89,  63, 158, 185,   2,  99,\n",
       "          125, 182, 187,  71, 190,  40, 197, 207, 173, 100,  28, 121,  23,\n",
       "          146, 109,  29,   5,  91, 106,  46, 181,  26, 103,  69,  18,  27,\n",
       "            6, 110, 151, 122, 144, 172, 194, 167,  16, 113,  87,  33, 117,\n",
       "          162,  57, 169, 165,  11,  47,  39,  74, 178,  85,  79, 136,  13,\n",
       "           62,  20,  78, 148,  84,  42,   9, 108, 152,  94, 183,  34,  10,\n",
       "            8, 154, 184, 155,  72,  68, 130, 192, 161,  15,  75,  70, 104,\n",
       "           54, 139, 195, 132,  67, 171, 142, 176,  14, 193, 186,  53,  31,\n",
       "          202,  32,  80, 123,  65, 131, 114, 107,  45,  60, 166,  98, 189,\n",
       "           48, 164,  88,  21,  86,  36, 200, 198,  59, 133, 191, 143,  35,\n",
       "          160, 102,  97, 199,   4, 174,  96,  90, 118,  64, 126,  51,  50,\n",
       "          206, 120, 205, 204, 177, 170,  25, 140,  58,  22,  12, 129, 163,\n",
       "           56, 138,  49,  76,  17,   1, 150,  19, 168,  73, 135,  83,  44,\n",
       "          112, 101, 105,  30, 201, 128, 180, 179,   3, 115,  92, 145,  43,\n",
       "          208, 116,  95, 149, 137,  77, 175, 159,  93, 127,  41,  55, 156,\n",
       "          134,   0, 147]),\n",
       "   array([ 37, 203,  52,   7,  82,  61, 196,  24, 141, 157,  66])))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.merge_datasets(config_path)\n",
    "deeplabcut.create_training_dataset(config_path, net_type='resnet_50', augmenter_type='imgaug')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLCGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
