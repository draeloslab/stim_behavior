{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project \"/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-06-03\" already exists!\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "\n",
    "config_path = deeplabcut.create_new_project(\n",
    "    'FESFatigue', \n",
    "    'Jake', \n",
    "    ['/home/jakejoseph/Desktop/Joseph_Code/FESNewCameraclips-Jake-2024-05-05/videos/fatiguetest0523ecrb12_2.mp4'], \n",
    "    working_directory='/home/jakejoseph/Desktop/Joseph_Code/', \n",
    "    copy_videos=True, \n",
    "    multianimal=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 14:04:40.190634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 14:04:40.295018: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-10 14:04:40.311565: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-10 14:04:40.611285: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-06-10 14:04:40.611321: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2024-06-10 14:04:40.611324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.10...\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "config_path = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/config.yaml'\n",
    "# deeplabcut.add_new_videos(config_path, ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/ECRB_interleaved_stim_5_30_1.mp4','/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4'])\n",
    "# deeplabcut.extract_frames(config_path, mode='automatic', algo='kmeans', userfeedback=False, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([48, 74, 59, 54, 34, 26, 68, 33, 45, 69, 27, 60,  3, 42, 43,  7, 22,\n",
       "          41, 50, 38, 61, 53, 62, 56, 72,  6, 52, 70,  4, 30, 49,  2, 28, 11,\n",
       "          23, 10, 31, 40, 57,  1, 32, 66, 14, 76, 19, 29, 63, 35, 18,  0, 75,\n",
       "          15,  5, 55, 16, 51, 20, 71,  8, 13, 25, 37, 17, 24, 46, 39, 65, 58,\n",
       "          12, 36, 21,  9, 73]),\n",
       "   array([67, 64, 47, 44])))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.create_training_dataset(config_path, augmenter_type='imgaug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-1/UnaugmentedDataSet_FESFatigueMay31/FESFatigue_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-0/FESFatigueMay31-trainset95shuffle1/train/snapshot-37000',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 2000,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-1/UnaugmentedDataSet_FESFatigueMay31/Documentation_data-FESFatigue_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-1/FESFatigueMay31-trainset95shuffle1/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Batch Size is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakejoseph/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading already trained DLC with backbone: resnet_50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 15:38:19.402973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-14 15:38:19.406330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-14 15:38:19.408041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-14 15:38:19.409699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-14 15:38:19.411567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-14 15:38:19.413142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9729 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display_iters overwritten as 100\n",
      "Save_iters overwritten as 1000\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-1/FESFatigueMay31-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4]], 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-1/UnaugmentedDataSet_FESFatigueMay31/FESFatigue_Jake95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-0/FESFatigueMay31-trainset95shuffle1/train/snapshot-37000', 'lr_init': 0.0005, 'max_input_size': 2000, 'metadataset': 'training-datasets/iteration-1/UnaugmentedDataSet_FESFatigueMay31/Documentation_data-FESFatigue_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 5, 'pos_dist_thresh': 17, 'project_path': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 37100 loss: 0.0066 lr: 0.005\n",
      "iteration: 37200 loss: 0.0066 lr: 0.005\n",
      "iteration: 37300 loss: 0.0058 lr: 0.005\n",
      "iteration: 37400 loss: 0.0062 lr: 0.005\n",
      "iteration: 37500 loss: 0.0069 lr: 0.005\n",
      "iteration: 37600 loss: 0.0058 lr: 0.005\n",
      "iteration: 37700 loss: 0.0058 lr: 0.005\n",
      "iteration: 37800 loss: 0.0066 lr: 0.005\n",
      "iteration: 37900 loss: 0.0057 lr: 0.005\n",
      "iteration: 38000 loss: 0.0059 lr: 0.005\n",
      "iteration: 38100 loss: 0.0051 lr: 0.005\n",
      "iteration: 38200 loss: 0.0048 lr: 0.005\n",
      "iteration: 38300 loss: 0.0054 lr: 0.005\n",
      "iteration: 38400 loss: 0.0049 lr: 0.005\n",
      "iteration: 38500 loss: 0.0051 lr: 0.005\n",
      "iteration: 38600 loss: 0.0050 lr: 0.005\n",
      "iteration: 38700 loss: 0.0057 lr: 0.005\n",
      "iteration: 38800 loss: 0.0046 lr: 0.005\n",
      "iteration: 38900 loss: 0.0051 lr: 0.005\n",
      "iteration: 39000 loss: 0.0049 lr: 0.005\n",
      "iteration: 39100 loss: 0.0046 lr: 0.005\n",
      "iteration: 39200 loss: 0.0048 lr: 0.005\n",
      "iteration: 39300 loss: 0.0054 lr: 0.005\n",
      "iteration: 39400 loss: 0.0045 lr: 0.005\n",
      "iteration: 39500 loss: 0.0050 lr: 0.005\n",
      "iteration: 39600 loss: 0.0054 lr: 0.005\n",
      "iteration: 39700 loss: 0.0053 lr: 0.005\n",
      "iteration: 39800 loss: 0.0054 lr: 0.005\n",
      "iteration: 39900 loss: 0.0049 lr: 0.005\n",
      "iteration: 40000 loss: 0.0041 lr: 0.005\n",
      "iteration: 40100 loss: 0.0052 lr: 0.005\n",
      "iteration: 40200 loss: 0.0052 lr: 0.005\n",
      "iteration: 40300 loss: 0.0047 lr: 0.005\n",
      "iteration: 40400 loss: 0.0048 lr: 0.005\n",
      "iteration: 40500 loss: 0.0047 lr: 0.005\n",
      "iteration: 40600 loss: 0.0052 lr: 0.005\n",
      "iteration: 40700 loss: 0.0046 lr: 0.005\n",
      "iteration: 40800 loss: 0.0043 lr: 0.005\n",
      "iteration: 40900 loss: 0.0044 lr: 0.005\n",
      "iteration: 41000 loss: 0.0047 lr: 0.005\n",
      "iteration: 41100 loss: 0.0044 lr: 0.005\n",
      "iteration: 41200 loss: 0.0046 lr: 0.005\n",
      "iteration: 41300 loss: 0.0048 lr: 0.005\n",
      "iteration: 41400 loss: 0.0043 lr: 0.005\n",
      "iteration: 41500 loss: 0.0047 lr: 0.005\n",
      "iteration: 41600 loss: 0.0041 lr: 0.005\n",
      "iteration: 41700 loss: 0.0053 lr: 0.005\n",
      "iteration: 41800 loss: 0.0052 lr: 0.005\n",
      "iteration: 41900 loss: 0.0057 lr: 0.005\n",
      "iteration: 42000 loss: 0.0045 lr: 0.005\n",
      "iteration: 42100 loss: 0.0049 lr: 0.005\n",
      "iteration: 42200 loss: 0.0044 lr: 0.005\n",
      "iteration: 42300 loss: 0.0045 lr: 0.005\n",
      "iteration: 42400 loss: 0.0044 lr: 0.005\n",
      "iteration: 42500 loss: 0.0054 lr: 0.005\n",
      "iteration: 42600 loss: 0.0051 lr: 0.005\n",
      "iteration: 42700 loss: 0.0048 lr: 0.005\n",
      "iteration: 42800 loss: 0.0048 lr: 0.005\n",
      "iteration: 42900 loss: 0.0048 lr: 0.005\n",
      "iteration: 43000 loss: 0.0046 lr: 0.005\n",
      "iteration: 43100 loss: 0.0042 lr: 0.005\n",
      "iteration: 43200 loss: 0.0044 lr: 0.005\n",
      "iteration: 43300 loss: 0.0047 lr: 0.005\n",
      "iteration: 43400 loss: 0.0047 lr: 0.005\n",
      "iteration: 43500 loss: 0.0041 lr: 0.005\n",
      "iteration: 43600 loss: 0.0038 lr: 0.005\n",
      "iteration: 43700 loss: 0.0044 lr: 0.005\n",
      "iteration: 43800 loss: 0.0043 lr: 0.005\n",
      "iteration: 43900 loss: 0.0042 lr: 0.005\n",
      "iteration: 44000 loss: 0.0045 lr: 0.005\n",
      "iteration: 44100 loss: 0.0047 lr: 0.005\n",
      "iteration: 44200 loss: 0.0043 lr: 0.005\n",
      "iteration: 44300 loss: 0.0046 lr: 0.005\n",
      "iteration: 44400 loss: 0.0045 lr: 0.005\n",
      "iteration: 44500 loss: 0.0039 lr: 0.005\n",
      "iteration: 44600 loss: 0.0048 lr: 0.005\n",
      "iteration: 44700 loss: 0.0041 lr: 0.005\n",
      "iteration: 44800 loss: 0.0044 lr: 0.005\n",
      "iteration: 44900 loss: 0.0047 lr: 0.005\n",
      "iteration: 45000 loss: 0.0040 lr: 0.005\n",
      "iteration: 45100 loss: 0.0039 lr: 0.005\n",
      "iteration: 45200 loss: 0.0041 lr: 0.005\n",
      "iteration: 45300 loss: 0.0043 lr: 0.005\n",
      "iteration: 45400 loss: 0.0040 lr: 0.005\n",
      "iteration: 45500 loss: 0.0046 lr: 0.005\n",
      "iteration: 45600 loss: 0.0043 lr: 0.005\n",
      "iteration: 45700 loss: 0.0035 lr: 0.005\n",
      "iteration: 45800 loss: 0.0040 lr: 0.005\n",
      "iteration: 45900 loss: 0.0041 lr: 0.005\n",
      "iteration: 46000 loss: 0.0044 lr: 0.005\n",
      "iteration: 46100 loss: 0.0043 lr: 0.005\n",
      "iteration: 46200 loss: 0.0042 lr: 0.005\n",
      "iteration: 46300 loss: 0.0040 lr: 0.005\n",
      "iteration: 46400 loss: 0.0043 lr: 0.005\n",
      "iteration: 46500 loss: 0.0043 lr: 0.005\n",
      "iteration: 46600 loss: 0.0044 lr: 0.005\n",
      "iteration: 46700 loss: 0.0039 lr: 0.005\n",
      "iteration: 46800 loss: 0.0039 lr: 0.005\n",
      "iteration: 46900 loss: 0.0046 lr: 0.005\n",
      "iteration: 47000 loss: 0.0044 lr: 0.005\n",
      "iteration: 47100 loss: 0.0054 lr: 0.02\n",
      "iteration: 47200 loss: 0.0051 lr: 0.02\n",
      "iteration: 47300 loss: 0.0051 lr: 0.02\n",
      "iteration: 47400 loss: 0.0047 lr: 0.02\n",
      "iteration: 47500 loss: 0.0054 lr: 0.02\n",
      "iteration: 47600 loss: 0.0047 lr: 0.02\n",
      "iteration: 47700 loss: 0.0051 lr: 0.02\n",
      "iteration: 47800 loss: 0.0050 lr: 0.02\n",
      "iteration: 47900 loss: 0.0041 lr: 0.02\n",
      "iteration: 48000 loss: 0.0049 lr: 0.02\n",
      "iteration: 48100 loss: 0.0051 lr: 0.02\n",
      "iteration: 48200 loss: 0.0054 lr: 0.02\n",
      "iteration: 48300 loss: 0.0050 lr: 0.02\n",
      "iteration: 48400 loss: 0.0049 lr: 0.02\n",
      "iteration: 48500 loss: 0.0044 lr: 0.02\n",
      "iteration: 48600 loss: 0.0050 lr: 0.02\n",
      "iteration: 48700 loss: 0.0051 lr: 0.02\n",
      "iteration: 48800 loss: 0.0046 lr: 0.02\n",
      "iteration: 48900 loss: 0.0050 lr: 0.02\n",
      "iteration: 49000 loss: 0.0053 lr: 0.02\n",
      "iteration: 49100 loss: 0.0048 lr: 0.02\n",
      "iteration: 49200 loss: 0.0045 lr: 0.02\n",
      "iteration: 49300 loss: 0.0047 lr: 0.02\n",
      "iteration: 49400 loss: 0.0046 lr: 0.02\n",
      "iteration: 49500 loss: 0.0047 lr: 0.02\n",
      "iteration: 49600 loss: 0.0047 lr: 0.02\n",
      "iteration: 49700 loss: 0.0047 lr: 0.02\n",
      "iteration: 49800 loss: 0.0050 lr: 0.02\n",
      "iteration: 49900 loss: 0.0049 lr: 0.02\n",
      "iteration: 50000 loss: 0.0049 lr: 0.02\n",
      "iteration: 50100 loss: 0.0045 lr: 0.02\n",
      "iteration: 50200 loss: 0.0051 lr: 0.02\n",
      "iteration: 50300 loss: 0.0044 lr: 0.02\n",
      "iteration: 50400 loss: 0.0047 lr: 0.02\n",
      "iteration: 50500 loss: 0.0044 lr: 0.02\n",
      "iteration: 50600 loss: 0.0043 lr: 0.02\n",
      "iteration: 50700 loss: 0.0047 lr: 0.02\n",
      "iteration: 50800 loss: 0.0050 lr: 0.02\n",
      "iteration: 50900 loss: 0.0046 lr: 0.02\n",
      "iteration: 51000 loss: 0.0047 lr: 0.02\n",
      "iteration: 51100 loss: 0.0044 lr: 0.02\n",
      "iteration: 51200 loss: 0.0043 lr: 0.02\n",
      "iteration: 51300 loss: 0.0044 lr: 0.02\n",
      "iteration: 51400 loss: 0.0052 lr: 0.02\n",
      "iteration: 51500 loss: 0.0047 lr: 0.02\n",
      "iteration: 51600 loss: 0.0046 lr: 0.02\n",
      "iteration: 51700 loss: 0.0042 lr: 0.02\n",
      "iteration: 51800 loss: 0.0045 lr: 0.02\n",
      "iteration: 51900 loss: 0.0046 lr: 0.02\n",
      "iteration: 52000 loss: 0.0046 lr: 0.02\n",
      "iteration: 52100 loss: 0.0045 lr: 0.02\n",
      "iteration: 52200 loss: 0.0042 lr: 0.02\n",
      "iteration: 52300 loss: 0.0046 lr: 0.02\n",
      "iteration: 52400 loss: 0.0042 lr: 0.02\n",
      "iteration: 52500 loss: 0.0044 lr: 0.02\n",
      "iteration: 52600 loss: 0.0046 lr: 0.02\n",
      "iteration: 52700 loss: 0.0040 lr: 0.02\n",
      "iteration: 52800 loss: 0.0046 lr: 0.02\n",
      "iteration: 52900 loss: 0.0043 lr: 0.02\n",
      "iteration: 53000 loss: 0.0044 lr: 0.02\n",
      "iteration: 53100 loss: 0.0043 lr: 0.02\n",
      "iteration: 53200 loss: 0.0044 lr: 0.02\n",
      "iteration: 53300 loss: 0.0041 lr: 0.02\n",
      "iteration: 53400 loss: 0.0043 lr: 0.02\n",
      "iteration: 53500 loss: 0.0045 lr: 0.02\n",
      "iteration: 53600 loss: 0.0046 lr: 0.02\n",
      "iteration: 53700 loss: 0.0044 lr: 0.02\n",
      "iteration: 53800 loss: 0.0044 lr: 0.02\n",
      "iteration: 53900 loss: 0.0046 lr: 0.02\n",
      "iteration: 54000 loss: 0.0044 lr: 0.02\n",
      "iteration: 54100 loss: 0.0045 lr: 0.02\n",
      "iteration: 54200 loss: 0.0045 lr: 0.02\n",
      "iteration: 54300 loss: 0.0042 lr: 0.02\n",
      "iteration: 54400 loss: 0.0043 lr: 0.02\n",
      "iteration: 54500 loss: 0.0038 lr: 0.02\n",
      "iteration: 54600 loss: 0.0042 lr: 0.02\n",
      "iteration: 54700 loss: 0.0042 lr: 0.02\n",
      "iteration: 54800 loss: 0.0047 lr: 0.02\n",
      "iteration: 54900 loss: 0.0045 lr: 0.02\n",
      "iteration: 55000 loss: 0.0037 lr: 0.02\n",
      "iteration: 55100 loss: 0.0046 lr: 0.02\n",
      "iteration: 55200 loss: 0.0044 lr: 0.02\n",
      "iteration: 55300 loss: 0.0044 lr: 0.02\n",
      "iteration: 55400 loss: 0.0040 lr: 0.02\n",
      "iteration: 55500 loss: 0.0043 lr: 0.02\n",
      "iteration: 55600 loss: 0.0045 lr: 0.02\n",
      "iteration: 55700 loss: 0.0044 lr: 0.02\n",
      "iteration: 55800 loss: 0.0045 lr: 0.02\n",
      "iteration: 55900 loss: 0.0041 lr: 0.02\n",
      "iteration: 56000 loss: 0.0039 lr: 0.02\n",
      "iteration: 56100 loss: 0.0040 lr: 0.02\n",
      "iteration: 56200 loss: 0.0046 lr: 0.02\n",
      "iteration: 56300 loss: 0.0041 lr: 0.02\n",
      "iteration: 56400 loss: 0.0041 lr: 0.02\n",
      "iteration: 56500 loss: 0.0050 lr: 0.02\n",
      "iteration: 56600 loss: 0.0042 lr: 0.02\n",
      "iteration: 56700 loss: 0.0042 lr: 0.02\n",
      "iteration: 56800 loss: 0.0045 lr: 0.02\n",
      "iteration: 56900 loss: 0.0041 lr: 0.02\n",
      "iteration: 57000 loss: 0.0041 lr: 0.02\n",
      "iteration: 57100 loss: 0.0041 lr: 0.02\n",
      "iteration: 57200 loss: 0.0041 lr: 0.02\n",
      "iteration: 57300 loss: 0.0041 lr: 0.02\n",
      "iteration: 57400 loss: 0.0039 lr: 0.02\n",
      "iteration: 57500 loss: 0.0043 lr: 0.02\n",
      "iteration: 57600 loss: 0.0042 lr: 0.02\n",
      "iteration: 57700 loss: 0.0039 lr: 0.02\n",
      "iteration: 57800 loss: 0.0039 lr: 0.02\n",
      "iteration: 57900 loss: 0.0041 lr: 0.02\n",
      "iteration: 58000 loss: 0.0040 lr: 0.02\n",
      "iteration: 58100 loss: 0.0044 lr: 0.02\n",
      "iteration: 58200 loss: 0.0035 lr: 0.02\n",
      "iteration: 58300 loss: 0.0039 lr: 0.02\n",
      "iteration: 58400 loss: 0.0040 lr: 0.02\n",
      "iteration: 58500 loss: 0.0041 lr: 0.02\n",
      "iteration: 58600 loss: 0.0037 lr: 0.02\n",
      "iteration: 58700 loss: 0.0041 lr: 0.02\n",
      "iteration: 58800 loss: 0.0044 lr: 0.02\n",
      "iteration: 58900 loss: 0.0043 lr: 0.02\n",
      "iteration: 59000 loss: 0.0043 lr: 0.02\n",
      "iteration: 59100 loss: 0.0042 lr: 0.02\n",
      "iteration: 59200 loss: 0.0039 lr: 0.02\n",
      "iteration: 59300 loss: 0.0042 lr: 0.02\n",
      "iteration: 59400 loss: 0.0036 lr: 0.02\n",
      "iteration: 59500 loss: 0.0037 lr: 0.02\n",
      "iteration: 59600 loss: 0.0040 lr: 0.02\n",
      "iteration: 59700 loss: 0.0038 lr: 0.02\n",
      "iteration: 59800 loss: 0.0038 lr: 0.02\n",
      "iteration: 59900 loss: 0.0038 lr: 0.02\n",
      "iteration: 60000 loss: 0.0042 lr: 0.02\n",
      "iteration: 60100 loss: 0.0037 lr: 0.02\n",
      "iteration: 60200 loss: 0.0038 lr: 0.02\n",
      "iteration: 60300 loss: 0.0039 lr: 0.02\n",
      "iteration: 60400 loss: 0.0036 lr: 0.02\n",
      "iteration: 60500 loss: 0.0043 lr: 0.02\n",
      "iteration: 60600 loss: 0.0037 lr: 0.02\n",
      "iteration: 60700 loss: 0.0038 lr: 0.02\n",
      "iteration: 60800 loss: 0.0038 lr: 0.02\n",
      "iteration: 60900 loss: 0.0041 lr: 0.02\n",
      "iteration: 61000 loss: 0.0039 lr: 0.02\n",
      "iteration: 61100 loss: 0.0039 lr: 0.02\n",
      "iteration: 61200 loss: 0.0038 lr: 0.02\n",
      "iteration: 61300 loss: 0.0039 lr: 0.02\n",
      "iteration: 61400 loss: 0.0039 lr: 0.02\n",
      "iteration: 61500 loss: 0.0037 lr: 0.02\n",
      "iteration: 61600 loss: 0.0040 lr: 0.02\n",
      "iteration: 61700 loss: 0.0039 lr: 0.02\n",
      "iteration: 61800 loss: 0.0033 lr: 0.02\n",
      "iteration: 61900 loss: 0.0037 lr: 0.02\n",
      "iteration: 62000 loss: 0.0038 lr: 0.02\n",
      "iteration: 62100 loss: 0.0041 lr: 0.02\n",
      "iteration: 62200 loss: 0.0036 lr: 0.02\n",
      "iteration: 62300 loss: 0.0042 lr: 0.02\n",
      "iteration: 62400 loss: 0.0041 lr: 0.02\n",
      "iteration: 62500 loss: 0.0039 lr: 0.02\n",
      "iteration: 62600 loss: 0.0039 lr: 0.02\n",
      "iteration: 62700 loss: 0.0035 lr: 0.02\n",
      "iteration: 62800 loss: 0.0036 lr: 0.02\n",
      "iteration: 62900 loss: 0.0038 lr: 0.02\n",
      "iteration: 63000 loss: 0.0038 lr: 0.02\n",
      "iteration: 63100 loss: 0.0039 lr: 0.02\n",
      "iteration: 63200 loss: 0.0038 lr: 0.02\n",
      "iteration: 63300 loss: 0.0039 lr: 0.02\n",
      "iteration: 63400 loss: 0.0041 lr: 0.02\n",
      "iteration: 63500 loss: 0.0035 lr: 0.02\n",
      "iteration: 63600 loss: 0.0036 lr: 0.02\n",
      "iteration: 63700 loss: 0.0035 lr: 0.02\n",
      "iteration: 63800 loss: 0.0041 lr: 0.02\n",
      "iteration: 63900 loss: 0.0039 lr: 0.02\n",
      "iteration: 64000 loss: 0.0036 lr: 0.02\n",
      "iteration: 64100 loss: 0.0036 lr: 0.02\n",
      "iteration: 64200 loss: 0.0036 lr: 0.02\n",
      "iteration: 64300 loss: 0.0039 lr: 0.02\n",
      "iteration: 64400 loss: 0.0042 lr: 0.02\n",
      "iteration: 64500 loss: 0.0034 lr: 0.02\n",
      "iteration: 64600 loss: 0.0037 lr: 0.02\n",
      "iteration: 64700 loss: 0.0035 lr: 0.02\n",
      "iteration: 64800 loss: 0.0037 lr: 0.02\n",
      "iteration: 64900 loss: 0.0037 lr: 0.02\n",
      "iteration: 65000 loss: 0.0032 lr: 0.02\n",
      "iteration: 65100 loss: 0.0037 lr: 0.02\n",
      "iteration: 65200 loss: 0.0037 lr: 0.02\n",
      "iteration: 65300 loss: 0.0037 lr: 0.02\n",
      "iteration: 65400 loss: 0.0035 lr: 0.02\n",
      "iteration: 65500 loss: 0.0039 lr: 0.02\n",
      "iteration: 65600 loss: 0.0037 lr: 0.02\n",
      "iteration: 65700 loss: 0.0034 lr: 0.02\n",
      "iteration: 65800 loss: 0.0035 lr: 0.02\n",
      "iteration: 65900 loss: 0.0037 lr: 0.02\n",
      "iteration: 66000 loss: 0.0041 lr: 0.02\n",
      "iteration: 66100 loss: 0.0036 lr: 0.02\n",
      "iteration: 66200 loss: 0.0039 lr: 0.02\n",
      "iteration: 66300 loss: 0.0041 lr: 0.02\n",
      "iteration: 66400 loss: 0.0035 lr: 0.02\n",
      "iteration: 66500 loss: 0.0037 lr: 0.02\n",
      "iteration: 66600 loss: 0.0036 lr: 0.02\n",
      "iteration: 66700 loss: 0.0034 lr: 0.02\n",
      "iteration: 66800 loss: 0.0041 lr: 0.02\n",
      "iteration: 66900 loss: 0.0036 lr: 0.02\n",
      "iteration: 67000 loss: 0.0035 lr: 0.02\n",
      "iteration: 67100 loss: 0.0037 lr: 0.02\n",
      "iteration: 67200 loss: 0.0036 lr: 0.02\n",
      "iteration: 67300 loss: 0.0038 lr: 0.02\n",
      "iteration: 67400 loss: 0.0034 lr: 0.02\n",
      "iteration: 67500 loss: 0.0035 lr: 0.02\n",
      "iteration: 67600 loss: 0.0039 lr: 0.02\n",
      "iteration: 67700 loss: 0.0037 lr: 0.02\n",
      "iteration: 67800 loss: 0.0038 lr: 0.02\n",
      "iteration: 67900 loss: 0.0037 lr: 0.02\n",
      "iteration: 68000 loss: 0.0040 lr: 0.02\n",
      "iteration: 68100 loss: 0.0032 lr: 0.02\n",
      "iteration: 68200 loss: 0.0033 lr: 0.02\n",
      "iteration: 68300 loss: 0.0033 lr: 0.02\n",
      "iteration: 68400 loss: 0.0038 lr: 0.02\n",
      "iteration: 68500 loss: 0.0037 lr: 0.02\n",
      "iteration: 68600 loss: 0.0036 lr: 0.02\n",
      "iteration: 68700 loss: 0.0033 lr: 0.02\n",
      "iteration: 68800 loss: 0.0036 lr: 0.02\n",
      "iteration: 68900 loss: 0.0036 lr: 0.02\n",
      "iteration: 69000 loss: 0.0036 lr: 0.02\n",
      "iteration: 69100 loss: 0.0037 lr: 0.02\n",
      "iteration: 69200 loss: 0.0032 lr: 0.02\n",
      "iteration: 69300 loss: 0.0038 lr: 0.02\n",
      "iteration: 69400 loss: 0.0035 lr: 0.02\n",
      "iteration: 69500 loss: 0.0032 lr: 0.02\n",
      "iteration: 69600 loss: 0.0033 lr: 0.02\n",
      "iteration: 69700 loss: 0.0035 lr: 0.02\n",
      "iteration: 69800 loss: 0.0035 lr: 0.02\n",
      "iteration: 69900 loss: 0.0033 lr: 0.02\n",
      "iteration: 70000 loss: 0.0035 lr: 0.02\n",
      "iteration: 70100 loss: 0.0038 lr: 0.02\n",
      "iteration: 70200 loss: 0.0034 lr: 0.02\n",
      "iteration: 70300 loss: 0.0031 lr: 0.02\n",
      "iteration: 70400 loss: 0.0039 lr: 0.02\n",
      "iteration: 70500 loss: 0.0034 lr: 0.02\n",
      "iteration: 70600 loss: 0.0031 lr: 0.02\n",
      "iteration: 70700 loss: 0.0032 lr: 0.02\n",
      "iteration: 70800 loss: 0.0034 lr: 0.02\n",
      "iteration: 70900 loss: 0.0034 lr: 0.02\n",
      "iteration: 71000 loss: 0.0037 lr: 0.02\n",
      "iteration: 71100 loss: 0.0036 lr: 0.02\n",
      "iteration: 71200 loss: 0.0037 lr: 0.02\n",
      "iteration: 71300 loss: 0.0033 lr: 0.02\n",
      "iteration: 71400 loss: 0.0037 lr: 0.02\n",
      "iteration: 71500 loss: 0.0035 lr: 0.02\n",
      "iteration: 71600 loss: 0.0030 lr: 0.02\n",
      "iteration: 71700 loss: 0.0038 lr: 0.02\n",
      "iteration: 71800 loss: 0.0036 lr: 0.02\n",
      "iteration: 71900 loss: 0.0033 lr: 0.02\n",
      "iteration: 72000 loss: 0.0039 lr: 0.02\n",
      "iteration: 72100 loss: 0.0034 lr: 0.02\n",
      "iteration: 72200 loss: 0.0033 lr: 0.02\n",
      "iteration: 72300 loss: 0.0036 lr: 0.02\n",
      "iteration: 72400 loss: 0.0032 lr: 0.02\n",
      "iteration: 72500 loss: 0.0035 lr: 0.02\n",
      "iteration: 72600 loss: 0.0033 lr: 0.02\n",
      "iteration: 72700 loss: 0.0034 lr: 0.02\n",
      "iteration: 72800 loss: 0.0036 lr: 0.02\n",
      "iteration: 72900 loss: 0.0033 lr: 0.02\n",
      "iteration: 73000 loss: 0.0033 lr: 0.02\n",
      "iteration: 73100 loss: 0.0037 lr: 0.02\n",
      "iteration: 73200 loss: 0.0035 lr: 0.02\n",
      "iteration: 73300 loss: 0.0030 lr: 0.02\n",
      "iteration: 73400 loss: 0.0032 lr: 0.02\n",
      "iteration: 73500 loss: 0.0035 lr: 0.02\n",
      "iteration: 73600 loss: 0.0037 lr: 0.02\n",
      "iteration: 73700 loss: 0.0032 lr: 0.02\n",
      "iteration: 73800 loss: 0.0034 lr: 0.02\n",
      "iteration: 73900 loss: 0.0032 lr: 0.02\n",
      "iteration: 74000 loss: 0.0033 lr: 0.02\n",
      "iteration: 74100 loss: 0.0031 lr: 0.02\n",
      "iteration: 74200 loss: 0.0038 lr: 0.02\n",
      "iteration: 74300 loss: 0.0034 lr: 0.02\n",
      "iteration: 74400 loss: 0.0034 lr: 0.02\n",
      "iteration: 74500 loss: 0.0032 lr: 0.02\n",
      "iteration: 74600 loss: 0.0032 lr: 0.02\n",
      "iteration: 74700 loss: 0.0034 lr: 0.02\n",
      "iteration: 74800 loss: 0.0030 lr: 0.02\n",
      "iteration: 74900 loss: 0.0033 lr: 0.02\n",
      "iteration: 75000 loss: 0.0036 lr: 0.02\n",
      "iteration: 75100 loss: 0.0035 lr: 0.02\n",
      "iteration: 75200 loss: 0.0036 lr: 0.02\n",
      "iteration: 75300 loss: 0.0031 lr: 0.02\n",
      "iteration: 75400 loss: 0.0033 lr: 0.02\n",
      "iteration: 75500 loss: 0.0031 lr: 0.02\n",
      "iteration: 75600 loss: 0.0032 lr: 0.02\n",
      "iteration: 75700 loss: 0.0033 lr: 0.02\n",
      "iteration: 75800 loss: 0.0031 lr: 0.02\n",
      "iteration: 75900 loss: 0.0031 lr: 0.02\n",
      "iteration: 76000 loss: 0.0030 lr: 0.02\n",
      "iteration: 76100 loss: 0.0032 lr: 0.02\n",
      "iteration: 76200 loss: 0.0035 lr: 0.02\n",
      "iteration: 76300 loss: 0.0030 lr: 0.02\n",
      "iteration: 76400 loss: 0.0031 lr: 0.02\n",
      "iteration: 76500 loss: 0.0035 lr: 0.02\n",
      "iteration: 76600 loss: 0.0035 lr: 0.02\n",
      "iteration: 76700 loss: 0.0033 lr: 0.02\n",
      "iteration: 76800 loss: 0.0033 lr: 0.02\n",
      "iteration: 76900 loss: 0.0030 lr: 0.02\n",
      "iteration: 77000 loss: 0.0030 lr: 0.02\n",
      "iteration: 77100 loss: 0.0032 lr: 0.02\n",
      "iteration: 77200 loss: 0.0032 lr: 0.02\n",
      "iteration: 77300 loss: 0.0034 lr: 0.02\n",
      "iteration: 77400 loss: 0.0034 lr: 0.02\n",
      "iteration: 77500 loss: 0.0032 lr: 0.02\n",
      "iteration: 77600 loss: 0.0030 lr: 0.02\n",
      "iteration: 77700 loss: 0.0030 lr: 0.02\n",
      "iteration: 77800 loss: 0.0030 lr: 0.02\n",
      "iteration: 77900 loss: 0.0029 lr: 0.02\n",
      "iteration: 78000 loss: 0.0033 lr: 0.02\n",
      "iteration: 78100 loss: 0.0036 lr: 0.02\n",
      "iteration: 78200 loss: 0.0031 lr: 0.02\n",
      "iteration: 78300 loss: 0.0031 lr: 0.02\n",
      "iteration: 78400 loss: 0.0033 lr: 0.02\n",
      "iteration: 78500 loss: 0.0031 lr: 0.02\n",
      "iteration: 78600 loss: 0.0029 lr: 0.02\n",
      "iteration: 78700 loss: 0.0034 lr: 0.02\n",
      "iteration: 78800 loss: 0.0036 lr: 0.02\n",
      "iteration: 78900 loss: 0.0036 lr: 0.02\n",
      "iteration: 79000 loss: 0.0034 lr: 0.02\n",
      "iteration: 79100 loss: 0.0032 lr: 0.02\n",
      "iteration: 79200 loss: 0.0033 lr: 0.02\n",
      "iteration: 79300 loss: 0.0033 lr: 0.02\n",
      "iteration: 79400 loss: 0.0031 lr: 0.02\n",
      "iteration: 79500 loss: 0.0031 lr: 0.02\n",
      "iteration: 79600 loss: 0.0034 lr: 0.02\n",
      "iteration: 79700 loss: 0.0034 lr: 0.02\n",
      "iteration: 79800 loss: 0.0033 lr: 0.02\n",
      "iteration: 79900 loss: 0.0033 lr: 0.02\n",
      "iteration: 80000 loss: 0.0034 lr: 0.02\n",
      "iteration: 80100 loss: 0.0031 lr: 0.02\n",
      "iteration: 80200 loss: 0.0028 lr: 0.02\n",
      "iteration: 80300 loss: 0.0031 lr: 0.02\n",
      "iteration: 80400 loss: 0.0031 lr: 0.02\n",
      "iteration: 80500 loss: 0.0033 lr: 0.02\n",
      "iteration: 80600 loss: 0.0033 lr: 0.02\n",
      "iteration: 80700 loss: 0.0034 lr: 0.02\n",
      "iteration: 80800 loss: 0.0033 lr: 0.02\n",
      "iteration: 80900 loss: 0.0031 lr: 0.02\n",
      "iteration: 81000 loss: 0.0031 lr: 0.02\n",
      "iteration: 81100 loss: 0.0035 lr: 0.02\n",
      "iteration: 81200 loss: 0.0034 lr: 0.02\n",
      "iteration: 81300 loss: 0.0034 lr: 0.02\n",
      "iteration: 81400 loss: 0.0031 lr: 0.02\n",
      "iteration: 81500 loss: 0.0028 lr: 0.02\n",
      "iteration: 81600 loss: 0.0029 lr: 0.02\n",
      "iteration: 81700 loss: 0.0031 lr: 0.02\n",
      "iteration: 81800 loss: 0.0030 lr: 0.02\n",
      "iteration: 81900 loss: 0.0027 lr: 0.02\n",
      "iteration: 82000 loss: 0.0030 lr: 0.02\n",
      "iteration: 82100 loss: 0.0032 lr: 0.02\n",
      "iteration: 82200 loss: 0.0029 lr: 0.02\n",
      "iteration: 82300 loss: 0.0029 lr: 0.02\n",
      "iteration: 82400 loss: 0.0032 lr: 0.02\n",
      "iteration: 82500 loss: 0.0027 lr: 0.02\n",
      "iteration: 82600 loss: 0.0031 lr: 0.02\n",
      "iteration: 82700 loss: 0.0031 lr: 0.02\n",
      "iteration: 82800 loss: 0.0030 lr: 0.02\n",
      "iteration: 82900 loss: 0.0034 lr: 0.02\n",
      "iteration: 83000 loss: 0.0033 lr: 0.02\n",
      "iteration: 83100 loss: 0.0035 lr: 0.02\n",
      "iteration: 83200 loss: 0.0034 lr: 0.02\n",
      "iteration: 83300 loss: 0.0029 lr: 0.02\n",
      "iteration: 83400 loss: 0.0031 lr: 0.02\n",
      "iteration: 83500 loss: 0.0032 lr: 0.02\n",
      "iteration: 83600 loss: 0.0031 lr: 0.02\n",
      "iteration: 83700 loss: 0.0030 lr: 0.02\n",
      "iteration: 83800 loss: 0.0031 lr: 0.02\n",
      "iteration: 83900 loss: 0.0027 lr: 0.02\n",
      "iteration: 84000 loss: 0.0034 lr: 0.02\n",
      "iteration: 84100 loss: 0.0030 lr: 0.02\n",
      "iteration: 84200 loss: 0.0033 lr: 0.02\n",
      "iteration: 84300 loss: 0.0032 lr: 0.02\n",
      "iteration: 84400 loss: 0.0030 lr: 0.02\n",
      "iteration: 84500 loss: 0.0029 lr: 0.02\n",
      "iteration: 84600 loss: 0.0035 lr: 0.02\n",
      "iteration: 84700 loss: 0.0032 lr: 0.02\n",
      "iteration: 84800 loss: 0.0033 lr: 0.02\n",
      "iteration: 84900 loss: 0.0033 lr: 0.02\n",
      "iteration: 85000 loss: 0.0033 lr: 0.02\n",
      "iteration: 85100 loss: 0.0034 lr: 0.02\n",
      "iteration: 85200 loss: 0.0032 lr: 0.02\n",
      "iteration: 85300 loss: 0.0032 lr: 0.02\n",
      "iteration: 85400 loss: 0.0031 lr: 0.02\n",
      "iteration: 85500 loss: 0.0029 lr: 0.02\n",
      "iteration: 85600 loss: 0.0031 lr: 0.02\n",
      "iteration: 85700 loss: 0.0029 lr: 0.02\n",
      "iteration: 85800 loss: 0.0035 lr: 0.02\n",
      "iteration: 85900 loss: 0.0032 lr: 0.02\n",
      "iteration: 86000 loss: 0.0027 lr: 0.02\n",
      "iteration: 86100 loss: 0.0029 lr: 0.02\n",
      "iteration: 86200 loss: 0.0030 lr: 0.02\n",
      "iteration: 86300 loss: 0.0029 lr: 0.02\n",
      "iteration: 86400 loss: 0.0031 lr: 0.02\n",
      "iteration: 86500 loss: 0.0031 lr: 0.02\n",
      "iteration: 86600 loss: 0.0032 lr: 0.02\n",
      "iteration: 86700 loss: 0.0031 lr: 0.02\n",
      "iteration: 86800 loss: 0.0030 lr: 0.02\n",
      "iteration: 86900 loss: 0.0030 lr: 0.02\n",
      "iteration: 87000 loss: 0.0036 lr: 0.02\n",
      "iteration: 87100 loss: 0.0030 lr: 0.02\n",
      "iteration: 87200 loss: 0.0030 lr: 0.02\n",
      "iteration: 87300 loss: 0.0026 lr: 0.02\n",
      "iteration: 87400 loss: 0.0031 lr: 0.02\n",
      "iteration: 87500 loss: 0.0031 lr: 0.02\n",
      "iteration: 87600 loss: 0.0030 lr: 0.02\n",
      "iteration: 87700 loss: 0.0031 lr: 0.02\n",
      "iteration: 87800 loss: 0.0027 lr: 0.02\n",
      "iteration: 87900 loss: 0.0031 lr: 0.02\n",
      "iteration: 88000 loss: 0.0031 lr: 0.02\n",
      "iteration: 88100 loss: 0.0028 lr: 0.02\n",
      "iteration: 88200 loss: 0.0028 lr: 0.02\n",
      "iteration: 88300 loss: 0.0029 lr: 0.02\n",
      "iteration: 88400 loss: 0.0028 lr: 0.02\n",
      "iteration: 88500 loss: 0.0027 lr: 0.02\n",
      "iteration: 88600 loss: 0.0032 lr: 0.02\n",
      "iteration: 88700 loss: 0.0027 lr: 0.02\n",
      "iteration: 88800 loss: 0.0027 lr: 0.02\n",
      "iteration: 88900 loss: 0.0029 lr: 0.02\n",
      "iteration: 89000 loss: 0.0030 lr: 0.02\n",
      "iteration: 89100 loss: 0.0032 lr: 0.02\n",
      "iteration: 89200 loss: 0.0030 lr: 0.02\n",
      "iteration: 89300 loss: 0.0032 lr: 0.02\n",
      "iteration: 89400 loss: 0.0029 lr: 0.02\n",
      "iteration: 89500 loss: 0.0031 lr: 0.02\n",
      "iteration: 89600 loss: 0.0029 lr: 0.02\n",
      "iteration: 89700 loss: 0.0030 lr: 0.02\n",
      "iteration: 89800 loss: 0.0030 lr: 0.02\n",
      "iteration: 89900 loss: 0.0029 lr: 0.02\n",
      "iteration: 90000 loss: 0.0030 lr: 0.02\n",
      "iteration: 90100 loss: 0.0029 lr: 0.02\n",
      "iteration: 90200 loss: 0.0027 lr: 0.02\n",
      "iteration: 90300 loss: 0.0031 lr: 0.02\n",
      "iteration: 90400 loss: 0.0029 lr: 0.02\n",
      "iteration: 90500 loss: 0.0030 lr: 0.02\n",
      "iteration: 90600 loss: 0.0026 lr: 0.02\n",
      "iteration: 90700 loss: 0.0032 lr: 0.02\n",
      "iteration: 90800 loss: 0.0030 lr: 0.02\n",
      "iteration: 90900 loss: 0.0031 lr: 0.02\n",
      "iteration: 91000 loss: 0.0030 lr: 0.02\n",
      "iteration: 91100 loss: 0.0029 lr: 0.02\n",
      "iteration: 91200 loss: 0.0028 lr: 0.02\n",
      "iteration: 91300 loss: 0.0031 lr: 0.02\n",
      "iteration: 91400 loss: 0.0024 lr: 0.02\n",
      "iteration: 91500 loss: 0.0028 lr: 0.02\n",
      "iteration: 91600 loss: 0.0028 lr: 0.02\n",
      "iteration: 91700 loss: 0.0028 lr: 0.02\n",
      "iteration: 91800 loss: 0.0029 lr: 0.02\n",
      "iteration: 91900 loss: 0.0027 lr: 0.02\n",
      "iteration: 92000 loss: 0.0030 lr: 0.02\n",
      "iteration: 92100 loss: 0.0027 lr: 0.02\n",
      "iteration: 92200 loss: 0.0028 lr: 0.02\n",
      "iteration: 92300 loss: 0.0029 lr: 0.02\n",
      "iteration: 92400 loss: 0.0030 lr: 0.02\n",
      "iteration: 92500 loss: 0.0028 lr: 0.02\n",
      "iteration: 92600 loss: 0.0027 lr: 0.02\n",
      "iteration: 92700 loss: 0.0029 lr: 0.02\n",
      "iteration: 92800 loss: 0.0028 lr: 0.02\n",
      "iteration: 92900 loss: 0.0030 lr: 0.02\n",
      "iteration: 93000 loss: 0.0028 lr: 0.02\n",
      "iteration: 93100 loss: 0.0030 lr: 0.02\n",
      "iteration: 93200 loss: 0.0031 lr: 0.02\n",
      "iteration: 93300 loss: 0.0028 lr: 0.02\n",
      "iteration: 93400 loss: 0.0028 lr: 0.02\n",
      "iteration: 93500 loss: 0.0027 lr: 0.02\n",
      "iteration: 93600 loss: 0.0027 lr: 0.02\n",
      "iteration: 93700 loss: 0.0028 lr: 0.02\n",
      "iteration: 93800 loss: 0.0027 lr: 0.02\n",
      "iteration: 93900 loss: 0.0030 lr: 0.02\n",
      "iteration: 94000 loss: 0.0025 lr: 0.02\n",
      "iteration: 94100 loss: 0.0029 lr: 0.02\n",
      "iteration: 94200 loss: 0.0029 lr: 0.02\n",
      "iteration: 94300 loss: 0.0029 lr: 0.02\n",
      "iteration: 94400 loss: 0.0026 lr: 0.02\n",
      "iteration: 94500 loss: 0.0028 lr: 0.02\n",
      "iteration: 94600 loss: 0.0027 lr: 0.02\n",
      "iteration: 94700 loss: 0.0027 lr: 0.02\n",
      "iteration: 94800 loss: 0.0026 lr: 0.02\n",
      "iteration: 94900 loss: 0.0027 lr: 0.02\n",
      "iteration: 95000 loss: 0.0027 lr: 0.02\n",
      "iteration: 95100 loss: 0.0028 lr: 0.02\n",
      "iteration: 95200 loss: 0.0026 lr: 0.02\n",
      "iteration: 95300 loss: 0.0026 lr: 0.02\n",
      "iteration: 95400 loss: 0.0029 lr: 0.02\n",
      "iteration: 95500 loss: 0.0024 lr: 0.02\n",
      "iteration: 95600 loss: 0.0027 lr: 0.02\n",
      "iteration: 95700 loss: 0.0027 lr: 0.02\n",
      "iteration: 95800 loss: 0.0028 lr: 0.02\n",
      "iteration: 95900 loss: 0.0027 lr: 0.02\n",
      "iteration: 96000 loss: 0.0029 lr: 0.02\n",
      "iteration: 96100 loss: 0.0028 lr: 0.02\n",
      "iteration: 96200 loss: 0.0028 lr: 0.02\n",
      "iteration: 96300 loss: 0.0028 lr: 0.02\n",
      "iteration: 96400 loss: 0.0031 lr: 0.02\n",
      "iteration: 96500 loss: 0.0029 lr: 0.02\n",
      "iteration: 96600 loss: 0.0023 lr: 0.02\n",
      "iteration: 96700 loss: 0.0028 lr: 0.02\n",
      "iteration: 96800 loss: 0.0029 lr: 0.02\n",
      "iteration: 96900 loss: 0.0028 lr: 0.02\n",
      "iteration: 97000 loss: 0.0026 lr: 0.02\n",
      "iteration: 97100 loss: 0.0028 lr: 0.02\n",
      "iteration: 97200 loss: 0.0026 lr: 0.02\n",
      "iteration: 97300 loss: 0.0028 lr: 0.02\n",
      "iteration: 97400 loss: 0.0027 lr: 0.02\n",
      "iteration: 97500 loss: 0.0025 lr: 0.02\n",
      "iteration: 97600 loss: 0.0027 lr: 0.02\n",
      "iteration: 97700 loss: 0.0032 lr: 0.02\n",
      "iteration: 97800 loss: 0.0030 lr: 0.02\n",
      "iteration: 97900 loss: 0.0027 lr: 0.02\n",
      "iteration: 98000 loss: 0.0028 lr: 0.02\n",
      "iteration: 98100 loss: 0.0028 lr: 0.02\n",
      "iteration: 98200 loss: 0.0026 lr: 0.02\n",
      "iteration: 98300 loss: 0.0028 lr: 0.02\n",
      "iteration: 98400 loss: 0.0029 lr: 0.02\n",
      "iteration: 98500 loss: 0.0028 lr: 0.02\n",
      "iteration: 98600 loss: 0.0027 lr: 0.02\n",
      "iteration: 98700 loss: 0.0027 lr: 0.02\n",
      "iteration: 98800 loss: 0.0026 lr: 0.02\n",
      "iteration: 98900 loss: 0.0025 lr: 0.02\n",
      "iteration: 99000 loss: 0.0024 lr: 0.02\n",
      "iteration: 99100 loss: 0.0027 lr: 0.02\n",
      "iteration: 99200 loss: 0.0028 lr: 0.02\n",
      "iteration: 99300 loss: 0.0028 lr: 0.02\n",
      "iteration: 99400 loss: 0.0026 lr: 0.02\n",
      "iteration: 99500 loss: 0.0029 lr: 0.02\n",
      "iteration: 99600 loss: 0.0029 lr: 0.02\n",
      "iteration: 99700 loss: 0.0030 lr: 0.02\n",
      "iteration: 99800 loss: 0.0025 lr: 0.02\n",
      "iteration: 99900 loss: 0.0025 lr: 0.02\n",
      "iteration: 100000 loss: 0.0026 lr: 0.02\n",
      "iteration: 100100 loss: 0.0025 lr: 0.02\n",
      "iteration: 100200 loss: 0.0025 lr: 0.02\n",
      "iteration: 100300 loss: 0.0028 lr: 0.02\n",
      "iteration: 100400 loss: 0.0030 lr: 0.02\n",
      "iteration: 100500 loss: 0.0031 lr: 0.02\n",
      "iteration: 100600 loss: 0.0027 lr: 0.02\n",
      "iteration: 100700 loss: 0.0024 lr: 0.02\n",
      "iteration: 100800 loss: 0.0029 lr: 0.02\n",
      "iteration: 100900 loss: 0.0027 lr: 0.02\n",
      "iteration: 101000 loss: 0.0027 lr: 0.02\n",
      "iteration: 101100 loss: 0.0025 lr: 0.02\n",
      "iteration: 101200 loss: 0.0032 lr: 0.02\n",
      "iteration: 101300 loss: 0.0030 lr: 0.02\n",
      "iteration: 101400 loss: 0.0024 lr: 0.02\n",
      "iteration: 101500 loss: 0.0024 lr: 0.02\n",
      "iteration: 101600 loss: 0.0027 lr: 0.02\n",
      "iteration: 101700 loss: 0.0025 lr: 0.02\n",
      "iteration: 101800 loss: 0.0030 lr: 0.02\n",
      "iteration: 101900 loss: 0.0026 lr: 0.02\n",
      "iteration: 102000 loss: 0.0025 lr: 0.02\n",
      "iteration: 102100 loss: 0.0027 lr: 0.02\n",
      "iteration: 102200 loss: 0.0025 lr: 0.02\n",
      "iteration: 102300 loss: 0.0027 lr: 0.02\n",
      "iteration: 102400 loss: 0.0029 lr: 0.02\n",
      "iteration: 102500 loss: 0.0027 lr: 0.02\n",
      "iteration: 102600 loss: 0.0025 lr: 0.02\n",
      "iteration: 102700 loss: 0.0029 lr: 0.02\n",
      "iteration: 102800 loss: 0.0027 lr: 0.02\n",
      "iteration: 102900 loss: 0.0030 lr: 0.02\n",
      "iteration: 103000 loss: 0.0028 lr: 0.02\n",
      "iteration: 103100 loss: 0.0030 lr: 0.02\n",
      "iteration: 103200 loss: 0.0026 lr: 0.02\n",
      "iteration: 103300 loss: 0.0027 lr: 0.02\n",
      "iteration: 103400 loss: 0.0023 lr: 0.02\n",
      "iteration: 103500 loss: 0.0025 lr: 0.02\n",
      "iteration: 103600 loss: 0.0024 lr: 0.02\n",
      "iteration: 103700 loss: 0.0028 lr: 0.02\n",
      "iteration: 103800 loss: 0.0029 lr: 0.02\n",
      "iteration: 103900 loss: 0.0028 lr: 0.02\n",
      "iteration: 104000 loss: 0.0026 lr: 0.02\n",
      "iteration: 104100 loss: 0.0026 lr: 0.02\n",
      "iteration: 104200 loss: 0.0027 lr: 0.02\n",
      "iteration: 104300 loss: 0.0029 lr: 0.02\n",
      "iteration: 104400 loss: 0.0027 lr: 0.02\n",
      "iteration: 104500 loss: 0.0030 lr: 0.02\n",
      "iteration: 104600 loss: 0.0027 lr: 0.02\n",
      "iteration: 104700 loss: 0.0028 lr: 0.02\n",
      "iteration: 104800 loss: 0.0026 lr: 0.02\n",
      "iteration: 104900 loss: 0.0027 lr: 0.02\n",
      "iteration: 105000 loss: 0.0025 lr: 0.02\n",
      "iteration: 105100 loss: 0.0024 lr: 0.02\n",
      "iteration: 105200 loss: 0.0030 lr: 0.02\n",
      "iteration: 105300 loss: 0.0031 lr: 0.02\n",
      "iteration: 105400 loss: 0.0028 lr: 0.02\n",
      "iteration: 105500 loss: 0.0024 lr: 0.02\n",
      "iteration: 105600 loss: 0.0028 lr: 0.02\n",
      "iteration: 105700 loss: 0.0026 lr: 0.02\n",
      "iteration: 105800 loss: 0.0024 lr: 0.02\n",
      "iteration: 105900 loss: 0.0024 lr: 0.02\n",
      "iteration: 106000 loss: 0.0025 lr: 0.02\n",
      "iteration: 106100 loss: 0.0024 lr: 0.02\n",
      "iteration: 106200 loss: 0.0025 lr: 0.02\n",
      "iteration: 106300 loss: 0.0029 lr: 0.02\n",
      "iteration: 106400 loss: 0.0026 lr: 0.02\n",
      "iteration: 106500 loss: 0.0026 lr: 0.02\n",
      "iteration: 106600 loss: 0.0026 lr: 0.02\n",
      "iteration: 106700 loss: 0.0028 lr: 0.02\n",
      "iteration: 106800 loss: 0.0028 lr: 0.02\n",
      "iteration: 106900 loss: 0.0025 lr: 0.02\n",
      "iteration: 107000 loss: 0.0026 lr: 0.02\n",
      "iteration: 107100 loss: 0.0028 lr: 0.02\n",
      "iteration: 107200 loss: 0.0029 lr: 0.02\n",
      "iteration: 107300 loss: 0.0025 lr: 0.02\n",
      "iteration: 107400 loss: 0.0029 lr: 0.02\n",
      "iteration: 107500 loss: 0.0026 lr: 0.02\n",
      "iteration: 107600 loss: 0.0026 lr: 0.02\n",
      "iteration: 107700 loss: 0.0023 lr: 0.02\n",
      "iteration: 107800 loss: 0.0024 lr: 0.02\n",
      "iteration: 107900 loss: 0.0025 lr: 0.02\n",
      "iteration: 108000 loss: 0.0026 lr: 0.02\n",
      "iteration: 108100 loss: 0.0028 lr: 0.02\n",
      "iteration: 108200 loss: 0.0027 lr: 0.02\n",
      "iteration: 108300 loss: 0.0026 lr: 0.02\n",
      "iteration: 108400 loss: 0.0024 lr: 0.02\n",
      "iteration: 108500 loss: 0.0026 lr: 0.02\n",
      "iteration: 108600 loss: 0.0026 lr: 0.02\n",
      "iteration: 108700 loss: 0.0026 lr: 0.02\n",
      "iteration: 108800 loss: 0.0021 lr: 0.02\n",
      "iteration: 108900 loss: 0.0025 lr: 0.02\n",
      "iteration: 109000 loss: 0.0025 lr: 0.02\n",
      "iteration: 109100 loss: 0.0026 lr: 0.02\n",
      "iteration: 109200 loss: 0.0023 lr: 0.02\n",
      "iteration: 109300 loss: 0.0022 lr: 0.02\n",
      "iteration: 109400 loss: 0.0024 lr: 0.02\n",
      "iteration: 109500 loss: 0.0023 lr: 0.02\n",
      "iteration: 109600 loss: 0.0026 lr: 0.02\n",
      "iteration: 109700 loss: 0.0025 lr: 0.02\n",
      "iteration: 109800 loss: 0.0025 lr: 0.02\n",
      "iteration: 109900 loss: 0.0022 lr: 0.02\n",
      "iteration: 110000 loss: 0.0026 lr: 0.02\n",
      "iteration: 110100 loss: 0.0026 lr: 0.02\n",
      "iteration: 110200 loss: 0.0030 lr: 0.02\n",
      "iteration: 110300 loss: 0.0025 lr: 0.02\n",
      "iteration: 110400 loss: 0.0025 lr: 0.02\n",
      "iteration: 110500 loss: 0.0027 lr: 0.02\n",
      "iteration: 110600 loss: 0.0025 lr: 0.02\n",
      "iteration: 110700 loss: 0.0026 lr: 0.02\n",
      "iteration: 110800 loss: 0.0024 lr: 0.02\n",
      "iteration: 110900 loss: 0.0024 lr: 0.02\n",
      "iteration: 111000 loss: 0.0026 lr: 0.02\n",
      "iteration: 111100 loss: 0.0025 lr: 0.02\n",
      "iteration: 111200 loss: 0.0024 lr: 0.02\n",
      "iteration: 111300 loss: 0.0025 lr: 0.02\n",
      "iteration: 111400 loss: 0.0025 lr: 0.02\n",
      "iteration: 111500 loss: 0.0025 lr: 0.02\n",
      "iteration: 111600 loss: 0.0021 lr: 0.02\n",
      "iteration: 111700 loss: 0.0023 lr: 0.02\n",
      "iteration: 111800 loss: 0.0021 lr: 0.02\n",
      "iteration: 111900 loss: 0.0024 lr: 0.02\n",
      "iteration: 112000 loss: 0.0025 lr: 0.02\n",
      "iteration: 112100 loss: 0.0023 lr: 0.02\n",
      "iteration: 112200 loss: 0.0024 lr: 0.02\n",
      "iteration: 112300 loss: 0.0022 lr: 0.02\n",
      "iteration: 112400 loss: 0.0023 lr: 0.02\n",
      "iteration: 112500 loss: 0.0022 lr: 0.02\n",
      "iteration: 112600 loss: 0.0024 lr: 0.02\n",
      "iteration: 112700 loss: 0.0024 lr: 0.02\n",
      "iteration: 112800 loss: 0.0025 lr: 0.02\n",
      "iteration: 112900 loss: 0.0026 lr: 0.02\n",
      "iteration: 113000 loss: 0.0028 lr: 0.02\n",
      "iteration: 113100 loss: 0.0026 lr: 0.02\n",
      "iteration: 113200 loss: 0.0026 lr: 0.02\n",
      "iteration: 113300 loss: 0.0024 lr: 0.02\n",
      "iteration: 113400 loss: 0.0026 lr: 0.02\n",
      "iteration: 113500 loss: 0.0028 lr: 0.02\n",
      "iteration: 113600 loss: 0.0026 lr: 0.02\n",
      "iteration: 113700 loss: 0.0026 lr: 0.02\n",
      "iteration: 113800 loss: 0.0026 lr: 0.02\n",
      "iteration: 113900 loss: 0.0024 lr: 0.02\n",
      "iteration: 114000 loss: 0.0025 lr: 0.02\n",
      "iteration: 114100 loss: 0.0028 lr: 0.02\n",
      "iteration: 114200 loss: 0.0028 lr: 0.02\n",
      "iteration: 114300 loss: 0.0026 lr: 0.02\n",
      "iteration: 114400 loss: 0.0024 lr: 0.02\n",
      "iteration: 114500 loss: 0.0025 lr: 0.02\n",
      "iteration: 114600 loss: 0.0027 lr: 0.02\n",
      "iteration: 114700 loss: 0.0025 lr: 0.02\n",
      "iteration: 114800 loss: 0.0022 lr: 0.02\n",
      "iteration: 114900 loss: 0.0023 lr: 0.02\n",
      "iteration: 115000 loss: 0.0024 lr: 0.02\n",
      "iteration: 115100 loss: 0.0025 lr: 0.02\n",
      "iteration: 115200 loss: 0.0024 lr: 0.02\n",
      "iteration: 115300 loss: 0.0025 lr: 0.02\n",
      "iteration: 115400 loss: 0.0022 lr: 0.02\n",
      "iteration: 115500 loss: 0.0021 lr: 0.02\n",
      "iteration: 115600 loss: 0.0021 lr: 0.02\n",
      "iteration: 115700 loss: 0.0024 lr: 0.02\n",
      "iteration: 115800 loss: 0.0024 lr: 0.02\n",
      "iteration: 115900 loss: 0.0026 lr: 0.02\n",
      "iteration: 116000 loss: 0.0023 lr: 0.02\n",
      "iteration: 116100 loss: 0.0023 lr: 0.02\n",
      "iteration: 116200 loss: 0.0022 lr: 0.02\n",
      "iteration: 116300 loss: 0.0024 lr: 0.02\n",
      "iteration: 116400 loss: 0.0023 lr: 0.02\n",
      "iteration: 116500 loss: 0.0022 lr: 0.02\n",
      "iteration: 116600 loss: 0.0024 lr: 0.02\n",
      "iteration: 116700 loss: 0.0021 lr: 0.02\n",
      "iteration: 116800 loss: 0.0024 lr: 0.02\n",
      "iteration: 116900 loss: 0.0020 lr: 0.02\n",
      "iteration: 117000 loss: 0.0022 lr: 0.02\n",
      "iteration: 117100 loss: 0.0024 lr: 0.02\n",
      "iteration: 117200 loss: 0.0023 lr: 0.02\n",
      "iteration: 117300 loss: 0.0023 lr: 0.02\n",
      "iteration: 117400 loss: 0.0026 lr: 0.02\n",
      "iteration: 117500 loss: 0.0027 lr: 0.02\n",
      "iteration: 117600 loss: 0.0025 lr: 0.02\n",
      "iteration: 117700 loss: 0.0024 lr: 0.02\n",
      "iteration: 117800 loss: 0.0022 lr: 0.02\n",
      "iteration: 117900 loss: 0.0027 lr: 0.02\n",
      "iteration: 118000 loss: 0.0023 lr: 0.02\n",
      "iteration: 118100 loss: 0.0024 lr: 0.02\n",
      "iteration: 118200 loss: 0.0023 lr: 0.02\n",
      "iteration: 118300 loss: 0.0024 lr: 0.02\n",
      "iteration: 118400 loss: 0.0026 lr: 0.02\n",
      "iteration: 118500 loss: 0.0025 lr: 0.02\n",
      "iteration: 118600 loss: 0.0024 lr: 0.02\n",
      "iteration: 118700 loss: 0.0025 lr: 0.02\n",
      "iteration: 118800 loss: 0.0024 lr: 0.02\n",
      "iteration: 118900 loss: 0.0024 lr: 0.02\n",
      "iteration: 119000 loss: 0.0021 lr: 0.02\n",
      "iteration: 119100 loss: 0.0024 lr: 0.02\n",
      "iteration: 119200 loss: 0.0026 lr: 0.02\n",
      "iteration: 119300 loss: 0.0024 lr: 0.02\n",
      "iteration: 119400 loss: 0.0027 lr: 0.02\n",
      "iteration: 119500 loss: 0.0022 lr: 0.02\n",
      "iteration: 119600 loss: 0.0023 lr: 0.02\n",
      "iteration: 119700 loss: 0.0023 lr: 0.02\n",
      "iteration: 119800 loss: 0.0023 lr: 0.02\n",
      "iteration: 119900 loss: 0.0032 lr: 0.02\n",
      "iteration: 120000 loss: 0.0025 lr: 0.02\n",
      "iteration: 120100 loss: 0.0023 lr: 0.02\n",
      "iteration: 120200 loss: 0.0025 lr: 0.02\n",
      "iteration: 120300 loss: 0.0023 lr: 0.02\n",
      "iteration: 120400 loss: 0.0021 lr: 0.02\n",
      "iteration: 120500 loss: 0.0026 lr: 0.02\n",
      "iteration: 120600 loss: 0.0025 lr: 0.02\n",
      "iteration: 120700 loss: 0.0023 lr: 0.02\n",
      "iteration: 120800 loss: 0.0023 lr: 0.02\n",
      "iteration: 120900 loss: 0.0024 lr: 0.02\n",
      "iteration: 121000 loss: 0.0021 lr: 0.02\n",
      "iteration: 121100 loss: 0.0025 lr: 0.02\n",
      "iteration: 121200 loss: 0.0022 lr: 0.02\n",
      "iteration: 121300 loss: 0.0023 lr: 0.02\n",
      "iteration: 121400 loss: 0.0024 lr: 0.02\n",
      "iteration: 121500 loss: 0.0023 lr: 0.02\n",
      "iteration: 121600 loss: 0.0023 lr: 0.02\n",
      "iteration: 121700 loss: 0.0021 lr: 0.02\n",
      "iteration: 121800 loss: 0.0022 lr: 0.02\n",
      "iteration: 121900 loss: 0.0025 lr: 0.02\n",
      "iteration: 122000 loss: 0.0021 lr: 0.02\n",
      "iteration: 122100 loss: 0.0023 lr: 0.02\n",
      "iteration: 122200 loss: 0.0026 lr: 0.02\n",
      "iteration: 122300 loss: 0.0024 lr: 0.02\n",
      "iteration: 122400 loss: 0.0024 lr: 0.02\n",
      "iteration: 122500 loss: 0.0026 lr: 0.02\n",
      "iteration: 122600 loss: 0.0026 lr: 0.02\n",
      "iteration: 122700 loss: 0.0022 lr: 0.02\n",
      "iteration: 122800 loss: 0.0023 lr: 0.02\n",
      "iteration: 122900 loss: 0.0023 lr: 0.02\n",
      "iteration: 123000 loss: 0.0022 lr: 0.02\n",
      "iteration: 123100 loss: 0.0023 lr: 0.02\n",
      "iteration: 123200 loss: 0.0023 lr: 0.02\n",
      "iteration: 123300 loss: 0.0024 lr: 0.02\n",
      "iteration: 123400 loss: 0.0019 lr: 0.02\n",
      "iteration: 123500 loss: 0.0025 lr: 0.02\n",
      "iteration: 123600 loss: 0.0022 lr: 0.02\n",
      "iteration: 123700 loss: 0.0023 lr: 0.02\n",
      "iteration: 123800 loss: 0.0026 lr: 0.02\n",
      "iteration: 123900 loss: 0.0026 lr: 0.02\n",
      "iteration: 124000 loss: 0.0023 lr: 0.02\n",
      "iteration: 124100 loss: 0.0022 lr: 0.02\n",
      "iteration: 124200 loss: 0.0025 lr: 0.02\n",
      "iteration: 124300 loss: 0.0026 lr: 0.02\n",
      "iteration: 124400 loss: 0.0021 lr: 0.02\n",
      "iteration: 124500 loss: 0.0024 lr: 0.02\n",
      "iteration: 124600 loss: 0.0023 lr: 0.02\n",
      "iteration: 124700 loss: 0.0022 lr: 0.02\n",
      "iteration: 124800 loss: 0.0023 lr: 0.02\n",
      "iteration: 124900 loss: 0.0022 lr: 0.02\n",
      "iteration: 125000 loss: 0.0024 lr: 0.02\n",
      "iteration: 125100 loss: 0.0021 lr: 0.02\n",
      "iteration: 125200 loss: 0.0024 lr: 0.02\n",
      "iteration: 125300 loss: 0.0022 lr: 0.02\n",
      "iteration: 125400 loss: 0.0022 lr: 0.02\n",
      "iteration: 125500 loss: 0.0023 lr: 0.02\n",
      "iteration: 125600 loss: 0.0024 lr: 0.02\n",
      "iteration: 125700 loss: 0.0023 lr: 0.02\n",
      "iteration: 125800 loss: 0.0023 lr: 0.02\n",
      "iteration: 125900 loss: 0.0023 lr: 0.02\n",
      "iteration: 126000 loss: 0.0025 lr: 0.02\n",
      "iteration: 126100 loss: 0.0028 lr: 0.02\n",
      "iteration: 126200 loss: 0.0022 lr: 0.02\n",
      "iteration: 126300 loss: 0.0021 lr: 0.02\n",
      "iteration: 126400 loss: 0.0024 lr: 0.02\n",
      "iteration: 126500 loss: 0.0022 lr: 0.02\n",
      "iteration: 126600 loss: 0.0021 lr: 0.02\n",
      "iteration: 126700 loss: 0.0024 lr: 0.02\n",
      "iteration: 126800 loss: 0.0024 lr: 0.02\n",
      "iteration: 126900 loss: 0.0021 lr: 0.02\n",
      "iteration: 127000 loss: 0.0020 lr: 0.02\n",
      "iteration: 127100 loss: 0.0022 lr: 0.02\n",
      "iteration: 127200 loss: 0.0023 lr: 0.02\n",
      "iteration: 127300 loss: 0.0022 lr: 0.02\n",
      "iteration: 127400 loss: 0.0026 lr: 0.02\n",
      "iteration: 127500 loss: 0.0019 lr: 0.02\n",
      "iteration: 127600 loss: 0.0023 lr: 0.02\n",
      "iteration: 127700 loss: 0.0020 lr: 0.02\n",
      "iteration: 127800 loss: 0.0025 lr: 0.02\n",
      "iteration: 127900 loss: 0.0024 lr: 0.02\n",
      "iteration: 128000 loss: 0.0021 lr: 0.02\n",
      "iteration: 128100 loss: 0.0024 lr: 0.02\n",
      "iteration: 128200 loss: 0.0023 lr: 0.02\n",
      "iteration: 128300 loss: 0.0024 lr: 0.02\n",
      "iteration: 128400 loss: 0.0022 lr: 0.02\n",
      "iteration: 128500 loss: 0.0021 lr: 0.02\n",
      "iteration: 128600 loss: 0.0022 lr: 0.02\n",
      "iteration: 128700 loss: 0.0020 lr: 0.02\n",
      "iteration: 128800 loss: 0.0024 lr: 0.02\n",
      "iteration: 128900 loss: 0.0022 lr: 0.02\n",
      "iteration: 129000 loss: 0.0023 lr: 0.02\n",
      "iteration: 129100 loss: 0.0023 lr: 0.02\n",
      "iteration: 129200 loss: 0.0025 lr: 0.02\n",
      "iteration: 129300 loss: 0.0022 lr: 0.02\n",
      "iteration: 129400 loss: 0.0025 lr: 0.02\n",
      "iteration: 129500 loss: 0.0023 lr: 0.02\n",
      "iteration: 129600 loss: 0.0023 lr: 0.02\n",
      "iteration: 129700 loss: 0.0022 lr: 0.02\n",
      "iteration: 129800 loss: 0.0023 lr: 0.02\n",
      "iteration: 129900 loss: 0.0021 lr: 0.02\n",
      "iteration: 130000 loss: 0.0024 lr: 0.02\n",
      "iteration: 130100 loss: 0.0024 lr: 0.02\n",
      "iteration: 130200 loss: 0.0021 lr: 0.02\n",
      "iteration: 130300 loss: 0.0022 lr: 0.02\n",
      "iteration: 130400 loss: 0.0021 lr: 0.02\n",
      "iteration: 130500 loss: 0.0020 lr: 0.02\n",
      "iteration: 130600 loss: 0.0020 lr: 0.02\n",
      "iteration: 130700 loss: 0.0023 lr: 0.02\n",
      "iteration: 130800 loss: 0.0023 lr: 0.02\n",
      "iteration: 130900 loss: 0.0021 lr: 0.02\n",
      "iteration: 131000 loss: 0.0022 lr: 0.02\n",
      "iteration: 131100 loss: 0.0020 lr: 0.02\n",
      "iteration: 131200 loss: 0.0019 lr: 0.02\n",
      "iteration: 131300 loss: 0.0020 lr: 0.02\n",
      "iteration: 131400 loss: 0.0022 lr: 0.02\n",
      "iteration: 131500 loss: 0.0021 lr: 0.02\n",
      "iteration: 131600 loss: 0.0020 lr: 0.02\n",
      "iteration: 131700 loss: 0.0019 lr: 0.02\n",
      "iteration: 131800 loss: 0.0024 lr: 0.02\n",
      "iteration: 131900 loss: 0.0022 lr: 0.02\n",
      "iteration: 132000 loss: 0.0026 lr: 0.02\n",
      "iteration: 132100 loss: 0.0023 lr: 0.02\n",
      "iteration: 132200 loss: 0.0022 lr: 0.02\n",
      "iteration: 132300 loss: 0.0021 lr: 0.02\n",
      "iteration: 132400 loss: 0.0022 lr: 0.02\n",
      "iteration: 132500 loss: 0.0025 lr: 0.02\n",
      "iteration: 132600 loss: 0.0022 lr: 0.02\n",
      "iteration: 132700 loss: 0.0022 lr: 0.02\n",
      "iteration: 132800 loss: 0.0020 lr: 0.02\n",
      "iteration: 132900 loss: 0.0018 lr: 0.02\n",
      "iteration: 133000 loss: 0.0022 lr: 0.02\n",
      "iteration: 133100 loss: 0.0024 lr: 0.02\n",
      "iteration: 133200 loss: 0.0025 lr: 0.02\n",
      "iteration: 133300 loss: 0.0022 lr: 0.02\n",
      "iteration: 133400 loss: 0.0022 lr: 0.02\n",
      "iteration: 133500 loss: 0.0023 lr: 0.02\n",
      "iteration: 133600 loss: 0.0023 lr: 0.02\n",
      "iteration: 133700 loss: 0.0023 lr: 0.02\n",
      "iteration: 133800 loss: 0.0022 lr: 0.02\n",
      "iteration: 133900 loss: 0.0022 lr: 0.02\n",
      "iteration: 134000 loss: 0.0020 lr: 0.02\n",
      "iteration: 134100 loss: 0.0020 lr: 0.02\n",
      "iteration: 134200 loss: 0.0023 lr: 0.02\n",
      "iteration: 134300 loss: 0.0021 lr: 0.02\n",
      "iteration: 134400 loss: 0.0023 lr: 0.02\n",
      "iteration: 134500 loss: 0.0021 lr: 0.02\n",
      "iteration: 134600 loss: 0.0022 lr: 0.02\n",
      "iteration: 134700 loss: 0.0022 lr: 0.02\n",
      "iteration: 134800 loss: 0.0022 lr: 0.02\n",
      "iteration: 134900 loss: 0.0023 lr: 0.02\n",
      "iteration: 135000 loss: 0.0023 lr: 0.02\n",
      "iteration: 135100 loss: 0.0026 lr: 0.02\n",
      "iteration: 135200 loss: 0.0022 lr: 0.02\n",
      "iteration: 135300 loss: 0.0020 lr: 0.02\n",
      "iteration: 135400 loss: 0.0021 lr: 0.02\n",
      "iteration: 135500 loss: 0.0021 lr: 0.02\n",
      "iteration: 135600 loss: 0.0022 lr: 0.02\n",
      "iteration: 135700 loss: 0.0022 lr: 0.02\n",
      "iteration: 135800 loss: 0.0021 lr: 0.02\n",
      "iteration: 135900 loss: 0.0021 lr: 0.02\n",
      "iteration: 136000 loss: 0.0021 lr: 0.02\n",
      "iteration: 136100 loss: 0.0019 lr: 0.02\n",
      "iteration: 136200 loss: 0.0019 lr: 0.02\n",
      "iteration: 136300 loss: 0.0020 lr: 0.02\n",
      "iteration: 136400 loss: 0.0020 lr: 0.02\n",
      "iteration: 136500 loss: 0.0021 lr: 0.02\n",
      "iteration: 136600 loss: 0.0022 lr: 0.02\n",
      "iteration: 136700 loss: 0.0019 lr: 0.02\n",
      "iteration: 136800 loss: 0.0021 lr: 0.02\n",
      "iteration: 136900 loss: 0.0022 lr: 0.02\n",
      "iteration: 137000 loss: 0.0022 lr: 0.02\n",
      "iteration: 137100 loss: 0.0018 lr: 0.02\n",
      "iteration: 137200 loss: 0.0022 lr: 0.02\n",
      "iteration: 137300 loss: 0.0021 lr: 0.02\n",
      "iteration: 137400 loss: 0.0021 lr: 0.02\n",
      "iteration: 137500 loss: 0.0023 lr: 0.02\n",
      "iteration: 137600 loss: 0.0018 lr: 0.02\n",
      "iteration: 137700 loss: 0.0023 lr: 0.02\n",
      "iteration: 137800 loss: 0.0023 lr: 0.02\n",
      "iteration: 137900 loss: 0.0022 lr: 0.02\n",
      "iteration: 138000 loss: 0.0023 lr: 0.02\n",
      "iteration: 138100 loss: 0.0023 lr: 0.02\n",
      "iteration: 138200 loss: 0.0020 lr: 0.02\n",
      "iteration: 138300 loss: 0.0020 lr: 0.02\n",
      "iteration: 138400 loss: 0.0020 lr: 0.02\n",
      "iteration: 138500 loss: 0.0022 lr: 0.02\n",
      "iteration: 138600 loss: 0.0023 lr: 0.02\n",
      "iteration: 138700 loss: 0.0021 lr: 0.02\n",
      "iteration: 138800 loss: 0.0024 lr: 0.02\n",
      "iteration: 138900 loss: 0.0021 lr: 0.02\n",
      "iteration: 139000 loss: 0.0023 lr: 0.02\n",
      "iteration: 139100 loss: 0.0024 lr: 0.02\n",
      "iteration: 139200 loss: 0.0022 lr: 0.02\n",
      "iteration: 139300 loss: 0.0020 lr: 0.02\n",
      "iteration: 139400 loss: 0.0024 lr: 0.02\n",
      "iteration: 139500 loss: 0.0021 lr: 0.02\n",
      "iteration: 139600 loss: 0.0022 lr: 0.02\n",
      "iteration: 139700 loss: 0.0022 lr: 0.02\n",
      "iteration: 139800 loss: 0.0021 lr: 0.02\n",
      "iteration: 139900 loss: 0.0022 lr: 0.02\n",
      "iteration: 140000 loss: 0.0022 lr: 0.02\n",
      "iteration: 140100 loss: 0.0024 lr: 0.02\n",
      "iteration: 140200 loss: 0.0021 lr: 0.02\n",
      "iteration: 140300 loss: 0.0021 lr: 0.02\n",
      "iteration: 140400 loss: 0.0025 lr: 0.02\n",
      "iteration: 140500 loss: 0.0024 lr: 0.02\n",
      "iteration: 140600 loss: 0.0020 lr: 0.02\n",
      "iteration: 140700 loss: 0.0022 lr: 0.02\n",
      "iteration: 140800 loss: 0.0021 lr: 0.02\n",
      "iteration: 140900 loss: 0.0024 lr: 0.02\n",
      "iteration: 141000 loss: 0.0021 lr: 0.02\n",
      "iteration: 141100 loss: 0.0021 lr: 0.02\n",
      "iteration: 141200 loss: 0.0022 lr: 0.02\n",
      "iteration: 141300 loss: 0.0022 lr: 0.02\n",
      "iteration: 141400 loss: 0.0019 lr: 0.02\n",
      "iteration: 141500 loss: 0.0020 lr: 0.02\n",
      "iteration: 141600 loss: 0.0023 lr: 0.02\n",
      "iteration: 141700 loss: 0.0021 lr: 0.02\n",
      "iteration: 141800 loss: 0.0021 lr: 0.02\n",
      "iteration: 141900 loss: 0.0022 lr: 0.02\n",
      "iteration: 142000 loss: 0.0020 lr: 0.02\n",
      "iteration: 142100 loss: 0.0022 lr: 0.02\n",
      "iteration: 142200 loss: 0.0021 lr: 0.02\n",
      "iteration: 142300 loss: 0.0021 lr: 0.02\n",
      "iteration: 142400 loss: 0.0020 lr: 0.02\n",
      "iteration: 142500 loss: 0.0020 lr: 0.02\n",
      "iteration: 142600 loss: 0.0021 lr: 0.02\n",
      "iteration: 142700 loss: 0.0021 lr: 0.02\n",
      "iteration: 142800 loss: 0.0021 lr: 0.02\n",
      "iteration: 142900 loss: 0.0019 lr: 0.02\n",
      "iteration: 143000 loss: 0.0021 lr: 0.02\n",
      "iteration: 143100 loss: 0.0021 lr: 0.02\n",
      "iteration: 143200 loss: 0.0023 lr: 0.02\n",
      "iteration: 143300 loss: 0.0024 lr: 0.02\n",
      "iteration: 143400 loss: 0.0022 lr: 0.02\n",
      "iteration: 143500 loss: 0.0022 lr: 0.02\n",
      "iteration: 143600 loss: 0.0024 lr: 0.02\n",
      "iteration: 143700 loss: 0.0019 lr: 0.02\n",
      "iteration: 143800 loss: 0.0021 lr: 0.02\n",
      "iteration: 143900 loss: 0.0023 lr: 0.02\n",
      "iteration: 144000 loss: 0.0020 lr: 0.02\n",
      "iteration: 144100 loss: 0.0023 lr: 0.02\n",
      "iteration: 144200 loss: 0.0022 lr: 0.02\n",
      "iteration: 144300 loss: 0.0022 lr: 0.02\n",
      "iteration: 144400 loss: 0.0020 lr: 0.02\n",
      "iteration: 144500 loss: 0.0020 lr: 0.02\n",
      "iteration: 144600 loss: 0.0021 lr: 0.02\n",
      "iteration: 144700 loss: 0.0021 lr: 0.02\n",
      "iteration: 144800 loss: 0.0020 lr: 0.02\n",
      "iteration: 144900 loss: 0.0022 lr: 0.02\n",
      "iteration: 145000 loss: 0.0021 lr: 0.02\n",
      "iteration: 145100 loss: 0.0020 lr: 0.02\n",
      "iteration: 145200 loss: 0.0021 lr: 0.02\n",
      "iteration: 145300 loss: 0.0022 lr: 0.02\n",
      "iteration: 145400 loss: 0.0021 lr: 0.02\n",
      "iteration: 145500 loss: 0.0023 lr: 0.02\n",
      "iteration: 145600 loss: 0.0019 lr: 0.02\n",
      "iteration: 145700 loss: 0.0022 lr: 0.02\n",
      "iteration: 145800 loss: 0.0020 lr: 0.02\n",
      "iteration: 145900 loss: 0.0018 lr: 0.02\n",
      "iteration: 146000 loss: 0.0022 lr: 0.02\n",
      "iteration: 146100 loss: 0.0025 lr: 0.02\n",
      "iteration: 146200 loss: 0.0019 lr: 0.02\n",
      "iteration: 146300 loss: 0.0021 lr: 0.02\n",
      "iteration: 146400 loss: 0.0019 lr: 0.02\n",
      "iteration: 146500 loss: 0.0022 lr: 0.02\n",
      "iteration: 146600 loss: 0.0021 lr: 0.02\n",
      "iteration: 146700 loss: 0.0018 lr: 0.02\n",
      "iteration: 146800 loss: 0.0021 lr: 0.02\n",
      "iteration: 146900 loss: 0.0020 lr: 0.02\n",
      "iteration: 147000 loss: 0.0019 lr: 0.02\n",
      "iteration: 147100 loss: 0.0027 lr: 0.02\n",
      "iteration: 147200 loss: 0.0023 lr: 0.02\n",
      "iteration: 147300 loss: 0.0022 lr: 0.02\n",
      "iteration: 147400 loss: 0.0020 lr: 0.02\n",
      "iteration: 147500 loss: 0.0020 lr: 0.02\n",
      "iteration: 147600 loss: 0.0024 lr: 0.02\n",
      "iteration: 147700 loss: 0.0020 lr: 0.02\n",
      "iteration: 147800 loss: 0.0020 lr: 0.02\n",
      "iteration: 147900 loss: 0.0020 lr: 0.02\n",
      "iteration: 148000 loss: 0.0021 lr: 0.02\n",
      "iteration: 148100 loss: 0.0020 lr: 0.02\n",
      "iteration: 148200 loss: 0.0023 lr: 0.02\n",
      "iteration: 148300 loss: 0.0021 lr: 0.02\n",
      "iteration: 148400 loss: 0.0023 lr: 0.02\n",
      "iteration: 148500 loss: 0.0025 lr: 0.02\n",
      "iteration: 148600 loss: 0.0020 lr: 0.02\n",
      "iteration: 148700 loss: 0.0021 lr: 0.02\n",
      "iteration: 148800 loss: 0.0019 lr: 0.02\n",
      "iteration: 148900 loss: 0.0022 lr: 0.02\n",
      "iteration: 149000 loss: 0.0020 lr: 0.02\n",
      "iteration: 149100 loss: 0.0021 lr: 0.02\n",
      "iteration: 149200 loss: 0.0018 lr: 0.02\n",
      "iteration: 149300 loss: 0.0020 lr: 0.02\n",
      "iteration: 149400 loss: 0.0020 lr: 0.02\n",
      "iteration: 149500 loss: 0.0019 lr: 0.02\n",
      "iteration: 149600 loss: 0.0018 lr: 0.02\n",
      "iteration: 149700 loss: 0.0022 lr: 0.02\n",
      "iteration: 149800 loss: 0.0021 lr: 0.02\n",
      "iteration: 149900 loss: 0.0019 lr: 0.02\n",
      "iteration: 150000 loss: 0.0018 lr: 0.02\n",
      "iteration: 150100 loss: 0.0019 lr: 0.02\n",
      "iteration: 150200 loss: 0.0022 lr: 0.02\n",
      "iteration: 150300 loss: 0.0018 lr: 0.02\n",
      "iteration: 150400 loss: 0.0021 lr: 0.02\n",
      "iteration: 150500 loss: 0.0022 lr: 0.02\n",
      "iteration: 150600 loss: 0.0020 lr: 0.02\n",
      "iteration: 150700 loss: 0.0019 lr: 0.02\n",
      "iteration: 150800 loss: 0.0021 lr: 0.02\n",
      "iteration: 150900 loss: 0.0019 lr: 0.02\n",
      "iteration: 151000 loss: 0.0018 lr: 0.02\n",
      "iteration: 151100 loss: 0.0020 lr: 0.02\n",
      "iteration: 151200 loss: 0.0021 lr: 0.02\n",
      "iteration: 151300 loss: 0.0021 lr: 0.02\n",
      "iteration: 151400 loss: 0.0018 lr: 0.02\n",
      "iteration: 151500 loss: 0.0023 lr: 0.02\n",
      "iteration: 151600 loss: 0.0022 lr: 0.02\n",
      "iteration: 151700 loss: 0.0020 lr: 0.02\n",
      "iteration: 151800 loss: 0.0021 lr: 0.02\n",
      "iteration: 151900 loss: 0.0022 lr: 0.02\n",
      "iteration: 152000 loss: 0.0021 lr: 0.02\n",
      "iteration: 152100 loss: 0.0022 lr: 0.02\n",
      "iteration: 152200 loss: 0.0023 lr: 0.02\n",
      "iteration: 152300 loss: 0.0020 lr: 0.02\n",
      "iteration: 152400 loss: 0.0020 lr: 0.02\n",
      "iteration: 152500 loss: 0.0020 lr: 0.02\n",
      "iteration: 152600 loss: 0.0021 lr: 0.02\n",
      "iteration: 152700 loss: 0.0019 lr: 0.02\n",
      "iteration: 152800 loss: 0.0022 lr: 0.02\n",
      "iteration: 152900 loss: 0.0019 lr: 0.02\n",
      "iteration: 153000 loss: 0.0020 lr: 0.02\n",
      "iteration: 153100 loss: 0.0022 lr: 0.02\n",
      "iteration: 153200 loss: 0.0020 lr: 0.02\n",
      "iteration: 153300 loss: 0.0021 lr: 0.02\n",
      "iteration: 153400 loss: 0.0020 lr: 0.02\n",
      "iteration: 153500 loss: 0.0020 lr: 0.02\n",
      "iteration: 153600 loss: 0.0022 lr: 0.02\n",
      "iteration: 153700 loss: 0.0020 lr: 0.02\n",
      "iteration: 153800 loss: 0.0020 lr: 0.02\n",
      "iteration: 153900 loss: 0.0019 lr: 0.02\n",
      "iteration: 154000 loss: 0.0020 lr: 0.02\n",
      "iteration: 154100 loss: 0.0020 lr: 0.02\n",
      "iteration: 154200 loss: 0.0018 lr: 0.02\n",
      "iteration: 154300 loss: 0.0021 lr: 0.02\n",
      "iteration: 154400 loss: 0.0020 lr: 0.02\n",
      "iteration: 154500 loss: 0.0022 lr: 0.02\n",
      "iteration: 154600 loss: 0.0022 lr: 0.02\n",
      "iteration: 154700 loss: 0.0021 lr: 0.02\n",
      "iteration: 154800 loss: 0.0019 lr: 0.02\n",
      "iteration: 154900 loss: 0.0021 lr: 0.02\n",
      "iteration: 155000 loss: 0.0019 lr: 0.02\n",
      "iteration: 155100 loss: 0.0019 lr: 0.02\n",
      "iteration: 155200 loss: 0.0018 lr: 0.02\n",
      "iteration: 155300 loss: 0.0021 lr: 0.02\n",
      "iteration: 155400 loss: 0.0020 lr: 0.02\n",
      "iteration: 155500 loss: 0.0018 lr: 0.02\n",
      "iteration: 155600 loss: 0.0021 lr: 0.02\n",
      "iteration: 155700 loss: 0.0020 lr: 0.02\n",
      "iteration: 155800 loss: 0.0020 lr: 0.02\n",
      "iteration: 155900 loss: 0.0017 lr: 0.02\n",
      "iteration: 156000 loss: 0.0019 lr: 0.02\n",
      "iteration: 156100 loss: 0.0018 lr: 0.02\n",
      "iteration: 156200 loss: 0.0017 lr: 0.02\n",
      "iteration: 156300 loss: 0.0017 lr: 0.02\n",
      "iteration: 156400 loss: 0.0020 lr: 0.02\n",
      "iteration: 156500 loss: 0.0019 lr: 0.02\n",
      "iteration: 156600 loss: 0.0020 lr: 0.02\n",
      "iteration: 156700 loss: 0.0019 lr: 0.02\n",
      "iteration: 156800 loss: 0.0019 lr: 0.02\n",
      "iteration: 156900 loss: 0.0019 lr: 0.02\n",
      "iteration: 157000 loss: 0.0020 lr: 0.02\n",
      "iteration: 157100 loss: 0.0019 lr: 0.02\n",
      "iteration: 157200 loss: 0.0019 lr: 0.02\n",
      "iteration: 157300 loss: 0.0020 lr: 0.02\n",
      "iteration: 157400 loss: 0.0018 lr: 0.02\n",
      "iteration: 157500 loss: 0.0018 lr: 0.02\n",
      "iteration: 157600 loss: 0.0023 lr: 0.02\n",
      "iteration: 157700 loss: 0.0017 lr: 0.02\n",
      "iteration: 157800 loss: 0.0020 lr: 0.02\n",
      "iteration: 157900 loss: 0.0022 lr: 0.02\n",
      "iteration: 158000 loss: 0.0020 lr: 0.02\n",
      "iteration: 158100 loss: 0.0019 lr: 0.02\n",
      "iteration: 158200 loss: 0.0021 lr: 0.02\n",
      "iteration: 158300 loss: 0.0020 lr: 0.02\n",
      "iteration: 158400 loss: 0.0019 lr: 0.02\n",
      "iteration: 158500 loss: 0.0021 lr: 0.02\n",
      "iteration: 158600 loss: 0.0019 lr: 0.02\n",
      "iteration: 158700 loss: 0.0021 lr: 0.02\n",
      "iteration: 158800 loss: 0.0019 lr: 0.02\n",
      "iteration: 158900 loss: 0.0022 lr: 0.02\n",
      "iteration: 159000 loss: 0.0019 lr: 0.02\n",
      "iteration: 159100 loss: 0.0021 lr: 0.02\n",
      "iteration: 159200 loss: 0.0018 lr: 0.02\n",
      "iteration: 159300 loss: 0.0021 lr: 0.02\n",
      "iteration: 159400 loss: 0.0021 lr: 0.02\n",
      "iteration: 159500 loss: 0.0019 lr: 0.02\n",
      "iteration: 159600 loss: 0.0019 lr: 0.02\n",
      "iteration: 159700 loss: 0.0019 lr: 0.02\n",
      "iteration: 159800 loss: 0.0020 lr: 0.02\n",
      "iteration: 159900 loss: 0.0019 lr: 0.02\n",
      "iteration: 160000 loss: 0.0019 lr: 0.02\n",
      "iteration: 160100 loss: 0.0021 lr: 0.02\n",
      "iteration: 160200 loss: 0.0020 lr: 0.02\n",
      "iteration: 160300 loss: 0.0018 lr: 0.02\n",
      "iteration: 160400 loss: 0.0020 lr: 0.02\n",
      "iteration: 160500 loss: 0.0019 lr: 0.02\n",
      "iteration: 160600 loss: 0.0019 lr: 0.02\n",
      "iteration: 160700 loss: 0.0019 lr: 0.02\n",
      "iteration: 160800 loss: 0.0018 lr: 0.02\n",
      "iteration: 160900 loss: 0.0019 lr: 0.02\n",
      "iteration: 161000 loss: 0.0022 lr: 0.02\n",
      "iteration: 161100 loss: 0.0020 lr: 0.02\n",
      "iteration: 161200 loss: 0.0021 lr: 0.02\n",
      "iteration: 161300 loss: 0.0020 lr: 0.02\n",
      "iteration: 161400 loss: 0.0021 lr: 0.02\n",
      "iteration: 161500 loss: 0.0021 lr: 0.02\n",
      "iteration: 161600 loss: 0.0020 lr: 0.02\n",
      "iteration: 161700 loss: 0.0018 lr: 0.02\n",
      "iteration: 161800 loss: 0.0018 lr: 0.02\n",
      "iteration: 161900 loss: 0.0021 lr: 0.02\n",
      "iteration: 162000 loss: 0.0020 lr: 0.02\n",
      "iteration: 162100 loss: 0.0021 lr: 0.02\n",
      "iteration: 162200 loss: 0.0019 lr: 0.02\n",
      "iteration: 162300 loss: 0.0017 lr: 0.02\n",
      "iteration: 162400 loss: 0.0019 lr: 0.02\n",
      "iteration: 162500 loss: 0.0023 lr: 0.02\n",
      "iteration: 162600 loss: 0.0019 lr: 0.02\n",
      "iteration: 162700 loss: 0.0020 lr: 0.02\n",
      "iteration: 162800 loss: 0.0020 lr: 0.02\n",
      "iteration: 162900 loss: 0.0018 lr: 0.02\n",
      "iteration: 163000 loss: 0.0020 lr: 0.02\n",
      "iteration: 163100 loss: 0.0018 lr: 0.02\n",
      "iteration: 163200 loss: 0.0016 lr: 0.02\n",
      "iteration: 163300 loss: 0.0021 lr: 0.02\n",
      "iteration: 163400 loss: 0.0021 lr: 0.02\n",
      "iteration: 163500 loss: 0.0020 lr: 0.02\n",
      "iteration: 163600 loss: 0.0020 lr: 0.02\n",
      "iteration: 163700 loss: 0.0019 lr: 0.02\n",
      "iteration: 163800 loss: 0.0021 lr: 0.02\n",
      "iteration: 163900 loss: 0.0021 lr: 0.02\n",
      "iteration: 164000 loss: 0.0023 lr: 0.02\n",
      "iteration: 164100 loss: 0.0020 lr: 0.02\n",
      "iteration: 164200 loss: 0.0022 lr: 0.02\n",
      "iteration: 164300 loss: 0.0018 lr: 0.02\n",
      "iteration: 164400 loss: 0.0021 lr: 0.02\n",
      "iteration: 164500 loss: 0.0021 lr: 0.02\n",
      "iteration: 164600 loss: 0.0019 lr: 0.02\n",
      "iteration: 164700 loss: 0.0018 lr: 0.02\n",
      "iteration: 164800 loss: 0.0021 lr: 0.02\n",
      "iteration: 164900 loss: 0.0017 lr: 0.02\n",
      "iteration: 165000 loss: 0.0020 lr: 0.02\n",
      "iteration: 165100 loss: 0.0020 lr: 0.02\n",
      "iteration: 165200 loss: 0.0020 lr: 0.02\n",
      "iteration: 165300 loss: 0.0020 lr: 0.02\n",
      "iteration: 165400 loss: 0.0019 lr: 0.02\n",
      "iteration: 165500 loss: 0.0020 lr: 0.02\n",
      "iteration: 165600 loss: 0.0020 lr: 0.02\n",
      "iteration: 165700 loss: 0.0020 lr: 0.02\n",
      "iteration: 165800 loss: 0.0019 lr: 0.02\n",
      "iteration: 165900 loss: 0.0019 lr: 0.02\n",
      "iteration: 166000 loss: 0.0020 lr: 0.02\n",
      "iteration: 166100 loss: 0.0021 lr: 0.02\n",
      "iteration: 166200 loss: 0.0017 lr: 0.02\n",
      "iteration: 166300 loss: 0.0020 lr: 0.02\n",
      "iteration: 166400 loss: 0.0021 lr: 0.02\n",
      "iteration: 166500 loss: 0.0021 lr: 0.02\n",
      "iteration: 166600 loss: 0.0020 lr: 0.02\n",
      "iteration: 166700 loss: 0.0020 lr: 0.02\n",
      "iteration: 166800 loss: 0.0020 lr: 0.02\n",
      "iteration: 166900 loss: 0.0020 lr: 0.02\n",
      "iteration: 167000 loss: 0.0020 lr: 0.02\n",
      "iteration: 167100 loss: 0.0020 lr: 0.02\n",
      "iteration: 167200 loss: 0.0019 lr: 0.02\n",
      "iteration: 167300 loss: 0.0017 lr: 0.02\n",
      "iteration: 167400 loss: 0.0019 lr: 0.02\n",
      "iteration: 167500 loss: 0.0019 lr: 0.02\n",
      "iteration: 167600 loss: 0.0019 lr: 0.02\n",
      "iteration: 167700 loss: 0.0019 lr: 0.02\n",
      "iteration: 167800 loss: 0.0019 lr: 0.02\n",
      "iteration: 167900 loss: 0.0020 lr: 0.02\n",
      "iteration: 168000 loss: 0.0019 lr: 0.02\n",
      "iteration: 168100 loss: 0.0018 lr: 0.02\n",
      "iteration: 168200 loss: 0.0020 lr: 0.02\n",
      "iteration: 168300 loss: 0.0019 lr: 0.02\n",
      "iteration: 168400 loss: 0.0018 lr: 0.02\n",
      "iteration: 168500 loss: 0.0020 lr: 0.02\n",
      "iteration: 168600 loss: 0.0020 lr: 0.02\n",
      "iteration: 168700 loss: 0.0019 lr: 0.02\n",
      "iteration: 168800 loss: 0.0019 lr: 0.02\n",
      "iteration: 168900 loss: 0.0020 lr: 0.02\n",
      "iteration: 169000 loss: 0.0019 lr: 0.02\n",
      "iteration: 169100 loss: 0.0018 lr: 0.02\n",
      "iteration: 169200 loss: 0.0019 lr: 0.02\n",
      "iteration: 169300 loss: 0.0019 lr: 0.02\n",
      "iteration: 169400 loss: 0.0019 lr: 0.02\n",
      "iteration: 169500 loss: 0.0018 lr: 0.02\n",
      "iteration: 169600 loss: 0.0019 lr: 0.02\n",
      "iteration: 169700 loss: 0.0020 lr: 0.02\n",
      "iteration: 169800 loss: 0.0018 lr: 0.02\n",
      "iteration: 169900 loss: 0.0018 lr: 0.02\n",
      "iteration: 170000 loss: 0.0019 lr: 0.02\n",
      "iteration: 170100 loss: 0.0017 lr: 0.02\n",
      "iteration: 170200 loss: 0.0020 lr: 0.02\n",
      "iteration: 170300 loss: 0.0018 lr: 0.02\n",
      "iteration: 170400 loss: 0.0019 lr: 0.02\n",
      "iteration: 170500 loss: 0.0018 lr: 0.02\n",
      "iteration: 170600 loss: 0.0018 lr: 0.02\n",
      "iteration: 170700 loss: 0.0019 lr: 0.02\n",
      "iteration: 170800 loss: 0.0022 lr: 0.02\n",
      "iteration: 170900 loss: 0.0020 lr: 0.02\n",
      "iteration: 171000 loss: 0.0022 lr: 0.02\n",
      "iteration: 171100 loss: 0.0020 lr: 0.02\n",
      "iteration: 171200 loss: 0.0020 lr: 0.02\n",
      "iteration: 171300 loss: 0.0020 lr: 0.02\n",
      "iteration: 171400 loss: 0.0018 lr: 0.02\n",
      "iteration: 171500 loss: 0.0018 lr: 0.02\n",
      "iteration: 171600 loss: 0.0021 lr: 0.02\n",
      "iteration: 171700 loss: 0.0019 lr: 0.02\n",
      "iteration: 171800 loss: 0.0019 lr: 0.02\n",
      "iteration: 171900 loss: 0.0019 lr: 0.02\n",
      "iteration: 172000 loss: 0.0020 lr: 0.02\n",
      "iteration: 172100 loss: 0.0020 lr: 0.02\n",
      "iteration: 172200 loss: 0.0020 lr: 0.02\n",
      "iteration: 172300 loss: 0.0019 lr: 0.02\n",
      "iteration: 172400 loss: 0.0019 lr: 0.02\n",
      "iteration: 172500 loss: 0.0020 lr: 0.02\n",
      "iteration: 172600 loss: 0.0018 lr: 0.02\n",
      "iteration: 172700 loss: 0.0020 lr: 0.02\n",
      "iteration: 172800 loss: 0.0019 lr: 0.02\n",
      "iteration: 172900 loss: 0.0019 lr: 0.02\n",
      "iteration: 173000 loss: 0.0020 lr: 0.02\n",
      "iteration: 173100 loss: 0.0018 lr: 0.02\n",
      "iteration: 173200 loss: 0.0020 lr: 0.02\n",
      "iteration: 173300 loss: 0.0019 lr: 0.02\n",
      "iteration: 173400 loss: 0.0018 lr: 0.02\n",
      "iteration: 173500 loss: 0.0018 lr: 0.02\n",
      "iteration: 173600 loss: 0.0019 lr: 0.02\n",
      "iteration: 173700 loss: 0.0021 lr: 0.02\n",
      "iteration: 173800 loss: 0.0019 lr: 0.02\n",
      "iteration: 173900 loss: 0.0018 lr: 0.02\n",
      "iteration: 174000 loss: 0.0018 lr: 0.02\n",
      "iteration: 174100 loss: 0.0022 lr: 0.02\n",
      "iteration: 174200 loss: 0.0019 lr: 0.02\n",
      "iteration: 174300 loss: 0.0019 lr: 0.02\n",
      "iteration: 174400 loss: 0.0017 lr: 0.02\n",
      "iteration: 174500 loss: 0.0021 lr: 0.02\n",
      "iteration: 174600 loss: 0.0017 lr: 0.02\n",
      "iteration: 174700 loss: 0.0020 lr: 0.02\n",
      "iteration: 174800 loss: 0.0018 lr: 0.02\n",
      "iteration: 174900 loss: 0.0019 lr: 0.02\n",
      "iteration: 175000 loss: 0.0018 lr: 0.02\n",
      "iteration: 175100 loss: 0.0020 lr: 0.02\n",
      "iteration: 175200 loss: 0.0019 lr: 0.02\n",
      "iteration: 175300 loss: 0.0017 lr: 0.02\n",
      "iteration: 175400 loss: 0.0020 lr: 0.02\n",
      "iteration: 175500 loss: 0.0017 lr: 0.02\n",
      "iteration: 175600 loss: 0.0021 lr: 0.02\n",
      "iteration: 175700 loss: 0.0022 lr: 0.02\n",
      "iteration: 175800 loss: 0.0020 lr: 0.02\n",
      "iteration: 175900 loss: 0.0017 lr: 0.02\n",
      "iteration: 176000 loss: 0.0018 lr: 0.02\n",
      "iteration: 176100 loss: 0.0019 lr: 0.02\n",
      "iteration: 176200 loss: 0.0020 lr: 0.02\n",
      "iteration: 176300 loss: 0.0019 lr: 0.02\n",
      "iteration: 176400 loss: 0.0021 lr: 0.02\n",
      "iteration: 176500 loss: 0.0019 lr: 0.02\n",
      "iteration: 176600 loss: 0.0021 lr: 0.02\n",
      "iteration: 176700 loss: 0.0019 lr: 0.02\n",
      "iteration: 176800 loss: 0.0017 lr: 0.02\n",
      "iteration: 176900 loss: 0.0019 lr: 0.02\n",
      "iteration: 177000 loss: 0.0020 lr: 0.02\n",
      "iteration: 177100 loss: 0.0020 lr: 0.02\n",
      "iteration: 177200 loss: 0.0018 lr: 0.02\n",
      "iteration: 177300 loss: 0.0018 lr: 0.02\n",
      "iteration: 177400 loss: 0.0018 lr: 0.02\n",
      "iteration: 177500 loss: 0.0020 lr: 0.02\n",
      "iteration: 177600 loss: 0.0019 lr: 0.02\n",
      "iteration: 177700 loss: 0.0019 lr: 0.02\n",
      "iteration: 177800 loss: 0.0019 lr: 0.02\n",
      "iteration: 177900 loss: 0.0020 lr: 0.02\n",
      "iteration: 178000 loss: 0.0020 lr: 0.02\n",
      "iteration: 178100 loss: 0.0021 lr: 0.02\n",
      "iteration: 178200 loss: 0.0016 lr: 0.02\n",
      "iteration: 178300 loss: 0.0016 lr: 0.02\n",
      "iteration: 178400 loss: 0.0018 lr: 0.02\n",
      "iteration: 178500 loss: 0.0019 lr: 0.02\n",
      "iteration: 178600 loss: 0.0018 lr: 0.02\n",
      "iteration: 178700 loss: 0.0018 lr: 0.02\n",
      "iteration: 178800 loss: 0.0019 lr: 0.02\n",
      "iteration: 178900 loss: 0.0018 lr: 0.02\n",
      "iteration: 179000 loss: 0.0019 lr: 0.02\n",
      "iteration: 179100 loss: 0.0018 lr: 0.02\n",
      "iteration: 179200 loss: 0.0018 lr: 0.02\n",
      "iteration: 179300 loss: 0.0019 lr: 0.02\n",
      "iteration: 179400 loss: 0.0018 lr: 0.02\n",
      "iteration: 179500 loss: 0.0019 lr: 0.02\n",
      "iteration: 179600 loss: 0.0020 lr: 0.02\n",
      "iteration: 179700 loss: 0.0020 lr: 0.02\n",
      "iteration: 179800 loss: 0.0018 lr: 0.02\n",
      "iteration: 179900 loss: 0.0018 lr: 0.02\n",
      "iteration: 180000 loss: 0.0019 lr: 0.02\n",
      "iteration: 180100 loss: 0.0019 lr: 0.02\n",
      "iteration: 180200 loss: 0.0019 lr: 0.02\n",
      "iteration: 180300 loss: 0.0018 lr: 0.02\n",
      "iteration: 180400 loss: 0.0020 lr: 0.02\n",
      "iteration: 180500 loss: 0.0020 lr: 0.02\n",
      "iteration: 180600 loss: 0.0018 lr: 0.02\n",
      "iteration: 180700 loss: 0.0019 lr: 0.02\n",
      "iteration: 180800 loss: 0.0019 lr: 0.02\n",
      "iteration: 180900 loss: 0.0020 lr: 0.02\n",
      "iteration: 181000 loss: 0.0018 lr: 0.02\n",
      "iteration: 181100 loss: 0.0018 lr: 0.02\n",
      "iteration: 181200 loss: 0.0019 lr: 0.02\n",
      "iteration: 181300 loss: 0.0017 lr: 0.02\n",
      "iteration: 181400 loss: 0.0016 lr: 0.02\n",
      "iteration: 181500 loss: 0.0020 lr: 0.02\n",
      "iteration: 181600 loss: 0.0018 lr: 0.02\n",
      "iteration: 181700 loss: 0.0018 lr: 0.02\n",
      "iteration: 181800 loss: 0.0019 lr: 0.02\n",
      "iteration: 181900 loss: 0.0019 lr: 0.02\n",
      "iteration: 182000 loss: 0.0018 lr: 0.02\n",
      "iteration: 182100 loss: 0.0020 lr: 0.02\n",
      "iteration: 182200 loss: 0.0021 lr: 0.02\n",
      "iteration: 182300 loss: 0.0019 lr: 0.02\n",
      "iteration: 182400 loss: 0.0020 lr: 0.02\n",
      "iteration: 182500 loss: 0.0019 lr: 0.02\n",
      "iteration: 182600 loss: 0.0019 lr: 0.02\n",
      "iteration: 182700 loss: 0.0021 lr: 0.02\n",
      "iteration: 182800 loss: 0.0018 lr: 0.02\n",
      "iteration: 182900 loss: 0.0021 lr: 0.02\n",
      "iteration: 183000 loss: 0.0019 lr: 0.02\n",
      "iteration: 183100 loss: 0.0020 lr: 0.02\n",
      "iteration: 183200 loss: 0.0019 lr: 0.02\n",
      "iteration: 183300 loss: 0.0019 lr: 0.02\n",
      "iteration: 183400 loss: 0.0020 lr: 0.02\n",
      "iteration: 183500 loss: 0.0019 lr: 0.02\n",
      "iteration: 183600 loss: 0.0018 lr: 0.02\n",
      "iteration: 183700 loss: 0.0019 lr: 0.02\n",
      "iteration: 183800 loss: 0.0019 lr: 0.02\n",
      "iteration: 183900 loss: 0.0020 lr: 0.02\n",
      "iteration: 184000 loss: 0.0017 lr: 0.02\n",
      "iteration: 184100 loss: 0.0019 lr: 0.02\n",
      "iteration: 184200 loss: 0.0017 lr: 0.02\n",
      "iteration: 184300 loss: 0.0017 lr: 0.02\n",
      "iteration: 184400 loss: 0.0019 lr: 0.02\n",
      "iteration: 184500 loss: 0.0018 lr: 0.02\n",
      "iteration: 184600 loss: 0.0018 lr: 0.02\n",
      "iteration: 184700 loss: 0.0016 lr: 0.02\n",
      "iteration: 184800 loss: 0.0017 lr: 0.02\n",
      "iteration: 184900 loss: 0.0018 lr: 0.02\n",
      "iteration: 185000 loss: 0.0017 lr: 0.02\n",
      "iteration: 185100 loss: 0.0019 lr: 0.02\n",
      "iteration: 185200 loss: 0.0017 lr: 0.02\n",
      "iteration: 185300 loss: 0.0017 lr: 0.02\n",
      "iteration: 185400 loss: 0.0021 lr: 0.02\n",
      "iteration: 185500 loss: 0.0019 lr: 0.02\n",
      "iteration: 185600 loss: 0.0017 lr: 0.02\n",
      "iteration: 185700 loss: 0.0016 lr: 0.02\n",
      "iteration: 185800 loss: 0.0018 lr: 0.02\n",
      "iteration: 185900 loss: 0.0020 lr: 0.02\n",
      "iteration: 186000 loss: 0.0019 lr: 0.02\n",
      "iteration: 186100 loss: 0.0021 lr: 0.02\n",
      "iteration: 186200 loss: 0.0018 lr: 0.02\n",
      "iteration: 186300 loss: 0.0018 lr: 0.02\n",
      "iteration: 186400 loss: 0.0017 lr: 0.02\n",
      "iteration: 186500 loss: 0.0017 lr: 0.02\n",
      "iteration: 186600 loss: 0.0019 lr: 0.02\n",
      "iteration: 186700 loss: 0.0018 lr: 0.02\n",
      "iteration: 186800 loss: 0.0019 lr: 0.02\n",
      "iteration: 186900 loss: 0.0018 lr: 0.02\n",
      "iteration: 187000 loss: 0.0018 lr: 0.02\n",
      "iteration: 187100 loss: 0.0016 lr: 0.02\n",
      "iteration: 187200 loss: 0.0019 lr: 0.02\n",
      "iteration: 187300 loss: 0.0019 lr: 0.02\n",
      "iteration: 187400 loss: 0.0019 lr: 0.02\n",
      "iteration: 187500 loss: 0.0019 lr: 0.02\n",
      "iteration: 187600 loss: 0.0018 lr: 0.02\n",
      "iteration: 187700 loss: 0.0018 lr: 0.02\n",
      "iteration: 187800 loss: 0.0018 lr: 0.02\n",
      "iteration: 187900 loss: 0.0018 lr: 0.02\n",
      "iteration: 188000 loss: 0.0018 lr: 0.02\n",
      "iteration: 188100 loss: 0.0017 lr: 0.02\n",
      "iteration: 188200 loss: 0.0017 lr: 0.02\n",
      "iteration: 188300 loss: 0.0017 lr: 0.02\n",
      "iteration: 188400 loss: 0.0019 lr: 0.02\n",
      "iteration: 188500 loss: 0.0019 lr: 0.02\n",
      "iteration: 188600 loss: 0.0018 lr: 0.02\n",
      "iteration: 188700 loss: 0.0018 lr: 0.02\n",
      "iteration: 188800 loss: 0.0018 lr: 0.02\n",
      "iteration: 188900 loss: 0.0018 lr: 0.02\n",
      "iteration: 189000 loss: 0.0019 lr: 0.02\n",
      "iteration: 189100 loss: 0.0021 lr: 0.02\n",
      "iteration: 189200 loss: 0.0016 lr: 0.02\n",
      "iteration: 189300 loss: 0.0017 lr: 0.02\n",
      "iteration: 189400 loss: 0.0019 lr: 0.02\n",
      "iteration: 189500 loss: 0.0018 lr: 0.02\n",
      "iteration: 189600 loss: 0.0018 lr: 0.02\n",
      "iteration: 189700 loss: 0.0019 lr: 0.02\n",
      "iteration: 189800 loss: 0.0020 lr: 0.02\n",
      "iteration: 189900 loss: 0.0018 lr: 0.02\n",
      "iteration: 190000 loss: 0.0018 lr: 0.02\n",
      "iteration: 190100 loss: 0.0019 lr: 0.02\n",
      "iteration: 190200 loss: 0.0018 lr: 0.02\n",
      "iteration: 190300 loss: 0.0019 lr: 0.02\n",
      "iteration: 190400 loss: 0.0018 lr: 0.02\n",
      "iteration: 190500 loss: 0.0017 lr: 0.02\n",
      "iteration: 190600 loss: 0.0017 lr: 0.02\n",
      "iteration: 190700 loss: 0.0020 lr: 0.02\n",
      "iteration: 190800 loss: 0.0019 lr: 0.02\n",
      "iteration: 190900 loss: 0.0017 lr: 0.02\n",
      "iteration: 191000 loss: 0.0017 lr: 0.02\n",
      "iteration: 191100 loss: 0.0016 lr: 0.02\n",
      "iteration: 191200 loss: 0.0018 lr: 0.02\n",
      "iteration: 191300 loss: 0.0016 lr: 0.02\n",
      "iteration: 191400 loss: 0.0019 lr: 0.02\n",
      "iteration: 191500 loss: 0.0018 lr: 0.02\n",
      "iteration: 191600 loss: 0.0016 lr: 0.02\n",
      "iteration: 191700 loss: 0.0018 lr: 0.02\n",
      "iteration: 191800 loss: 0.0017 lr: 0.02\n",
      "iteration: 191900 loss: 0.0017 lr: 0.02\n",
      "iteration: 192000 loss: 0.0016 lr: 0.02\n",
      "iteration: 192100 loss: 0.0018 lr: 0.02\n",
      "iteration: 192200 loss: 0.0019 lr: 0.02\n",
      "iteration: 192300 loss: 0.0018 lr: 0.02\n",
      "iteration: 192400 loss: 0.0018 lr: 0.02\n",
      "iteration: 192500 loss: 0.0017 lr: 0.02\n",
      "iteration: 192600 loss: 0.0017 lr: 0.02\n",
      "iteration: 192700 loss: 0.0017 lr: 0.02\n",
      "iteration: 192800 loss: 0.0016 lr: 0.02\n",
      "iteration: 192900 loss: 0.0018 lr: 0.02\n",
      "iteration: 193000 loss: 0.0018 lr: 0.02\n",
      "iteration: 193100 loss: 0.0020 lr: 0.02\n",
      "iteration: 193200 loss: 0.0018 lr: 0.02\n",
      "iteration: 193300 loss: 0.0017 lr: 0.02\n",
      "iteration: 193400 loss: 0.0017 lr: 0.02\n",
      "iteration: 193500 loss: 0.0017 lr: 0.02\n",
      "iteration: 193600 loss: 0.0019 lr: 0.02\n",
      "iteration: 193700 loss: 0.0016 lr: 0.02\n",
      "iteration: 193800 loss: 0.0015 lr: 0.02\n",
      "iteration: 193900 loss: 0.0018 lr: 0.02\n",
      "iteration: 194000 loss: 0.0019 lr: 0.02\n",
      "iteration: 194100 loss: 0.0017 lr: 0.02\n",
      "iteration: 194200 loss: 0.0018 lr: 0.02\n",
      "iteration: 194300 loss: 0.0017 lr: 0.02\n",
      "iteration: 194400 loss: 0.0019 lr: 0.02\n",
      "iteration: 194500 loss: 0.0018 lr: 0.02\n",
      "iteration: 194600 loss: 0.0016 lr: 0.02\n",
      "iteration: 194700 loss: 0.0018 lr: 0.02\n",
      "iteration: 194800 loss: 0.0017 lr: 0.02\n",
      "iteration: 194900 loss: 0.0020 lr: 0.02\n",
      "iteration: 195000 loss: 0.0020 lr: 0.02\n",
      "iteration: 195100 loss: 0.0018 lr: 0.02\n",
      "iteration: 195200 loss: 0.0017 lr: 0.02\n",
      "iteration: 195300 loss: 0.0017 lr: 0.02\n",
      "iteration: 195400 loss: 0.0019 lr: 0.02\n",
      "iteration: 195500 loss: 0.0018 lr: 0.02\n",
      "iteration: 195600 loss: 0.0019 lr: 0.02\n",
      "iteration: 195700 loss: 0.0017 lr: 0.02\n",
      "iteration: 195800 loss: 0.0017 lr: 0.02\n",
      "iteration: 195900 loss: 0.0017 lr: 0.02\n",
      "iteration: 196000 loss: 0.0017 lr: 0.02\n",
      "iteration: 196100 loss: 0.0017 lr: 0.02\n",
      "iteration: 196200 loss: 0.0018 lr: 0.02\n",
      "iteration: 196300 loss: 0.0019 lr: 0.02\n",
      "iteration: 196400 loss: 0.0019 lr: 0.02\n",
      "iteration: 196500 loss: 0.0016 lr: 0.02\n",
      "iteration: 196600 loss: 0.0018 lr: 0.02\n",
      "iteration: 196700 loss: 0.0017 lr: 0.02\n",
      "iteration: 196800 loss: 0.0017 lr: 0.02\n",
      "iteration: 196900 loss: 0.0017 lr: 0.02\n",
      "iteration: 197000 loss: 0.0016 lr: 0.02\n",
      "iteration: 197100 loss: 0.0018 lr: 0.02\n",
      "iteration: 197200 loss: 0.0017 lr: 0.02\n",
      "iteration: 197300 loss: 0.0019 lr: 0.02\n",
      "iteration: 197400 loss: 0.0018 lr: 0.02\n",
      "iteration: 197500 loss: 0.0018 lr: 0.02\n",
      "iteration: 197600 loss: 0.0018 lr: 0.02\n",
      "iteration: 197700 loss: 0.0018 lr: 0.02\n",
      "iteration: 197800 loss: 0.0020 lr: 0.02\n",
      "iteration: 197900 loss: 0.0017 lr: 0.02\n",
      "iteration: 198000 loss: 0.0018 lr: 0.02\n",
      "iteration: 198100 loss: 0.0019 lr: 0.02\n",
      "iteration: 198200 loss: 0.0018 lr: 0.02\n",
      "iteration: 198300 loss: 0.0018 lr: 0.02\n",
      "iteration: 198400 loss: 0.0020 lr: 0.02\n",
      "iteration: 198500 loss: 0.0017 lr: 0.02\n",
      "iteration: 198600 loss: 0.0018 lr: 0.02\n",
      "iteration: 198700 loss: 0.0018 lr: 0.02\n",
      "iteration: 198800 loss: 0.0016 lr: 0.02\n",
      "iteration: 198900 loss: 0.0017 lr: 0.02\n",
      "iteration: 199000 loss: 0.0017 lr: 0.02\n",
      "iteration: 199100 loss: 0.0016 lr: 0.02\n",
      "iteration: 199200 loss: 0.0020 lr: 0.02\n",
      "iteration: 199300 loss: 0.0017 lr: 0.02\n",
      "iteration: 199400 loss: 0.0018 lr: 0.02\n",
      "iteration: 199500 loss: 0.0017 lr: 0.02\n",
      "iteration: 199600 loss: 0.0017 lr: 0.02\n",
      "iteration: 199700 loss: 0.0016 lr: 0.02\n",
      "iteration: 199800 loss: 0.0018 lr: 0.02\n",
      "iteration: 199900 loss: 0.0018 lr: 0.02\n",
      "iteration: 200000 loss: 0.0018 lr: 0.02\n",
      "iteration: 200100 loss: 0.0018 lr: 0.02\n",
      "iteration: 200200 loss: 0.0017 lr: 0.02\n",
      "iteration: 200300 loss: 0.0017 lr: 0.02\n",
      "iteration: 200400 loss: 0.0017 lr: 0.02\n",
      "iteration: 200500 loss: 0.0016 lr: 0.02\n",
      "iteration: 200600 loss: 0.0016 lr: 0.02\n",
      "iteration: 200700 loss: 0.0017 lr: 0.02\n",
      "iteration: 200800 loss: 0.0015 lr: 0.02\n",
      "iteration: 200900 loss: 0.0017 lr: 0.02\n",
      "iteration: 201000 loss: 0.0019 lr: 0.02\n",
      "iteration: 201100 loss: 0.0018 lr: 0.02\n",
      "iteration: 201200 loss: 0.0017 lr: 0.02\n",
      "iteration: 201300 loss: 0.0016 lr: 0.02\n",
      "iteration: 201400 loss: 0.0016 lr: 0.02\n",
      "iteration: 201500 loss: 0.0018 lr: 0.02\n",
      "iteration: 201600 loss: 0.0018 lr: 0.02\n",
      "iteration: 201700 loss: 0.0017 lr: 0.02\n",
      "iteration: 201800 loss: 0.0016 lr: 0.02\n",
      "iteration: 201900 loss: 0.0016 lr: 0.02\n",
      "iteration: 202000 loss: 0.0018 lr: 0.02\n",
      "iteration: 202100 loss: 0.0017 lr: 0.02\n",
      "iteration: 202200 loss: 0.0016 lr: 0.02\n",
      "iteration: 202300 loss: 0.0017 lr: 0.02\n",
      "iteration: 202400 loss: 0.0015 lr: 0.02\n",
      "iteration: 202500 loss: 0.0018 lr: 0.02\n",
      "iteration: 202600 loss: 0.0018 lr: 0.02\n",
      "iteration: 202700 loss: 0.0018 lr: 0.02\n",
      "iteration: 202800 loss: 0.0018 lr: 0.02\n",
      "iteration: 202900 loss: 0.0016 lr: 0.02\n",
      "iteration: 203000 loss: 0.0017 lr: 0.02\n",
      "iteration: 203100 loss: 0.0017 lr: 0.02\n",
      "iteration: 203200 loss: 0.0016 lr: 0.02\n",
      "iteration: 203300 loss: 0.0017 lr: 0.02\n",
      "iteration: 203400 loss: 0.0017 lr: 0.02\n",
      "iteration: 203500 loss: 0.0017 lr: 0.02\n",
      "iteration: 203600 loss: 0.0017 lr: 0.02\n",
      "iteration: 203700 loss: 0.0015 lr: 0.02\n",
      "iteration: 203800 loss: 0.0020 lr: 0.02\n",
      "iteration: 203900 loss: 0.0018 lr: 0.02\n",
      "iteration: 204000 loss: 0.0016 lr: 0.02\n",
      "iteration: 204100 loss: 0.0017 lr: 0.02\n",
      "iteration: 204200 loss: 0.0017 lr: 0.02\n",
      "iteration: 204300 loss: 0.0017 lr: 0.02\n",
      "iteration: 204400 loss: 0.0017 lr: 0.02\n",
      "iteration: 204500 loss: 0.0019 lr: 0.02\n",
      "iteration: 204600 loss: 0.0019 lr: 0.02\n",
      "iteration: 204700 loss: 0.0015 lr: 0.02\n",
      "iteration: 204800 loss: 0.0017 lr: 0.02\n",
      "iteration: 204900 loss: 0.0018 lr: 0.02\n",
      "iteration: 205000 loss: 0.0017 lr: 0.02\n",
      "iteration: 205100 loss: 0.0015 lr: 0.02\n",
      "iteration: 205200 loss: 0.0017 lr: 0.02\n",
      "iteration: 205300 loss: 0.0018 lr: 0.02\n",
      "iteration: 205400 loss: 0.0019 lr: 0.02\n",
      "iteration: 205500 loss: 0.0017 lr: 0.02\n",
      "iteration: 205600 loss: 0.0017 lr: 0.02\n",
      "iteration: 205700 loss: 0.0016 lr: 0.02\n",
      "iteration: 205800 loss: 0.0017 lr: 0.02\n",
      "iteration: 205900 loss: 0.0021 lr: 0.02\n",
      "iteration: 206000 loss: 0.0017 lr: 0.02\n",
      "iteration: 206100 loss: 0.0017 lr: 0.02\n",
      "iteration: 206200 loss: 0.0017 lr: 0.02\n",
      "iteration: 206300 loss: 0.0019 lr: 0.02\n",
      "iteration: 206400 loss: 0.0017 lr: 0.02\n",
      "iteration: 206500 loss: 0.0018 lr: 0.02\n",
      "iteration: 206600 loss: 0.0016 lr: 0.02\n",
      "iteration: 206700 loss: 0.0016 lr: 0.02\n",
      "iteration: 206800 loss: 0.0018 lr: 0.02\n",
      "iteration: 206900 loss: 0.0018 lr: 0.02\n",
      "iteration: 207000 loss: 0.0016 lr: 0.02\n",
      "iteration: 207100 loss: 0.0017 lr: 0.02\n",
      "iteration: 207200 loss: 0.0017 lr: 0.02\n",
      "iteration: 207300 loss: 0.0017 lr: 0.02\n",
      "iteration: 207400 loss: 0.0019 lr: 0.02\n",
      "iteration: 207500 loss: 0.0016 lr: 0.02\n",
      "iteration: 207600 loss: 0.0018 lr: 0.02\n",
      "iteration: 207700 loss: 0.0018 lr: 0.02\n",
      "iteration: 207800 loss: 0.0017 lr: 0.02\n",
      "iteration: 207900 loss: 0.0016 lr: 0.02\n",
      "iteration: 208000 loss: 0.0017 lr: 0.02\n",
      "iteration: 208100 loss: 0.0016 lr: 0.02\n",
      "iteration: 208200 loss: 0.0016 lr: 0.02\n",
      "iteration: 208300 loss: 0.0015 lr: 0.02\n",
      "iteration: 208400 loss: 0.0015 lr: 0.02\n",
      "iteration: 208500 loss: 0.0017 lr: 0.02\n",
      "iteration: 208600 loss: 0.0016 lr: 0.02\n",
      "iteration: 208700 loss: 0.0018 lr: 0.02\n",
      "iteration: 208800 loss: 0.0018 lr: 0.02\n",
      "iteration: 208900 loss: 0.0017 lr: 0.02\n",
      "iteration: 209000 loss: 0.0017 lr: 0.02\n",
      "iteration: 209100 loss: 0.0017 lr: 0.02\n",
      "iteration: 209200 loss: 0.0017 lr: 0.02\n",
      "iteration: 209300 loss: 0.0015 lr: 0.02\n",
      "iteration: 209400 loss: 0.0016 lr: 0.02\n",
      "iteration: 209500 loss: 0.0016 lr: 0.02\n",
      "iteration: 209600 loss: 0.0018 lr: 0.02\n",
      "iteration: 209700 loss: 0.0017 lr: 0.02\n",
      "iteration: 209800 loss: 0.0017 lr: 0.02\n",
      "iteration: 209900 loss: 0.0016 lr: 0.02\n",
      "iteration: 210000 loss: 0.0017 lr: 0.02\n",
      "iteration: 210100 loss: 0.0018 lr: 0.02\n",
      "iteration: 210200 loss: 0.0016 lr: 0.02\n",
      "iteration: 210300 loss: 0.0017 lr: 0.02\n",
      "iteration: 210400 loss: 0.0018 lr: 0.02\n",
      "iteration: 210500 loss: 0.0018 lr: 0.02\n",
      "iteration: 210600 loss: 0.0017 lr: 0.02\n",
      "iteration: 210700 loss: 0.0016 lr: 0.02\n",
      "iteration: 210800 loss: 0.0016 lr: 0.02\n",
      "iteration: 210900 loss: 0.0017 lr: 0.02\n",
      "iteration: 211000 loss: 0.0018 lr: 0.02\n",
      "iteration: 211100 loss: 0.0017 lr: 0.02\n",
      "iteration: 211200 loss: 0.0017 lr: 0.02\n",
      "iteration: 211300 loss: 0.0017 lr: 0.02\n",
      "iteration: 211400 loss: 0.0017 lr: 0.02\n",
      "iteration: 211500 loss: 0.0018 lr: 0.02\n",
      "iteration: 211600 loss: 0.0016 lr: 0.02\n",
      "iteration: 211700 loss: 0.0017 lr: 0.02\n",
      "iteration: 211800 loss: 0.0017 lr: 0.02\n",
      "iteration: 211900 loss: 0.0017 lr: 0.02\n",
      "iteration: 212000 loss: 0.0017 lr: 0.02\n",
      "iteration: 212100 loss: 0.0016 lr: 0.02\n",
      "iteration: 212200 loss: 0.0017 lr: 0.02\n",
      "iteration: 212300 loss: 0.0020 lr: 0.02\n",
      "iteration: 212400 loss: 0.0018 lr: 0.02\n",
      "iteration: 212500 loss: 0.0016 lr: 0.02\n",
      "iteration: 212600 loss: 0.0020 lr: 0.02\n",
      "iteration: 212700 loss: 0.0017 lr: 0.02\n",
      "iteration: 212800 loss: 0.0017 lr: 0.02\n",
      "iteration: 212900 loss: 0.0016 lr: 0.02\n",
      "iteration: 213000 loss: 0.0017 lr: 0.02\n",
      "iteration: 213100 loss: 0.0018 lr: 0.02\n",
      "iteration: 213200 loss: 0.0019 lr: 0.02\n",
      "iteration: 213300 loss: 0.0018 lr: 0.02\n",
      "iteration: 213400 loss: 0.0018 lr: 0.02\n",
      "iteration: 213500 loss: 0.0017 lr: 0.02\n",
      "iteration: 213600 loss: 0.0017 lr: 0.02\n",
      "iteration: 213700 loss: 0.0015 lr: 0.02\n",
      "iteration: 213800 loss: 0.0019 lr: 0.02\n",
      "iteration: 213900 loss: 0.0018 lr: 0.02\n",
      "iteration: 214000 loss: 0.0015 lr: 0.02\n",
      "iteration: 214100 loss: 0.0017 lr: 0.02\n",
      "iteration: 214200 loss: 0.0019 lr: 0.02\n",
      "iteration: 214300 loss: 0.0019 lr: 0.02\n",
      "iteration: 214400 loss: 0.0017 lr: 0.02\n",
      "iteration: 214500 loss: 0.0017 lr: 0.02\n",
      "iteration: 214600 loss: 0.0017 lr: 0.02\n",
      "iteration: 214700 loss: 0.0020 lr: 0.02\n",
      "iteration: 214800 loss: 0.0019 lr: 0.02\n",
      "iteration: 214900 loss: 0.0018 lr: 0.02\n",
      "iteration: 215000 loss: 0.0017 lr: 0.02\n",
      "iteration: 215100 loss: 0.0017 lr: 0.02\n",
      "iteration: 215200 loss: 0.0017 lr: 0.02\n",
      "iteration: 215300 loss: 0.0017 lr: 0.02\n",
      "iteration: 215400 loss: 0.0016 lr: 0.02\n",
      "iteration: 215500 loss: 0.0016 lr: 0.02\n",
      "iteration: 215600 loss: 0.0018 lr: 0.02\n",
      "iteration: 215700 loss: 0.0017 lr: 0.02\n",
      "iteration: 215800 loss: 0.0017 lr: 0.02\n",
      "iteration: 215900 loss: 0.0017 lr: 0.02\n",
      "iteration: 216000 loss: 0.0016 lr: 0.02\n",
      "iteration: 216100 loss: 0.0017 lr: 0.02\n",
      "iteration: 216200 loss: 0.0017 lr: 0.02\n",
      "iteration: 216300 loss: 0.0018 lr: 0.02\n",
      "iteration: 216400 loss: 0.0017 lr: 0.02\n",
      "iteration: 216500 loss: 0.0017 lr: 0.02\n",
      "iteration: 216600 loss: 0.0017 lr: 0.02\n",
      "iteration: 216700 loss: 0.0020 lr: 0.02\n",
      "iteration: 216800 loss: 0.0018 lr: 0.02\n",
      "iteration: 216900 loss: 0.0016 lr: 0.02\n",
      "iteration: 217000 loss: 0.0016 lr: 0.02\n",
      "iteration: 217100 loss: 0.0017 lr: 0.02\n",
      "iteration: 217200 loss: 0.0016 lr: 0.02\n",
      "iteration: 217300 loss: 0.0016 lr: 0.02\n",
      "iteration: 217400 loss: 0.0018 lr: 0.02\n",
      "iteration: 217500 loss: 0.0016 lr: 0.02\n",
      "iteration: 217600 loss: 0.0015 lr: 0.02\n",
      "iteration: 217700 loss: 0.0015 lr: 0.02\n",
      "iteration: 217800 loss: 0.0017 lr: 0.02\n",
      "iteration: 217900 loss: 0.0017 lr: 0.02\n",
      "iteration: 218000 loss: 0.0017 lr: 0.02\n",
      "iteration: 218100 loss: 0.0017 lr: 0.02\n",
      "iteration: 218200 loss: 0.0015 lr: 0.02\n",
      "iteration: 218300 loss: 0.0017 lr: 0.02\n",
      "iteration: 218400 loss: 0.0019 lr: 0.02\n",
      "iteration: 218500 loss: 0.0021 lr: 0.02\n",
      "iteration: 218600 loss: 0.0018 lr: 0.02\n",
      "iteration: 218700 loss: 0.0016 lr: 0.02\n",
      "iteration: 218800 loss: 0.0016 lr: 0.02\n",
      "iteration: 218900 loss: 0.0018 lr: 0.02\n",
      "iteration: 219000 loss: 0.0019 lr: 0.02\n",
      "iteration: 219100 loss: 0.0019 lr: 0.02\n",
      "iteration: 219200 loss: 0.0018 lr: 0.02\n",
      "iteration: 219300 loss: 0.0017 lr: 0.02\n",
      "iteration: 219400 loss: 0.0018 lr: 0.02\n",
      "iteration: 219500 loss: 0.0017 lr: 0.02\n",
      "iteration: 219600 loss: 0.0018 lr: 0.02\n",
      "iteration: 219700 loss: 0.0018 lr: 0.02\n",
      "iteration: 219800 loss: 0.0020 lr: 0.02\n",
      "iteration: 219900 loss: 0.0014 lr: 0.02\n",
      "iteration: 220000 loss: 0.0018 lr: 0.02\n",
      "iteration: 220100 loss: 0.0016 lr: 0.02\n",
      "iteration: 220200 loss: 0.0018 lr: 0.02\n",
      "iteration: 220300 loss: 0.0019 lr: 0.02\n",
      "iteration: 220400 loss: 0.0017 lr: 0.02\n",
      "iteration: 220500 loss: 0.0020 lr: 0.02\n",
      "iteration: 220600 loss: 0.0018 lr: 0.02\n",
      "iteration: 220700 loss: 0.0019 lr: 0.02\n",
      "iteration: 220800 loss: 0.0016 lr: 0.02\n",
      "iteration: 220900 loss: 0.0016 lr: 0.02\n",
      "iteration: 221000 loss: 0.0023 lr: 0.02\n",
      "iteration: 221100 loss: 0.0018 lr: 0.02\n",
      "iteration: 221200 loss: 0.0017 lr: 0.02\n",
      "iteration: 221300 loss: 0.0019 lr: 0.02\n",
      "iteration: 221400 loss: 0.0017 lr: 0.02\n",
      "iteration: 221500 loss: 0.0017 lr: 0.02\n",
      "iteration: 221600 loss: 0.0016 lr: 0.02\n",
      "iteration: 221700 loss: 0.0018 lr: 0.02\n",
      "iteration: 221800 loss: 0.0017 lr: 0.02\n",
      "iteration: 221900 loss: 0.0015 lr: 0.02\n",
      "iteration: 222000 loss: 0.0017 lr: 0.02\n",
      "iteration: 222100 loss: 0.0016 lr: 0.02\n",
      "iteration: 222200 loss: 0.0016 lr: 0.02\n",
      "iteration: 222300 loss: 0.0017 lr: 0.02\n",
      "iteration: 222400 loss: 0.0016 lr: 0.02\n",
      "iteration: 222500 loss: 0.0018 lr: 0.02\n",
      "iteration: 222600 loss: 0.0017 lr: 0.02\n",
      "iteration: 222700 loss: 0.0018 lr: 0.02\n",
      "iteration: 222800 loss: 0.0017 lr: 0.02\n",
      "iteration: 222900 loss: 0.0016 lr: 0.02\n",
      "iteration: 223000 loss: 0.0016 lr: 0.02\n",
      "iteration: 223100 loss: 0.0017 lr: 0.02\n",
      "iteration: 223200 loss: 0.0016 lr: 0.02\n",
      "iteration: 223300 loss: 0.0016 lr: 0.02\n",
      "iteration: 223400 loss: 0.0017 lr: 0.02\n",
      "iteration: 223500 loss: 0.0018 lr: 0.02\n",
      "iteration: 223600 loss: 0.0017 lr: 0.02\n",
      "iteration: 223700 loss: 0.0017 lr: 0.02\n",
      "iteration: 223800 loss: 0.0017 lr: 0.02\n",
      "iteration: 223900 loss: 0.0017 lr: 0.02\n",
      "iteration: 224000 loss: 0.0015 lr: 0.02\n",
      "iteration: 224100 loss: 0.0017 lr: 0.02\n",
      "iteration: 224200 loss: 0.0020 lr: 0.02\n",
      "iteration: 224300 loss: 0.0015 lr: 0.02\n",
      "iteration: 224400 loss: 0.0016 lr: 0.02\n",
      "iteration: 224500 loss: 0.0017 lr: 0.02\n",
      "iteration: 224600 loss: 0.0017 lr: 0.02\n",
      "iteration: 224700 loss: 0.0016 lr: 0.02\n",
      "iteration: 224800 loss: 0.0015 lr: 0.02\n",
      "iteration: 224900 loss: 0.0016 lr: 0.02\n",
      "iteration: 225000 loss: 0.0017 lr: 0.02\n",
      "iteration: 225100 loss: 0.0017 lr: 0.02\n",
      "iteration: 225200 loss: 0.0017 lr: 0.02\n",
      "iteration: 225300 loss: 0.0016 lr: 0.02\n",
      "iteration: 225400 loss: 0.0015 lr: 0.02\n",
      "iteration: 225500 loss: 0.0017 lr: 0.02\n",
      "iteration: 225600 loss: 0.0016 lr: 0.02\n",
      "iteration: 225700 loss: 0.0017 lr: 0.02\n",
      "iteration: 225800 loss: 0.0017 lr: 0.02\n",
      "iteration: 225900 loss: 0.0016 lr: 0.02\n",
      "iteration: 226000 loss: 0.0017 lr: 0.02\n",
      "iteration: 226100 loss: 0.0016 lr: 0.02\n",
      "iteration: 226200 loss: 0.0017 lr: 0.02\n",
      "iteration: 226300 loss: 0.0016 lr: 0.02\n",
      "iteration: 226400 loss: 0.0015 lr: 0.02\n",
      "iteration: 226500 loss: 0.0016 lr: 0.02\n",
      "iteration: 226600 loss: 0.0016 lr: 0.02\n",
      "iteration: 226700 loss: 0.0018 lr: 0.02\n",
      "iteration: 226800 loss: 0.0018 lr: 0.02\n",
      "iteration: 226900 loss: 0.0015 lr: 0.02\n",
      "iteration: 227000 loss: 0.0016 lr: 0.02\n",
      "iteration: 227100 loss: 0.0016 lr: 0.02\n",
      "iteration: 227200 loss: 0.0016 lr: 0.02\n",
      "iteration: 227300 loss: 0.0015 lr: 0.02\n",
      "iteration: 227400 loss: 0.0017 lr: 0.02\n",
      "iteration: 227500 loss: 0.0017 lr: 0.02\n",
      "iteration: 227600 loss: 0.0016 lr: 0.02\n",
      "iteration: 227700 loss: 0.0017 lr: 0.02\n",
      "iteration: 227800 loss: 0.0016 lr: 0.02\n",
      "iteration: 227900 loss: 0.0016 lr: 0.02\n",
      "iteration: 228000 loss: 0.0019 lr: 0.02\n",
      "iteration: 228100 loss: 0.0016 lr: 0.02\n",
      "iteration: 228200 loss: 0.0016 lr: 0.02\n",
      "iteration: 228300 loss: 0.0018 lr: 0.02\n",
      "iteration: 228400 loss: 0.0018 lr: 0.02\n",
      "iteration: 228500 loss: 0.0018 lr: 0.02\n",
      "iteration: 228600 loss: 0.0015 lr: 0.02\n",
      "iteration: 228700 loss: 0.0015 lr: 0.02\n",
      "iteration: 228800 loss: 0.0015 lr: 0.02\n",
      "iteration: 228900 loss: 0.0014 lr: 0.02\n",
      "iteration: 229000 loss: 0.0017 lr: 0.02\n",
      "iteration: 229100 loss: 0.0015 lr: 0.02\n",
      "iteration: 229200 loss: 0.0016 lr: 0.02\n",
      "iteration: 229300 loss: 0.0018 lr: 0.02\n",
      "iteration: 229400 loss: 0.0018 lr: 0.02\n",
      "iteration: 229500 loss: 0.0018 lr: 0.02\n",
      "iteration: 229600 loss: 0.0015 lr: 0.02\n",
      "iteration: 229700 loss: 0.0015 lr: 0.02\n",
      "iteration: 229800 loss: 0.0017 lr: 0.02\n",
      "iteration: 229900 loss: 0.0015 lr: 0.02\n",
      "iteration: 230000 loss: 0.0017 lr: 0.02\n",
      "iteration: 230100 loss: 0.0016 lr: 0.02\n",
      "iteration: 230200 loss: 0.0016 lr: 0.02\n",
      "iteration: 230300 loss: 0.0015 lr: 0.02\n",
      "iteration: 230400 loss: 0.0014 lr: 0.02\n",
      "iteration: 230500 loss: 0.0013 lr: 0.02\n",
      "iteration: 230600 loss: 0.0016 lr: 0.02\n",
      "iteration: 230700 loss: 0.0014 lr: 0.02\n",
      "iteration: 230800 loss: 0.0016 lr: 0.02\n",
      "iteration: 230900 loss: 0.0015 lr: 0.02\n",
      "iteration: 231000 loss: 0.0017 lr: 0.02\n",
      "iteration: 231100 loss: 0.0015 lr: 0.02\n",
      "iteration: 231200 loss: 0.0015 lr: 0.02\n",
      "iteration: 231300 loss: 0.0016 lr: 0.02\n",
      "iteration: 231400 loss: 0.0017 lr: 0.02\n",
      "iteration: 231500 loss: 0.0019 lr: 0.02\n",
      "iteration: 231600 loss: 0.0019 lr: 0.02\n",
      "iteration: 231700 loss: 0.0015 lr: 0.02\n",
      "iteration: 231800 loss: 0.0017 lr: 0.02\n",
      "iteration: 231900 loss: 0.0015 lr: 0.02\n",
      "iteration: 232000 loss: 0.0016 lr: 0.02\n",
      "iteration: 232100 loss: 0.0019 lr: 0.02\n",
      "iteration: 232200 loss: 0.0017 lr: 0.02\n",
      "iteration: 232300 loss: 0.0018 lr: 0.02\n",
      "iteration: 232400 loss: 0.0014 lr: 0.02\n",
      "iteration: 232500 loss: 0.0015 lr: 0.02\n",
      "iteration: 232600 loss: 0.0019 lr: 0.02\n",
      "iteration: 232700 loss: 0.0016 lr: 0.02\n",
      "iteration: 232800 loss: 0.0016 lr: 0.02\n",
      "iteration: 232900 loss: 0.0017 lr: 0.02\n",
      "iteration: 233000 loss: 0.0017 lr: 0.02\n",
      "iteration: 233100 loss: 0.0018 lr: 0.02\n",
      "iteration: 233200 loss: 0.0016 lr: 0.02\n",
      "iteration: 233300 loss: 0.0017 lr: 0.02\n",
      "iteration: 233400 loss: 0.0018 lr: 0.02\n",
      "iteration: 233500 loss: 0.0015 lr: 0.02\n",
      "iteration: 233600 loss: 0.0014 lr: 0.02\n",
      "iteration: 233700 loss: 0.0015 lr: 0.02\n",
      "iteration: 233800 loss: 0.0016 lr: 0.02\n",
      "iteration: 233900 loss: 0.0017 lr: 0.02\n",
      "iteration: 234000 loss: 0.0016 lr: 0.02\n",
      "iteration: 234100 loss: 0.0017 lr: 0.02\n",
      "iteration: 234200 loss: 0.0016 lr: 0.02\n",
      "iteration: 234300 loss: 0.0017 lr: 0.02\n",
      "iteration: 234400 loss: 0.0016 lr: 0.02\n",
      "iteration: 234500 loss: 0.0016 lr: 0.02\n",
      "iteration: 234600 loss: 0.0015 lr: 0.02\n",
      "iteration: 234700 loss: 0.0018 lr: 0.02\n",
      "iteration: 234800 loss: 0.0016 lr: 0.02\n",
      "iteration: 234900 loss: 0.0016 lr: 0.02\n",
      "iteration: 235000 loss: 0.0015 lr: 0.02\n",
      "iteration: 235100 loss: 0.0017 lr: 0.02\n",
      "iteration: 235200 loss: 0.0016 lr: 0.02\n",
      "iteration: 235300 loss: 0.0016 lr: 0.02\n",
      "iteration: 235400 loss: 0.0016 lr: 0.02\n",
      "iteration: 235500 loss: 0.0017 lr: 0.02\n",
      "iteration: 235600 loss: 0.0017 lr: 0.02\n",
      "iteration: 235700 loss: 0.0017 lr: 0.02\n",
      "iteration: 235800 loss: 0.0015 lr: 0.02\n",
      "iteration: 235900 loss: 0.0017 lr: 0.02\n",
      "iteration: 236000 loss: 0.0013 lr: 0.02\n",
      "iteration: 236100 loss: 0.0015 lr: 0.02\n",
      "iteration: 236200 loss: 0.0016 lr: 0.02\n",
      "iteration: 236300 loss: 0.0015 lr: 0.02\n",
      "iteration: 236400 loss: 0.0017 lr: 0.02\n",
      "iteration: 236500 loss: 0.0015 lr: 0.02\n",
      "iteration: 236600 loss: 0.0015 lr: 0.02\n",
      "iteration: 236700 loss: 0.0016 lr: 0.02\n",
      "iteration: 236800 loss: 0.0015 lr: 0.02\n",
      "iteration: 236900 loss: 0.0017 lr: 0.02\n",
      "iteration: 237000 loss: 0.0016 lr: 0.02\n",
      "iteration: 237100 loss: 0.0018 lr: 0.02\n",
      "iteration: 237200 loss: 0.0016 lr: 0.02\n",
      "iteration: 237300 loss: 0.0014 lr: 0.02\n",
      "iteration: 237400 loss: 0.0016 lr: 0.02\n",
      "iteration: 237500 loss: 0.0015 lr: 0.02\n",
      "iteration: 237600 loss: 0.0018 lr: 0.02\n",
      "iteration: 237700 loss: 0.0015 lr: 0.02\n",
      "iteration: 237800 loss: 0.0017 lr: 0.02\n",
      "iteration: 237900 loss: 0.0016 lr: 0.02\n",
      "iteration: 238000 loss: 0.0016 lr: 0.02\n",
      "iteration: 238100 loss: 0.0015 lr: 0.02\n",
      "iteration: 238200 loss: 0.0016 lr: 0.02\n",
      "iteration: 238300 loss: 0.0014 lr: 0.02\n",
      "iteration: 238400 loss: 0.0015 lr: 0.02\n",
      "iteration: 238500 loss: 0.0017 lr: 0.02\n",
      "iteration: 238600 loss: 0.0018 lr: 0.02\n",
      "iteration: 238700 loss: 0.0017 lr: 0.02\n",
      "iteration: 238800 loss: 0.0017 lr: 0.02\n",
      "iteration: 238900 loss: 0.0015 lr: 0.02\n",
      "iteration: 239000 loss: 0.0016 lr: 0.02\n",
      "iteration: 239100 loss: 0.0015 lr: 0.02\n",
      "iteration: 239200 loss: 0.0018 lr: 0.02\n",
      "iteration: 239300 loss: 0.0017 lr: 0.02\n",
      "iteration: 239400 loss: 0.0015 lr: 0.02\n",
      "iteration: 239500 loss: 0.0016 lr: 0.02\n",
      "iteration: 239600 loss: 0.0016 lr: 0.02\n",
      "iteration: 239700 loss: 0.0017 lr: 0.02\n",
      "iteration: 239800 loss: 0.0017 lr: 0.02\n",
      "iteration: 239900 loss: 0.0015 lr: 0.02\n",
      "iteration: 240000 loss: 0.0016 lr: 0.02\n",
      "iteration: 240100 loss: 0.0016 lr: 0.02\n",
      "iteration: 240200 loss: 0.0015 lr: 0.02\n",
      "iteration: 240300 loss: 0.0017 lr: 0.02\n",
      "iteration: 240400 loss: 0.0016 lr: 0.02\n",
      "iteration: 240500 loss: 0.0014 lr: 0.02\n",
      "iteration: 240600 loss: 0.0015 lr: 0.02\n",
      "iteration: 240700 loss: 0.0015 lr: 0.02\n",
      "iteration: 240800 loss: 0.0016 lr: 0.02\n",
      "iteration: 240900 loss: 0.0014 lr: 0.02\n",
      "iteration: 241000 loss: 0.0016 lr: 0.02\n",
      "iteration: 241100 loss: 0.0016 lr: 0.02\n",
      "iteration: 241200 loss: 0.0015 lr: 0.02\n",
      "iteration: 241300 loss: 0.0015 lr: 0.02\n",
      "iteration: 241400 loss: 0.0016 lr: 0.02\n",
      "iteration: 241500 loss: 0.0017 lr: 0.02\n",
      "iteration: 241600 loss: 0.0017 lr: 0.02\n",
      "iteration: 241700 loss: 0.0019 lr: 0.02\n",
      "iteration: 241800 loss: 0.0016 lr: 0.02\n",
      "iteration: 241900 loss: 0.0017 lr: 0.02\n",
      "iteration: 242000 loss: 0.0016 lr: 0.02\n",
      "iteration: 242100 loss: 0.0016 lr: 0.02\n",
      "iteration: 242200 loss: 0.0016 lr: 0.02\n",
      "iteration: 242300 loss: 0.0017 lr: 0.02\n",
      "iteration: 242400 loss: 0.0017 lr: 0.02\n",
      "iteration: 242500 loss: 0.0017 lr: 0.02\n",
      "iteration: 242600 loss: 0.0020 lr: 0.02\n",
      "iteration: 242700 loss: 0.0015 lr: 0.02\n",
      "iteration: 242800 loss: 0.0015 lr: 0.02\n",
      "iteration: 242900 loss: 0.0014 lr: 0.02\n",
      "iteration: 243000 loss: 0.0017 lr: 0.02\n",
      "iteration: 243100 loss: 0.0016 lr: 0.02\n",
      "iteration: 243200 loss: 0.0016 lr: 0.02\n",
      "iteration: 243300 loss: 0.0015 lr: 0.02\n",
      "iteration: 243400 loss: 0.0016 lr: 0.02\n",
      "iteration: 243500 loss: 0.0016 lr: 0.02\n",
      "iteration: 243600 loss: 0.0016 lr: 0.02\n",
      "iteration: 243700 loss: 0.0018 lr: 0.02\n",
      "iteration: 243800 loss: 0.0016 lr: 0.02\n",
      "iteration: 243900 loss: 0.0016 lr: 0.02\n",
      "iteration: 244000 loss: 0.0015 lr: 0.02\n",
      "iteration: 244100 loss: 0.0016 lr: 0.02\n",
      "iteration: 244200 loss: 0.0016 lr: 0.02\n",
      "iteration: 244300 loss: 0.0015 lr: 0.02\n",
      "iteration: 244400 loss: 0.0016 lr: 0.02\n",
      "iteration: 244500 loss: 0.0017 lr: 0.02\n",
      "iteration: 244600 loss: 0.0015 lr: 0.02\n",
      "iteration: 244700 loss: 0.0015 lr: 0.02\n",
      "iteration: 244800 loss: 0.0014 lr: 0.02\n",
      "iteration: 244900 loss: 0.0015 lr: 0.02\n",
      "iteration: 245000 loss: 0.0016 lr: 0.02\n",
      "iteration: 245100 loss: 0.0016 lr: 0.02\n",
      "iteration: 245200 loss: 0.0016 lr: 0.02\n",
      "iteration: 245300 loss: 0.0017 lr: 0.02\n",
      "iteration: 245400 loss: 0.0019 lr: 0.02\n",
      "iteration: 245500 loss: 0.0018 lr: 0.02\n",
      "iteration: 245600 loss: 0.0015 lr: 0.02\n",
      "iteration: 245700 loss: 0.0016 lr: 0.02\n",
      "iteration: 245800 loss: 0.0016 lr: 0.02\n",
      "iteration: 245900 loss: 0.0016 lr: 0.02\n",
      "iteration: 246000 loss: 0.0017 lr: 0.02\n",
      "iteration: 246100 loss: 0.0017 lr: 0.02\n",
      "iteration: 246200 loss: 0.0017 lr: 0.02\n",
      "iteration: 246300 loss: 0.0015 lr: 0.02\n",
      "iteration: 246400 loss: 0.0014 lr: 0.02\n",
      "iteration: 246500 loss: 0.0015 lr: 0.02\n",
      "iteration: 246600 loss: 0.0016 lr: 0.02\n",
      "iteration: 246700 loss: 0.0017 lr: 0.02\n",
      "iteration: 246800 loss: 0.0016 lr: 0.02\n",
      "iteration: 246900 loss: 0.0017 lr: 0.02\n",
      "iteration: 247000 loss: 0.0017 lr: 0.02\n",
      "iteration: 247100 loss: 0.0015 lr: 0.02\n",
      "iteration: 247200 loss: 0.0017 lr: 0.02\n",
      "iteration: 247300 loss: 0.0017 lr: 0.02\n",
      "iteration: 247400 loss: 0.0018 lr: 0.02\n",
      "iteration: 247500 loss: 0.0015 lr: 0.02\n",
      "iteration: 247600 loss: 0.0016 lr: 0.02\n",
      "iteration: 247700 loss: 0.0015 lr: 0.02\n",
      "iteration: 247800 loss: 0.0017 lr: 0.02\n",
      "iteration: 247900 loss: 0.0017 lr: 0.02\n",
      "iteration: 248000 loss: 0.0015 lr: 0.02\n",
      "iteration: 248100 loss: 0.0016 lr: 0.02\n",
      "iteration: 248200 loss: 0.0015 lr: 0.02\n",
      "iteration: 248300 loss: 0.0015 lr: 0.02\n",
      "iteration: 248400 loss: 0.0014 lr: 0.02\n",
      "iteration: 248500 loss: 0.0015 lr: 0.02\n",
      "iteration: 248600 loss: 0.0018 lr: 0.02\n",
      "iteration: 248700 loss: 0.0016 lr: 0.02\n",
      "iteration: 248800 loss: 0.0015 lr: 0.02\n",
      "iteration: 248900 loss: 0.0015 lr: 0.02\n",
      "iteration: 249000 loss: 0.0016 lr: 0.02\n",
      "iteration: 249100 loss: 0.0015 lr: 0.02\n",
      "iteration: 249200 loss: 0.0014 lr: 0.02\n",
      "iteration: 249300 loss: 0.0017 lr: 0.02\n",
      "iteration: 249400 loss: 0.0016 lr: 0.02\n",
      "iteration: 249500 loss: 0.0015 lr: 0.02\n",
      "iteration: 249600 loss: 0.0015 lr: 0.02\n",
      "iteration: 249700 loss: 0.0016 lr: 0.02\n",
      "iteration: 249800 loss: 0.0016 lr: 0.02\n",
      "iteration: 249900 loss: 0.0016 lr: 0.02\n",
      "iteration: 250000 loss: 0.0016 lr: 0.02\n",
      "iteration: 250100 loss: 0.0016 lr: 0.02\n",
      "iteration: 250200 loss: 0.0014 lr: 0.02\n",
      "iteration: 250300 loss: 0.0015 lr: 0.02\n",
      "iteration: 250400 loss: 0.0016 lr: 0.02\n",
      "iteration: 250500 loss: 0.0015 lr: 0.02\n",
      "iteration: 250600 loss: 0.0015 lr: 0.02\n",
      "iteration: 250700 loss: 0.0015 lr: 0.02\n",
      "iteration: 250800 loss: 0.0018 lr: 0.02\n",
      "iteration: 250900 loss: 0.0015 lr: 0.02\n",
      "iteration: 251000 loss: 0.0015 lr: 0.02\n",
      "iteration: 251100 loss: 0.0016 lr: 0.02\n",
      "iteration: 251200 loss: 0.0016 lr: 0.02\n",
      "iteration: 251300 loss: 0.0017 lr: 0.02\n",
      "iteration: 251400 loss: 0.0015 lr: 0.02\n",
      "iteration: 251500 loss: 0.0013 lr: 0.02\n",
      "iteration: 251600 loss: 0.0014 lr: 0.02\n",
      "iteration: 251700 loss: 0.0015 lr: 0.02\n",
      "iteration: 251800 loss: 0.0015 lr: 0.02\n",
      "iteration: 251900 loss: 0.0016 lr: 0.02\n",
      "iteration: 252000 loss: 0.0014 lr: 0.02\n",
      "iteration: 252100 loss: 0.0014 lr: 0.02\n",
      "iteration: 252200 loss: 0.0014 lr: 0.02\n",
      "iteration: 252300 loss: 0.0016 lr: 0.02\n",
      "iteration: 252400 loss: 0.0015 lr: 0.02\n",
      "iteration: 252500 loss: 0.0014 lr: 0.02\n",
      "iteration: 252600 loss: 0.0016 lr: 0.02\n",
      "iteration: 252700 loss: 0.0017 lr: 0.02\n",
      "iteration: 252800 loss: 0.0015 lr: 0.02\n",
      "iteration: 252900 loss: 0.0017 lr: 0.02\n",
      "iteration: 253000 loss: 0.0017 lr: 0.02\n",
      "iteration: 253100 loss: 0.0016 lr: 0.02\n",
      "iteration: 253200 loss: 0.0015 lr: 0.02\n",
      "iteration: 253300 loss: 0.0015 lr: 0.02\n",
      "iteration: 253400 loss: 0.0017 lr: 0.02\n",
      "iteration: 253500 loss: 0.0017 lr: 0.02\n",
      "iteration: 253600 loss: 0.0017 lr: 0.02\n",
      "iteration: 253700 loss: 0.0016 lr: 0.02\n",
      "iteration: 253800 loss: 0.0017 lr: 0.02\n",
      "iteration: 253900 loss: 0.0017 lr: 0.02\n",
      "iteration: 254000 loss: 0.0016 lr: 0.02\n",
      "iteration: 254100 loss: 0.0017 lr: 0.02\n",
      "iteration: 254200 loss: 0.0017 lr: 0.02\n",
      "iteration: 254300 loss: 0.0014 lr: 0.02\n",
      "iteration: 254400 loss: 0.0015 lr: 0.02\n",
      "iteration: 254500 loss: 0.0016 lr: 0.02\n",
      "iteration: 254600 loss: 0.0017 lr: 0.02\n",
      "iteration: 254700 loss: 0.0016 lr: 0.02\n",
      "iteration: 254800 loss: 0.0016 lr: 0.02\n",
      "iteration: 254900 loss: 0.0016 lr: 0.02\n",
      "iteration: 255000 loss: 0.0015 lr: 0.02\n",
      "iteration: 255100 loss: 0.0018 lr: 0.02\n",
      "iteration: 255200 loss: 0.0016 lr: 0.02\n",
      "iteration: 255300 loss: 0.0015 lr: 0.02\n",
      "iteration: 255400 loss: 0.0016 lr: 0.02\n",
      "iteration: 255500 loss: 0.0017 lr: 0.02\n",
      "iteration: 255600 loss: 0.0016 lr: 0.02\n",
      "iteration: 255700 loss: 0.0017 lr: 0.02\n",
      "iteration: 255800 loss: 0.0016 lr: 0.02\n",
      "iteration: 255900 loss: 0.0016 lr: 0.02\n",
      "iteration: 256000 loss: 0.0016 lr: 0.02\n",
      "iteration: 256100 loss: 0.0016 lr: 0.02\n",
      "iteration: 256200 loss: 0.0018 lr: 0.02\n",
      "iteration: 256300 loss: 0.0014 lr: 0.02\n",
      "iteration: 256400 loss: 0.0016 lr: 0.02\n",
      "iteration: 256500 loss: 0.0018 lr: 0.02\n",
      "iteration: 256600 loss: 0.0015 lr: 0.02\n",
      "iteration: 256700 loss: 0.0015 lr: 0.02\n",
      "iteration: 256800 loss: 0.0014 lr: 0.02\n",
      "iteration: 256900 loss: 0.0016 lr: 0.02\n",
      "iteration: 257000 loss: 0.0016 lr: 0.02\n",
      "iteration: 257100 loss: 0.0014 lr: 0.02\n",
      "iteration: 257200 loss: 0.0015 lr: 0.02\n",
      "iteration: 257300 loss: 0.0015 lr: 0.02\n",
      "iteration: 257400 loss: 0.0015 lr: 0.02\n",
      "iteration: 257500 loss: 0.0017 lr: 0.02\n",
      "iteration: 257600 loss: 0.0014 lr: 0.02\n",
      "iteration: 257700 loss: 0.0016 lr: 0.02\n",
      "iteration: 257800 loss: 0.0018 lr: 0.02\n",
      "iteration: 257900 loss: 0.0015 lr: 0.02\n",
      "iteration: 258000 loss: 0.0016 lr: 0.02\n",
      "iteration: 258100 loss: 0.0018 lr: 0.02\n",
      "iteration: 258200 loss: 0.0015 lr: 0.02\n",
      "iteration: 258300 loss: 0.0017 lr: 0.02\n",
      "iteration: 258400 loss: 0.0015 lr: 0.02\n",
      "iteration: 258500 loss: 0.0016 lr: 0.02\n",
      "iteration: 258600 loss: 0.0017 lr: 0.02\n",
      "iteration: 258700 loss: 0.0016 lr: 0.02\n",
      "iteration: 258800 loss: 0.0017 lr: 0.02\n",
      "iteration: 258900 loss: 0.0016 lr: 0.02\n",
      "iteration: 259000 loss: 0.0015 lr: 0.02\n",
      "iteration: 259100 loss: 0.0015 lr: 0.02\n",
      "iteration: 259200 loss: 0.0014 lr: 0.02\n",
      "iteration: 259300 loss: 0.0018 lr: 0.02\n",
      "iteration: 259400 loss: 0.0016 lr: 0.02\n",
      "iteration: 259500 loss: 0.0015 lr: 0.02\n",
      "iteration: 259600 loss: 0.0016 lr: 0.02\n",
      "iteration: 259700 loss: 0.0015 lr: 0.02\n",
      "iteration: 259800 loss: 0.0018 lr: 0.02\n",
      "iteration: 259900 loss: 0.0016 lr: 0.02\n",
      "iteration: 260000 loss: 0.0015 lr: 0.02\n",
      "iteration: 260100 loss: 0.0018 lr: 0.02\n",
      "iteration: 260200 loss: 0.0016 lr: 0.02\n",
      "iteration: 260300 loss: 0.0017 lr: 0.02\n",
      "iteration: 260400 loss: 0.0015 lr: 0.02\n",
      "iteration: 260500 loss: 0.0016 lr: 0.02\n",
      "iteration: 260600 loss: 0.0020 lr: 0.02\n",
      "iteration: 260700 loss: 0.0015 lr: 0.02\n",
      "iteration: 260800 loss: 0.0015 lr: 0.02\n",
      "iteration: 260900 loss: 0.0015 lr: 0.02\n",
      "iteration: 261000 loss: 0.0016 lr: 0.02\n",
      "iteration: 261100 loss: 0.0016 lr: 0.02\n",
      "iteration: 261200 loss: 0.0016 lr: 0.02\n",
      "iteration: 261300 loss: 0.0016 lr: 0.02\n",
      "iteration: 261400 loss: 0.0016 lr: 0.02\n",
      "iteration: 261500 loss: 0.0016 lr: 0.02\n",
      "iteration: 261600 loss: 0.0017 lr: 0.02\n",
      "iteration: 261700 loss: 0.0017 lr: 0.02\n",
      "iteration: 261800 loss: 0.0016 lr: 0.02\n",
      "iteration: 261900 loss: 0.0016 lr: 0.02\n",
      "iteration: 262000 loss: 0.0014 lr: 0.02\n",
      "iteration: 262100 loss: 0.0014 lr: 0.02\n",
      "iteration: 262200 loss: 0.0015 lr: 0.02\n",
      "iteration: 262300 loss: 0.0014 lr: 0.02\n",
      "iteration: 262400 loss: 0.0015 lr: 0.02\n",
      "iteration: 262500 loss: 0.0018 lr: 0.02\n",
      "iteration: 262600 loss: 0.0015 lr: 0.02\n",
      "iteration: 262700 loss: 0.0015 lr: 0.02\n",
      "iteration: 262800 loss: 0.0014 lr: 0.02\n",
      "iteration: 262900 loss: 0.0015 lr: 0.02\n",
      "iteration: 263000 loss: 0.0015 lr: 0.02\n",
      "iteration: 263100 loss: 0.0015 lr: 0.02\n",
      "iteration: 263200 loss: 0.0015 lr: 0.02\n",
      "iteration: 263300 loss: 0.0015 lr: 0.02\n",
      "iteration: 263400 loss: 0.0015 lr: 0.02\n",
      "iteration: 263500 loss: 0.0015 lr: 0.02\n",
      "iteration: 263600 loss: 0.0015 lr: 0.02\n",
      "iteration: 263700 loss: 0.0017 lr: 0.02\n",
      "iteration: 263800 loss: 0.0016 lr: 0.02\n",
      "iteration: 263900 loss: 0.0017 lr: 0.02\n",
      "iteration: 264000 loss: 0.0015 lr: 0.02\n",
      "iteration: 264100 loss: 0.0016 lr: 0.02\n",
      "iteration: 264200 loss: 0.0015 lr: 0.02\n",
      "iteration: 264300 loss: 0.0015 lr: 0.02\n",
      "iteration: 264400 loss: 0.0014 lr: 0.02\n",
      "iteration: 264500 loss: 0.0014 lr: 0.02\n",
      "iteration: 264600 loss: 0.0016 lr: 0.02\n",
      "iteration: 264700 loss: 0.0015 lr: 0.02\n",
      "iteration: 264800 loss: 0.0014 lr: 0.02\n",
      "iteration: 264900 loss: 0.0015 lr: 0.02\n",
      "iteration: 265000 loss: 0.0015 lr: 0.02\n",
      "iteration: 265100 loss: 0.0015 lr: 0.02\n",
      "iteration: 265200 loss: 0.0015 lr: 0.02\n",
      "iteration: 265300 loss: 0.0015 lr: 0.02\n",
      "iteration: 265400 loss: 0.0015 lr: 0.02\n",
      "iteration: 265500 loss: 0.0016 lr: 0.02\n",
      "iteration: 265600 loss: 0.0014 lr: 0.02\n",
      "iteration: 265700 loss: 0.0015 lr: 0.02\n",
      "iteration: 265800 loss: 0.0015 lr: 0.02\n",
      "iteration: 265900 loss: 0.0015 lr: 0.02\n",
      "iteration: 266000 loss: 0.0014 lr: 0.02\n",
      "iteration: 266100 loss: 0.0014 lr: 0.02\n",
      "iteration: 266200 loss: 0.0017 lr: 0.02\n",
      "iteration: 266300 loss: 0.0015 lr: 0.02\n",
      "iteration: 266400 loss: 0.0018 lr: 0.02\n",
      "iteration: 266500 loss: 0.0015 lr: 0.02\n",
      "iteration: 266600 loss: 0.0017 lr: 0.02\n",
      "iteration: 266700 loss: 0.0015 lr: 0.02\n",
      "iteration: 266800 loss: 0.0015 lr: 0.02\n",
      "iteration: 266900 loss: 0.0016 lr: 0.02\n",
      "iteration: 267000 loss: 0.0016 lr: 0.02\n",
      "iteration: 267100 loss: 0.0015 lr: 0.02\n",
      "iteration: 267200 loss: 0.0015 lr: 0.02\n",
      "iteration: 267300 loss: 0.0015 lr: 0.02\n",
      "iteration: 267400 loss: 0.0015 lr: 0.02\n",
      "iteration: 267500 loss: 0.0014 lr: 0.02\n",
      "iteration: 267600 loss: 0.0014 lr: 0.02\n",
      "iteration: 267700 loss: 0.0015 lr: 0.02\n",
      "iteration: 267800 loss: 0.0015 lr: 0.02\n",
      "iteration: 267900 loss: 0.0015 lr: 0.02\n",
      "iteration: 268000 loss: 0.0013 lr: 0.02\n",
      "iteration: 268100 loss: 0.0013 lr: 0.02\n",
      "iteration: 268200 loss: 0.0015 lr: 0.02\n",
      "iteration: 268300 loss: 0.0015 lr: 0.02\n",
      "iteration: 268400 loss: 0.0015 lr: 0.02\n",
      "iteration: 268500 loss: 0.0014 lr: 0.02\n",
      "iteration: 268600 loss: 0.0015 lr: 0.02\n",
      "iteration: 268700 loss: 0.0015 lr: 0.02\n",
      "iteration: 268800 loss: 0.0015 lr: 0.02\n",
      "iteration: 268900 loss: 0.0014 lr: 0.02\n",
      "iteration: 269000 loss: 0.0015 lr: 0.02\n",
      "iteration: 269100 loss: 0.0016 lr: 0.02\n",
      "iteration: 269200 loss: 0.0014 lr: 0.02\n",
      "iteration: 269300 loss: 0.0017 lr: 0.02\n",
      "iteration: 269400 loss: 0.0015 lr: 0.02\n",
      "iteration: 269500 loss: 0.0015 lr: 0.02\n",
      "iteration: 269600 loss: 0.0016 lr: 0.02\n",
      "iteration: 269700 loss: 0.0016 lr: 0.02\n",
      "iteration: 269800 loss: 0.0015 lr: 0.02\n",
      "iteration: 269900 loss: 0.0015 lr: 0.02\n",
      "iteration: 270000 loss: 0.0015 lr: 0.02\n",
      "iteration: 270100 loss: 0.0014 lr: 0.02\n",
      "iteration: 270200 loss: 0.0016 lr: 0.02\n",
      "iteration: 270300 loss: 0.0016 lr: 0.02\n",
      "iteration: 270400 loss: 0.0015 lr: 0.02\n",
      "iteration: 270500 loss: 0.0014 lr: 0.02\n",
      "iteration: 270600 loss: 0.0015 lr: 0.02\n",
      "iteration: 270700 loss: 0.0014 lr: 0.02\n",
      "iteration: 270800 loss: 0.0013 lr: 0.02\n",
      "iteration: 270900 loss: 0.0014 lr: 0.02\n",
      "iteration: 271000 loss: 0.0013 lr: 0.02\n",
      "iteration: 271100 loss: 0.0015 lr: 0.02\n",
      "iteration: 271200 loss: 0.0014 lr: 0.02\n",
      "iteration: 271300 loss: 0.0015 lr: 0.02\n",
      "iteration: 271400 loss: 0.0015 lr: 0.02\n",
      "iteration: 271500 loss: 0.0016 lr: 0.02\n",
      "iteration: 271600 loss: 0.0015 lr: 0.02\n",
      "iteration: 271700 loss: 0.0014 lr: 0.02\n",
      "iteration: 271800 loss: 0.0016 lr: 0.02\n",
      "iteration: 271900 loss: 0.0015 lr: 0.02\n",
      "iteration: 272000 loss: 0.0014 lr: 0.02\n",
      "iteration: 272100 loss: 0.0014 lr: 0.02\n",
      "iteration: 272200 loss: 0.0014 lr: 0.02\n",
      "iteration: 272300 loss: 0.0015 lr: 0.02\n",
      "iteration: 272400 loss: 0.0015 lr: 0.02\n",
      "iteration: 272500 loss: 0.0017 lr: 0.02\n",
      "iteration: 272600 loss: 0.0017 lr: 0.02\n",
      "iteration: 272700 loss: 0.0017 lr: 0.02\n",
      "iteration: 272800 loss: 0.0016 lr: 0.02\n",
      "iteration: 272900 loss: 0.0016 lr: 0.02\n",
      "iteration: 273000 loss: 0.0015 lr: 0.02\n",
      "iteration: 273100 loss: 0.0016 lr: 0.02\n",
      "iteration: 273200 loss: 0.0015 lr: 0.02\n",
      "iteration: 273300 loss: 0.0015 lr: 0.02\n",
      "iteration: 273400 loss: 0.0018 lr: 0.02\n",
      "iteration: 273500 loss: 0.0016 lr: 0.02\n",
      "iteration: 273600 loss: 0.0016 lr: 0.02\n",
      "iteration: 273700 loss: 0.0016 lr: 0.02\n",
      "iteration: 273800 loss: 0.0015 lr: 0.02\n",
      "iteration: 273900 loss: 0.0014 lr: 0.02\n",
      "iteration: 274000 loss: 0.0015 lr: 0.02\n",
      "iteration: 274100 loss: 0.0014 lr: 0.02\n",
      "iteration: 274200 loss: 0.0015 lr: 0.02\n",
      "iteration: 274300 loss: 0.0016 lr: 0.02\n",
      "iteration: 274400 loss: 0.0018 lr: 0.02\n",
      "iteration: 274500 loss: 0.0017 lr: 0.02\n",
      "iteration: 274600 loss: 0.0015 lr: 0.02\n",
      "iteration: 274700 loss: 0.0015 lr: 0.02\n",
      "iteration: 274800 loss: 0.0016 lr: 0.02\n",
      "iteration: 274900 loss: 0.0017 lr: 0.02\n",
      "iteration: 275000 loss: 0.0017 lr: 0.02\n",
      "iteration: 275100 loss: 0.0014 lr: 0.02\n",
      "iteration: 275200 loss: 0.0014 lr: 0.02\n",
      "iteration: 275300 loss: 0.0014 lr: 0.02\n",
      "iteration: 275400 loss: 0.0015 lr: 0.02\n",
      "iteration: 275500 loss: 0.0014 lr: 0.02\n",
      "iteration: 275600 loss: 0.0014 lr: 0.02\n",
      "iteration: 275700 loss: 0.0016 lr: 0.02\n",
      "iteration: 275800 loss: 0.0017 lr: 0.02\n",
      "iteration: 275900 loss: 0.0016 lr: 0.02\n",
      "iteration: 276000 loss: 0.0017 lr: 0.02\n",
      "iteration: 276100 loss: 0.0015 lr: 0.02\n",
      "iteration: 276200 loss: 0.0015 lr: 0.02\n",
      "iteration: 276300 loss: 0.0015 lr: 0.02\n",
      "iteration: 276400 loss: 0.0015 lr: 0.02\n",
      "iteration: 276500 loss: 0.0016 lr: 0.02\n",
      "iteration: 276600 loss: 0.0015 lr: 0.02\n",
      "iteration: 276700 loss: 0.0014 lr: 0.02\n",
      "iteration: 276800 loss: 0.0014 lr: 0.02\n",
      "iteration: 276900 loss: 0.0014 lr: 0.02\n",
      "iteration: 277000 loss: 0.0014 lr: 0.02\n",
      "iteration: 277100 loss: 0.0014 lr: 0.02\n",
      "iteration: 277200 loss: 0.0018 lr: 0.02\n",
      "iteration: 277300 loss: 0.0015 lr: 0.02\n",
      "iteration: 277400 loss: 0.0017 lr: 0.02\n",
      "iteration: 277500 loss: 0.0015 lr: 0.02\n",
      "iteration: 277600 loss: 0.0017 lr: 0.02\n",
      "iteration: 277700 loss: 0.0014 lr: 0.02\n",
      "iteration: 277800 loss: 0.0016 lr: 0.02\n",
      "iteration: 277900 loss: 0.0014 lr: 0.02\n",
      "iteration: 278000 loss: 0.0016 lr: 0.02\n",
      "iteration: 278100 loss: 0.0016 lr: 0.02\n",
      "iteration: 278200 loss: 0.0016 lr: 0.02\n",
      "iteration: 278300 loss: 0.0015 lr: 0.02\n",
      "iteration: 278400 loss: 0.0017 lr: 0.02\n",
      "iteration: 278500 loss: 0.0016 lr: 0.02\n",
      "iteration: 278600 loss: 0.0016 lr: 0.02\n",
      "iteration: 278700 loss: 0.0016 lr: 0.02\n",
      "iteration: 278800 loss: 0.0015 lr: 0.02\n",
      "iteration: 278900 loss: 0.0016 lr: 0.02\n",
      "iteration: 279000 loss: 0.0016 lr: 0.02\n",
      "iteration: 279100 loss: 0.0015 lr: 0.02\n",
      "iteration: 279200 loss: 0.0017 lr: 0.02\n",
      "iteration: 279300 loss: 0.0016 lr: 0.02\n",
      "iteration: 279400 loss: 0.0015 lr: 0.02\n",
      "iteration: 279500 loss: 0.0013 lr: 0.02\n",
      "iteration: 279600 loss: 0.0017 lr: 0.02\n",
      "iteration: 279700 loss: 0.0015 lr: 0.02\n",
      "iteration: 279800 loss: 0.0014 lr: 0.02\n",
      "iteration: 279900 loss: 0.0015 lr: 0.02\n",
      "iteration: 280000 loss: 0.0015 lr: 0.02\n",
      "iteration: 280100 loss: 0.0015 lr: 0.02\n",
      "iteration: 280200 loss: 0.0017 lr: 0.02\n",
      "iteration: 280300 loss: 0.0015 lr: 0.02\n",
      "iteration: 280400 loss: 0.0015 lr: 0.02\n",
      "iteration: 280500 loss: 0.0015 lr: 0.02\n",
      "iteration: 280600 loss: 0.0016 lr: 0.02\n",
      "iteration: 280700 loss: 0.0014 lr: 0.02\n",
      "iteration: 280800 loss: 0.0016 lr: 0.02\n",
      "iteration: 280900 loss: 0.0015 lr: 0.02\n",
      "iteration: 281000 loss: 0.0018 lr: 0.02\n",
      "iteration: 281100 loss: 0.0015 lr: 0.02\n",
      "iteration: 281200 loss: 0.0014 lr: 0.02\n",
      "iteration: 281300 loss: 0.0014 lr: 0.02\n",
      "iteration: 281400 loss: 0.0015 lr: 0.02\n",
      "iteration: 281500 loss: 0.0014 lr: 0.02\n",
      "iteration: 281600 loss: 0.0014 lr: 0.02\n",
      "iteration: 281700 loss: 0.0016 lr: 0.02\n",
      "iteration: 281800 loss: 0.0018 lr: 0.02\n",
      "iteration: 281900 loss: 0.0015 lr: 0.02\n",
      "iteration: 282000 loss: 0.0015 lr: 0.02\n",
      "iteration: 282100 loss: 0.0016 lr: 0.02\n",
      "iteration: 282200 loss: 0.0014 lr: 0.02\n",
      "iteration: 282300 loss: 0.0015 lr: 0.02\n",
      "iteration: 282400 loss: 0.0015 lr: 0.02\n",
      "iteration: 282500 loss: 0.0015 lr: 0.02\n",
      "iteration: 282600 loss: 0.0014 lr: 0.02\n",
      "iteration: 282700 loss: 0.0016 lr: 0.02\n",
      "iteration: 282800 loss: 0.0015 lr: 0.02\n",
      "iteration: 282900 loss: 0.0016 lr: 0.02\n",
      "iteration: 283000 loss: 0.0015 lr: 0.02\n",
      "iteration: 283100 loss: 0.0018 lr: 0.02\n",
      "iteration: 283200 loss: 0.0015 lr: 0.02\n",
      "iteration: 283300 loss: 0.0014 lr: 0.02\n",
      "iteration: 283400 loss: 0.0013 lr: 0.02\n",
      "iteration: 283500 loss: 0.0016 lr: 0.02\n",
      "iteration: 283600 loss: 0.0015 lr: 0.02\n",
      "iteration: 283700 loss: 0.0015 lr: 0.02\n",
      "iteration: 283800 loss: 0.0015 lr: 0.02\n",
      "iteration: 283900 loss: 0.0015 lr: 0.02\n",
      "iteration: 284000 loss: 0.0015 lr: 0.02\n",
      "iteration: 284100 loss: 0.0014 lr: 0.02\n",
      "iteration: 284200 loss: 0.0015 lr: 0.02\n",
      "iteration: 284300 loss: 0.0016 lr: 0.02\n",
      "iteration: 284400 loss: 0.0015 lr: 0.02\n",
      "iteration: 284500 loss: 0.0017 lr: 0.02\n",
      "iteration: 284600 loss: 0.0013 lr: 0.02\n",
      "iteration: 284700 loss: 0.0015 lr: 0.02\n",
      "iteration: 284800 loss: 0.0016 lr: 0.02\n",
      "iteration: 284900 loss: 0.0015 lr: 0.02\n",
      "iteration: 285000 loss: 0.0015 lr: 0.02\n",
      "iteration: 285100 loss: 0.0014 lr: 0.02\n",
      "iteration: 285200 loss: 0.0014 lr: 0.02\n",
      "iteration: 285300 loss: 0.0014 lr: 0.02\n",
      "iteration: 285400 loss: 0.0013 lr: 0.02\n",
      "iteration: 285500 loss: 0.0016 lr: 0.02\n",
      "iteration: 285600 loss: 0.0015 lr: 0.02\n",
      "iteration: 285700 loss: 0.0015 lr: 0.02\n",
      "iteration: 285800 loss: 0.0015 lr: 0.02\n",
      "iteration: 285900 loss: 0.0014 lr: 0.02\n",
      "iteration: 286000 loss: 0.0016 lr: 0.02\n",
      "iteration: 286100 loss: 0.0015 lr: 0.02\n",
      "iteration: 286200 loss: 0.0013 lr: 0.02\n",
      "iteration: 286300 loss: 0.0016 lr: 0.02\n",
      "iteration: 286400 loss: 0.0016 lr: 0.02\n",
      "iteration: 286500 loss: 0.0016 lr: 0.02\n",
      "iteration: 286600 loss: 0.0014 lr: 0.02\n",
      "iteration: 286700 loss: 0.0017 lr: 0.02\n",
      "iteration: 286800 loss: 0.0015 lr: 0.02\n",
      "iteration: 286900 loss: 0.0014 lr: 0.02\n",
      "iteration: 287000 loss: 0.0015 lr: 0.02\n",
      "iteration: 287100 loss: 0.0013 lr: 0.02\n",
      "iteration: 287200 loss: 0.0014 lr: 0.02\n",
      "iteration: 287300 loss: 0.0014 lr: 0.02\n",
      "iteration: 287400 loss: 0.0014 lr: 0.02\n",
      "iteration: 287500 loss: 0.0013 lr: 0.02\n",
      "iteration: 287600 loss: 0.0016 lr: 0.02\n",
      "iteration: 287700 loss: 0.0014 lr: 0.02\n",
      "iteration: 287800 loss: 0.0016 lr: 0.02\n",
      "iteration: 287900 loss: 0.0016 lr: 0.02\n",
      "iteration: 288000 loss: 0.0014 lr: 0.02\n",
      "iteration: 288100 loss: 0.0015 lr: 0.02\n",
      "iteration: 288200 loss: 0.0016 lr: 0.02\n",
      "iteration: 288300 loss: 0.0015 lr: 0.02\n",
      "iteration: 288400 loss: 0.0017 lr: 0.02\n",
      "iteration: 288500 loss: 0.0015 lr: 0.02\n",
      "iteration: 288600 loss: 0.0013 lr: 0.02\n",
      "iteration: 288700 loss: 0.0014 lr: 0.02\n",
      "iteration: 288800 loss: 0.0014 lr: 0.02\n",
      "iteration: 288900 loss: 0.0015 lr: 0.02\n",
      "iteration: 289000 loss: 0.0015 lr: 0.02\n",
      "iteration: 289100 loss: 0.0014 lr: 0.02\n",
      "iteration: 289200 loss: 0.0013 lr: 0.02\n",
      "iteration: 289300 loss: 0.0014 lr: 0.02\n",
      "iteration: 289400 loss: 0.0015 lr: 0.02\n",
      "iteration: 289500 loss: 0.0016 lr: 0.02\n",
      "iteration: 289600 loss: 0.0014 lr: 0.02\n",
      "iteration: 289700 loss: 0.0015 lr: 0.02\n",
      "iteration: 289800 loss: 0.0015 lr: 0.02\n",
      "iteration: 289900 loss: 0.0015 lr: 0.02\n",
      "iteration: 290000 loss: 0.0014 lr: 0.02\n",
      "iteration: 290100 loss: 0.0015 lr: 0.02\n",
      "iteration: 290200 loss: 0.0015 lr: 0.02\n",
      "iteration: 290300 loss: 0.0016 lr: 0.02\n",
      "iteration: 290400 loss: 0.0014 lr: 0.02\n",
      "iteration: 290500 loss: 0.0014 lr: 0.02\n",
      "iteration: 290600 loss: 0.0014 lr: 0.02\n",
      "iteration: 290700 loss: 0.0016 lr: 0.02\n",
      "iteration: 290800 loss: 0.0015 lr: 0.02\n",
      "iteration: 290900 loss: 0.0015 lr: 0.02\n",
      "iteration: 291000 loss: 0.0014 lr: 0.02\n",
      "iteration: 291100 loss: 0.0014 lr: 0.02\n",
      "iteration: 291200 loss: 0.0014 lr: 0.02\n",
      "iteration: 291300 loss: 0.0016 lr: 0.02\n",
      "iteration: 291400 loss: 0.0015 lr: 0.02\n",
      "iteration: 291500 loss: 0.0014 lr: 0.02\n",
      "iteration: 291600 loss: 0.0013 lr: 0.02\n",
      "iteration: 291700 loss: 0.0017 lr: 0.02\n",
      "iteration: 291800 loss: 0.0016 lr: 0.02\n",
      "iteration: 291900 loss: 0.0014 lr: 0.02\n",
      "iteration: 292000 loss: 0.0017 lr: 0.02\n",
      "iteration: 292100 loss: 0.0016 lr: 0.02\n",
      "iteration: 292200 loss: 0.0015 lr: 0.02\n",
      "iteration: 292300 loss: 0.0016 lr: 0.02\n",
      "iteration: 292400 loss: 0.0014 lr: 0.02\n",
      "iteration: 292500 loss: 0.0016 lr: 0.02\n",
      "iteration: 292600 loss: 0.0014 lr: 0.02\n",
      "iteration: 292700 loss: 0.0015 lr: 0.02\n",
      "iteration: 292800 loss: 0.0016 lr: 0.02\n",
      "iteration: 292900 loss: 0.0015 lr: 0.02\n",
      "iteration: 293000 loss: 0.0015 lr: 0.02\n",
      "iteration: 293100 loss: 0.0014 lr: 0.02\n",
      "iteration: 293200 loss: 0.0016 lr: 0.02\n",
      "iteration: 293300 loss: 0.0014 lr: 0.02\n",
      "iteration: 293400 loss: 0.0015 lr: 0.02\n",
      "iteration: 293500 loss: 0.0015 lr: 0.02\n",
      "iteration: 293600 loss: 0.0015 lr: 0.02\n",
      "iteration: 293700 loss: 0.0014 lr: 0.02\n",
      "iteration: 293800 loss: 0.0013 lr: 0.02\n",
      "iteration: 293900 loss: 0.0015 lr: 0.02\n",
      "iteration: 294000 loss: 0.0014 lr: 0.02\n",
      "iteration: 294100 loss: 0.0014 lr: 0.02\n",
      "iteration: 294200 loss: 0.0018 lr: 0.02\n",
      "iteration: 294300 loss: 0.0015 lr: 0.02\n",
      "iteration: 294400 loss: 0.0014 lr: 0.02\n",
      "iteration: 294500 loss: 0.0015 lr: 0.02\n",
      "iteration: 294600 loss: 0.0014 lr: 0.02\n",
      "iteration: 294700 loss: 0.0015 lr: 0.02\n",
      "iteration: 294800 loss: 0.0015 lr: 0.02\n",
      "iteration: 294900 loss: 0.0015 lr: 0.02\n",
      "iteration: 295000 loss: 0.0016 lr: 0.02\n",
      "iteration: 295100 loss: 0.0013 lr: 0.02\n",
      "iteration: 295200 loss: 0.0014 lr: 0.02\n",
      "iteration: 295300 loss: 0.0017 lr: 0.02\n",
      "iteration: 295400 loss: 0.0017 lr: 0.02\n",
      "iteration: 295500 loss: 0.0016 lr: 0.02\n",
      "iteration: 295600 loss: 0.0014 lr: 0.02\n",
      "iteration: 295700 loss: 0.0013 lr: 0.02\n",
      "iteration: 295800 loss: 0.0015 lr: 0.02\n",
      "iteration: 295900 loss: 0.0014 lr: 0.02\n",
      "iteration: 296000 loss: 0.0015 lr: 0.02\n",
      "iteration: 296100 loss: 0.0016 lr: 0.02\n",
      "iteration: 296200 loss: 0.0014 lr: 0.02\n",
      "iteration: 296300 loss: 0.0014 lr: 0.02\n",
      "iteration: 296400 loss: 0.0015 lr: 0.02\n",
      "iteration: 296500 loss: 0.0014 lr: 0.02\n",
      "iteration: 296600 loss: 0.0016 lr: 0.02\n",
      "iteration: 296700 loss: 0.0016 lr: 0.02\n",
      "iteration: 296800 loss: 0.0014 lr: 0.02\n",
      "iteration: 296900 loss: 0.0014 lr: 0.02\n",
      "iteration: 297000 loss: 0.0017 lr: 0.02\n",
      "iteration: 297100 loss: 0.0014 lr: 0.02\n",
      "iteration: 297200 loss: 0.0015 lr: 0.02\n",
      "iteration: 297300 loss: 0.0014 lr: 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplayiters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveiters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/training.py:284\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning)\u001b[0m\n\u001b[1;32m    273\u001b[0m         train(\n\u001b[1;32m    274\u001b[0m             \u001b[38;5;28mstr\u001b[39m(poseconfigfile),\n\u001b[1;32m    275\u001b[0m             displayiters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m             allow_growth\u001b[38;5;241m=\u001b[39mallow_growth,\n\u001b[1;32m    281\u001b[0m         )  \u001b[38;5;66;03m# pass on path and file name for pose_cfg.yaml!\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;28mstr\u001b[39m(start_path))\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/training.py:273\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix, superanimal_name, superanimal_transfer_learning)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeeplabcut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose_estimation_tensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelecting single-animal trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mposeconfigfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisplayiters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43msaveiters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmaxiters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_snapshots_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeepdeconvweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdeconvweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass on path and file name for pose_cfg.yaml!\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/core/train.py:287\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    284\u001b[0m     current_lr \u001b[38;5;241m=\u001b[39m lr_gen\u001b[38;5;241m.\u001b[39mget_lr(it \u001b[38;5;241m-\u001b[39m start_iter)\n\u001b[1;32m    285\u001b[0m     lr_dict \u001b[38;5;241m=\u001b[39m {learning_rate: current_lr}\n\u001b[0;32m--> 287\u001b[0m [_, loss_val, summary] \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_summaries\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_dict\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m cum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_val\n\u001b[1;32m    291\u001b[0m train_writer\u001b[38;5;241m.\u001b[39madd_summary(summary, it)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(config_path, shuffle=1, displayiters=100, saveiters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FESNewCameraApr19/FESNewCamera_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/dlc-models/iteration-0/FESNewCameraApr19-trainset95shuffle1/test/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'alpha_r': 0.02,\n",
      " 'apply_prob': 0.5,\n",
      " 'batch_size': 1,\n",
      " 'contrast': {'clahe': True,\n",
      "              'claheratio': 0.1,\n",
      "              'histeq': True,\n",
      "              'histeqratio': 0.1},\n",
      " 'convolution': {'edge': False,\n",
      "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
      "                 'embossratio': 0.1,\n",
      "                 'sharpen': False,\n",
      "                 'sharpenratio': 0.3},\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FESNewCameraApr19/FESNewCamera_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 2000,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_FESNewCameraApr19/Documentation_data-FESNewCamera_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_stage': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': '/home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/dlc-models/iteration-0/FESNewCameraApr19-trainset95shuffle1/train/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "/home/jakejoseph/anaconda3/envs/DLCGPU/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet50_FESNewCameraApr19shuffle1_50000  with # of training iterations: 50000\n",
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:19,  1.03it/s]\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/pose_estimation_tensorflow/core/evaluate.py:930: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  DataMachine.to_hdf(resultsfilename, \"df_with_missing\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-50000\n",
      "Results for 50000  training iterations: 95 1 train error: 2.36 pixels. Test error: 28.35  pixels.\n",
      "With pcutoff of 0.6  train error: 2.36 pixels. Test error: 33.95 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:67: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  DataCombined[loopscorer][bp][\"y\"][imagenr]\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  + DataCombined[loopscorer][bp][\"x\"][imagenr]\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  int(DataCombined[loopscorer][bp][\"y\"][imagenr]),\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  int(DataCombined[loopscorer][bp][\"x\"][imagenr]),\n",
      "/home/jakejoseph/Desktop/Joseph_Code/DeepLabCut/deeplabcut/utils/visualization.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = DataCombined[loopscorer][bp][\"likelihood\"][imagenr]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2048x1536 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(config_path,Shuffles=[1], plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-2000 for model /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-0/FESFatigueMay31-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2024-06-03 10:17:06.392409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-03 10:17:06.392612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.392653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.392684: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.392715: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.413813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.413878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jakejoseph/anaconda3/envs/dlcv22/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2024-06-03 10:17:06.413883: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-06-03 10:17:06.414816: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 10:17:06.429121: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  /home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/videos/Individual Motor Points top 03-05.mp4\n",
      "Loading  /home/jakejoseph/Desktop/Joseph_Code/FESNewCamera-Jake-2024-04-19/videos/Individual Motor Points top 03-05.mp4\n",
      "Duration of video [s]:  2355.87 , recorded with  30.0 fps!\n",
      "Overall # of frames:  70676  found with (before cropping) frame dimensions:  2048 1536\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2024/70676 [31:23<21:18:04,  1.12s/it]"
     ]
    }
   ],
   "source": [
    "video = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/ECRB_interleaved_stim_5_30_1.mp4'\n",
    "config_path = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/config.yaml'\n",
    "deeplabcut.analyze_videos(config_path, video, shuffle=1, save_as_csv=True, videotype='mp4')\n",
    "# deeplabcut.extract_outlier_frames(config_path, video,outlieralgorithm='uncertain',p_bound=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3], [4]],\n",
      " 'all_joints_names': ['DIP', 'PIP', 'MCP', 'Wrist', 'Forearm'],\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'dataset': 'training-datasets/iteration-1/UnaugmentedDataSet_FESFatigueMay31/FESFatigue_Jake95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'deterministic': False,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': '/home/jakejoseph/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 1.0,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'mirror': False,\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 5,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': True,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'regularize': False,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-1/FESFatigueMay31-trainset95shuffle1/test/snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n",
      "/home/jakejoseph/anaconda3/envs/DEEPLABCUT/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-297000 for model /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/dlc-models/iteration-1/FESFatigueMay31-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:06:35.179441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-18 10:06:35.186431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-18 10:06:35.188417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-18 10:06:35.190258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-18 10:06:35.192232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-18 10:06:35.193874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9729 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4\n",
      "Loading  /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4\n",
      "Duration of video [s]:  344.3 , recorded with  30.0 fps!\n",
      "Overall # of frames:  10329  found with (before cropping) frame dimensions:  2048 1536\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10329/10329 [11:56<00:00, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process video: /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4\n",
      "Loading /home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4 and data.\n",
      "Duration of video [s]: 344.3, recorded with 30.0 fps!\n",
      "Overall # of frames: 10329 with cropped frame dimensions: 2048 1536\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10329/10329 [01:56<00:00, 88.63it/s]\n"
     ]
    }
   ],
   "source": [
    "videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue_6_13_2024_2.mp4',\n",
    "           '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueadditional_6_13.mp4',\n",
    "           '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue2_6_13.mp4']\n",
    "\n",
    "original_videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4']\n",
    "for video in original_videos:\n",
    "    deeplabcut.analyze_videos(config_path, video, shuffle=1, save_as_csv=True, videotype='mp4')\n",
    "    deeplabcut.create_labeled_video(config_path, video, videotype = 'mp4', save_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_video = '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/fatiguetest0523ecrb12_2.mp4'\n",
    "videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue_6_13_2024_2.mp4',\n",
    "           '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigueadditional_6_13.mp4',\n",
    "           '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/rhodesfatigue2_6_13.mp4']\n",
    "original_videos = ['/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/ECRB_interleaved_stim_5_30_1.mp4',\n",
    "                    '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/fatiguetest0523ecrb12_2.mp4',\n",
    "                    '/home/jakejoseph/Desktop/Joseph_Code/FESFatigue-Jake-2024-05-31/videos/napierecrbfatigue_05_31_3_1.mp4']\n",
    "# deeplabcut.extract_outlier_frames(config_path, original_videos, automatic=True)\n",
    "deeplabcut.refine_labels(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data sets and updated refinement iteration to 1.\n",
      "Now you can create a new training set for the expanded annotated images (use create_training_dataset).\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.95,\n",
       "  1,\n",
       "  (array([ 11,  54,  34,   6, 155, 158,  99, 145,  38,  73,  21,  75,  63,\n",
       "          149, 188,  39, 107, 121,  88,  27,  24,  83,  61,  91,  26,  17,\n",
       "           43,  74,  56,   7, 108,  69,  71, 182, 129, 177, 110,  14,  42,\n",
       "          128,  95, 143,  44,  90, 131, 118,  76, 142,  19, 109,  81, 141,\n",
       "          100,  37,  66,  30,  53, 136, 161, 152, 114,  51,   3,  41, 127,\n",
       "          166, 113, 156,  58,  79, 151,   9, 159, 181,  48, 157, 139, 160,\n",
       "          103, 111, 101,  80,  67, 134,  49, 104,  68, 144, 183,  64, 116,\n",
       "           70,  28,  16,  52,   1,  77, 137,  35, 169, 154,  96, 112, 138,\n",
       "          178,  84, 187, 124,   0,  12,  57, 115, 167, 185,   5,  47,  72,\n",
       "          117, 133,  18,  33,  40,  25, 122,   4,  29, 165,  55, 123,  93,\n",
       "           36,  15, 126,  50,   2,  60,  89, 175,  46, 164,  85, 163,  97,\n",
       "          176, 162,  86,  94, 172, 150, 168, 130, 120, 170, 119, 125,  62,\n",
       "          148, 140,  31,  78,  13,  45,  92,  32,  22, 179, 153, 147, 174,\n",
       "            8,  59,  10,  82, 184, 102, 135, 186, 180, 132]),\n",
       "   array([ 20,  87, 171,  65, 146, 105, 173,  23,  98, 106])))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplabcut.merge_datasets(config_path)\n",
    "deeplabcut.create_training_dataset(config_path, net_type='resnet_50', augmenter_type='imgaug')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLCGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
